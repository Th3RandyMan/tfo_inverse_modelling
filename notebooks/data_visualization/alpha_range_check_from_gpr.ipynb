{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Ranges of IRL Alpha Using GPR Trained on Simulation\n",
    "We had 2 hypothesis on why GPR was having a bad prediction confidence on IRL TFO PPG data.  \n",
    "1. We do not have enough TMPs or knobs with which we can correctly guess the model\n",
    "2. The range is not sufficient for the TMPs currently is use, leading to extrapolation rather than interpolation\n",
    "\n",
    "In this notebook, we try to solve problem#2. The way we do this is by training a GPR on current simulation data and predict on a set of real data. The prediction gives clues as to which range we need to explore. Then based on that, we update our simulation space and re-iterate. Hopefully, at somepoint we should be able to reach interpolation.\n",
    "\n",
    "## GPR Prediction Method\n",
    "Theoretically, the TMPs should be independent. (Although I have some doubts based on how independece is actually measured in GPR - via a custom covariance metric using the spatial intensity fitting params. Which in my opinion, should show dependence if  multiple combinations can produce the same distance metric). Based on this independent nature, it is safe to predict each TMP with its own individual GPR model. Keep in mind, everything needs to be zero-mean, unit variance. In this notebook, I train my Scaler on the training data and use that directly on TFO PPG. But if we ever want to include TFO PPG into the training itself, this method needs to be changed.\n",
    "\n",
    "## Results\n",
    "It does seem that we are capturing the correct range since the predictions are still within the values. But somehow always predicts the same thing. This might not be such a good test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TFO_dataset import SheepData\n",
    "from math import pi\n",
    "from sklearn.gaussian_process import *\n",
    "from inverse_modelling_tfo.data.intensity_interpolation import interpolate_exp_chunk, get_interpolate_fit_params\n",
    "from inverse_modelling_tfo.data import normalize_zero_mean \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from typing import Union, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wave Int</th>\n",
       "      <th>Uterus Thickness</th>\n",
       "      <th>Maternal Wall Thickness</th>\n",
       "      <th>Maternal Mu_a</th>\n",
       "      <th>Fetal Mu_a</th>\n",
       "      <th>alpha0</th>\n",
       "      <th>alpha1</th>\n",
       "      <th>alpha2</th>\n",
       "      <th>alpha3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.05</td>\n",
       "      <td>98.489193</td>\n",
       "      <td>-2.926028</td>\n",
       "      <td>93.351279</td>\n",
       "      <td>-171.975994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.05</td>\n",
       "      <td>102.376438</td>\n",
       "      <td>-3.064418</td>\n",
       "      <td>97.312697</td>\n",
       "      <td>-178.975132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.05</td>\n",
       "      <td>106.081459</td>\n",
       "      <td>-3.196045</td>\n",
       "      <td>101.087085</td>\n",
       "      <td>-185.645325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.05</td>\n",
       "      <td>109.625911</td>\n",
       "      <td>-3.321710</td>\n",
       "      <td>104.696132</td>\n",
       "      <td>-192.024911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.05</td>\n",
       "      <td>113.027654</td>\n",
       "      <td>-3.442082</td>\n",
       "      <td>108.157935</td>\n",
       "      <td>-198.145779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Wave Int  Uterus Thickness  Maternal Wall Thickness  Maternal Mu_a  \\\n",
       "0       2.0               5.0                     26.0          0.005   \n",
       "1       2.0               5.0                     26.0          0.006   \n",
       "2       2.0               5.0                     26.0          0.007   \n",
       "3       2.0               5.0                     26.0          0.008   \n",
       "4       2.0               5.0                     26.0          0.009   \n",
       "\n",
       "   Fetal Mu_a      alpha0    alpha1      alpha2      alpha3  \n",
       "0        0.05   98.489193 -2.926028   93.351279 -171.975994  \n",
       "1        0.05  102.376438 -3.064418   97.312697 -178.975132  \n",
       "2        0.05  106.081459 -3.196045  101.087085 -185.645325  \n",
       "3        0.05  109.625911 -3.321710  104.696132 -192.024911  \n",
       "4        0.05  113.027654 -3.442082  108.157935 -198.145779  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_data = pd.read_pickle(r'/home/rraiyan/personal_projects/tfo_inverse_modelling/data/intensity/intensity_averaged_sim_data.pkl')\n",
    "train_data = pd.read_pickle(\n",
    "    r'/home/rraiyan/personal_projects/tfo_inverse_modelling/data/intensity/intensity_summed_sim_data_equidistance_detector2.pkl')\n",
    "print(len(train_data))\n",
    "# Only for intensity_summed_sim_data_equidistance_detector.pkl\n",
    "sdd = train_data['SDD'].to_numpy()[:20]\n",
    "detector_count = [11, 16, 22, 27, 32, 38, 43, 48, 53,\n",
    "                  59, 64, 69, 75, 80, 85, 90, 96, 101, 106, 111]\n",
    "sdd_to_detector_count_map = {\n",
    "    dist: count for dist, count in zip(sdd, detector_count)}\n",
    "train_data['Intensity'] /= train_data['SDD'].map(\n",
    "    sdd_to_detector_count_map)\n",
    "\n",
    "# For the other cases\n",
    "# train_data['Intensity'] /= 20   # Normalize by the number of detectors per ring\n",
    "\n",
    "train_data['Intensity'] /= 1e9  # Photon count/Initial intensity\n",
    "\n",
    "\n",
    "\n",
    "interpolated_training_data = get_interpolate_fit_params(\n",
    "    train_data, weights=[1, -1])\n",
    "\n",
    "interpolated_training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiIndex([(       'Uterus Thickness',  ''),\n",
      "            ('Maternal Wall Thickness',  ''),\n",
      "            (          'Maternal Mu_a',  ''),\n",
      "            (             'Fetal Mu_a',  ''),\n",
      "            (                 'alpha0', 1.0),\n",
      "            (                 'alpha0', 2.0),\n",
      "            (                 'alpha1', 1.0),\n",
      "            (                 'alpha1', 2.0),\n",
      "            (                 'alpha2', 1.0),\n",
      "            (                 'alpha2', 2.0),\n",
      "            (                 'alpha3', 1.0),\n",
      "            (                 'alpha3', 2.0)],\n",
      "           names=[None, 'Wave Int'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Uterus Thickness</th>\n",
       "      <th>Maternal Wall Thickness</th>\n",
       "      <th>Maternal Mu_a</th>\n",
       "      <th>Fetal Mu_a</th>\n",
       "      <th>alpha0_1</th>\n",
       "      <th>alpha0_2</th>\n",
       "      <th>alpha1_1</th>\n",
       "      <th>alpha1_2</th>\n",
       "      <th>alpha2_1</th>\n",
       "      <th>alpha2_2</th>\n",
       "      <th>alpha3_1</th>\n",
       "      <th>alpha3_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.05</td>\n",
       "      <td>113.221144</td>\n",
       "      <td>131.476209</td>\n",
       "      <td>-3.409377</td>\n",
       "      <td>-4.078752</td>\n",
       "      <td>106.769431</td>\n",
       "      <td>122.767976</td>\n",
       "      <td>-196.437138</td>\n",
       "      <td>-225.582066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.06</td>\n",
       "      <td>114.708633</td>\n",
       "      <td>136.732862</td>\n",
       "      <td>-3.485477</td>\n",
       "      <td>-4.290443</td>\n",
       "      <td>108.613750</td>\n",
       "      <td>128.726450</td>\n",
       "      <td>-199.483973</td>\n",
       "      <td>-235.785531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.07</td>\n",
       "      <td>116.106105</td>\n",
       "      <td>141.064586</td>\n",
       "      <td>-3.554691</td>\n",
       "      <td>-4.464508</td>\n",
       "      <td>110.322606</td>\n",
       "      <td>133.631160</td>\n",
       "      <td>-202.321745</td>\n",
       "      <td>-244.187574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.08</td>\n",
       "      <td>117.423466</td>\n",
       "      <td>144.738851</td>\n",
       "      <td>-3.618310</td>\n",
       "      <td>-4.611828</td>\n",
       "      <td>111.916476</td>\n",
       "      <td>137.786931</td>\n",
       "      <td>-204.979218</td>\n",
       "      <td>-251.309266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.09</td>\n",
       "      <td>118.667929</td>\n",
       "      <td>147.922095</td>\n",
       "      <td>-3.677211</td>\n",
       "      <td>-4.739188</td>\n",
       "      <td>113.409611</td>\n",
       "      <td>141.383590</td>\n",
       "      <td>-207.476654</td>\n",
       "      <td>-257.475002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Uterus Thickness  Maternal Wall Thickness  Maternal Mu_a  Fetal Mu_a  \\\n",
       "0               5.0                      2.0          0.005        0.05   \n",
       "1               5.0                      2.0          0.005        0.06   \n",
       "2               5.0                      2.0          0.005        0.07   \n",
       "3               5.0                      2.0          0.005        0.08   \n",
       "4               5.0                      2.0          0.005        0.09   \n",
       "\n",
       "     alpha0_1    alpha0_2  alpha1_1  alpha1_2    alpha2_1    alpha2_2  \\\n",
       "0  113.221144  131.476209 -3.409377 -4.078752  106.769431  122.767976   \n",
       "1  114.708633  136.732862 -3.485477 -4.290443  108.613750  128.726450   \n",
       "2  116.106105  141.064586 -3.554691 -4.464508  110.322606  133.631160   \n",
       "3  117.423466  144.738851 -3.618310 -4.611828  111.916476  137.786931   \n",
       "4  118.667929  147.922095 -3.677211 -4.739188  113.409611  141.383590   \n",
       "\n",
       "     alpha3_1    alpha3_2  \n",
       "0 -196.437138 -225.582066  \n",
       "1 -199.483973 -235.785531  \n",
       "2 -202.321745 -244.187574  \n",
       "3 -204.979218 -251.309266  \n",
       "4 -207.476654 -257.475002  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Incorporate both wavelengths by moving to a Wide Format from Long Format\n",
    "interpolated_training_data = interpolated_training_data.pivot_table(\n",
    "    index=['Uterus Thickness', 'Maternal Wall Thickness', 'Maternal Mu_a', 'Fetal Mu_a'], columns='Wave Int', values=['alpha0', 'alpha1', 'alpha2', 'alpha3']).reset_index()\n",
    "\n",
    "\n",
    "print(interpolated_training_data.columns)\n",
    "\n",
    "def _renaming_func(x, y):\n",
    "    if y == '':\n",
    "        return f'{x}'\n",
    "    else:\n",
    "        return f'{x}_{int(y)}'\n",
    "\n",
    "\n",
    "interpolated_training_data.columns = [_renaming_func(\n",
    "    x, y) for x, y in interpolated_training_data.columns]\n",
    "interpolated_training_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Uterus Thickness</th>\n",
       "      <th>Maternal Wall Thickness</th>\n",
       "      <th>Maternal Mu_a</th>\n",
       "      <th>Fetal Mu_a</th>\n",
       "      <th>alpha0_1</th>\n",
       "      <th>alpha0_2</th>\n",
       "      <th>alpha1_1</th>\n",
       "      <th>alpha1_2</th>\n",
       "      <th>alpha2_1</th>\n",
       "      <th>alpha2_2</th>\n",
       "      <th>alpha3_1</th>\n",
       "      <th>alpha3_2</th>\n",
       "      <th>Bias Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>475.0</td>\n",
       "      <td>475.000</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>475.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.0</td>\n",
       "      <td>20.000</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>113.757879</td>\n",
       "      <td>118.257236</td>\n",
       "      <td>-3.470978</td>\n",
       "      <td>-3.699648</td>\n",
       "      <td>109.017676</td>\n",
       "      <td>114.461372</td>\n",
       "      <td>-199.609237</td>\n",
       "      <td>-208.632460</td>\n",
       "      <td>0.975197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.966</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.014157</td>\n",
       "      <td>7.197941</td>\n",
       "      <td>18.694420</td>\n",
       "      <td>0.284560</td>\n",
       "      <td>0.764378</td>\n",
       "      <td>7.725566</td>\n",
       "      <td>20.343442</td>\n",
       "      <td>13.384610</td>\n",
       "      <td>35.040578</td>\n",
       "      <td>0.087413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>101.210185</td>\n",
       "      <td>97.028772</td>\n",
       "      <td>-4.149646</td>\n",
       "      <td>-5.853067</td>\n",
       "      <td>95.806122</td>\n",
       "      <td>91.751952</td>\n",
       "      <td>-231.712818</td>\n",
       "      <td>-308.248897</td>\n",
       "      <td>0.732893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>108.486387</td>\n",
       "      <td>105.693772</td>\n",
       "      <td>-3.634716</td>\n",
       "      <td>-4.052920</td>\n",
       "      <td>104.102567</td>\n",
       "      <td>100.630894</td>\n",
       "      <td>-207.259792</td>\n",
       "      <td>-224.509410</td>\n",
       "      <td>0.930474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.0</td>\n",
       "      <td>20.000</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>113.837463</td>\n",
       "      <td>112.565299</td>\n",
       "      <td>-3.447280</td>\n",
       "      <td>-3.434677</td>\n",
       "      <td>108.769624</td>\n",
       "      <td>107.658676</td>\n",
       "      <td>-199.420592</td>\n",
       "      <td>-197.276481</td>\n",
       "      <td>1.027122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.0</td>\n",
       "      <td>30.000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>118.130341</td>\n",
       "      <td>126.483307</td>\n",
       "      <td>-3.292540</td>\n",
       "      <td>-3.179258</td>\n",
       "      <td>113.281405</td>\n",
       "      <td>123.668910</td>\n",
       "      <td>-190.694314</td>\n",
       "      <td>-184.873533</td>\n",
       "      <td>1.035254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.0</td>\n",
       "      <td>38.000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>131.180255</td>\n",
       "      <td>171.336254</td>\n",
       "      <td>-2.996616</td>\n",
       "      <td>-2.871157</td>\n",
       "      <td>127.471357</td>\n",
       "      <td>172.229066</td>\n",
       "      <td>-176.537592</td>\n",
       "      <td>-169.205054</td>\n",
       "      <td>1.046877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Uterus Thickness  Maternal Wall Thickness  Maternal Mu_a  Fetal Mu_a  \\\n",
       "count             475.0                  475.000     475.000000  475.000000   \n",
       "mean                5.0                   20.000       0.007000    0.070000   \n",
       "std                 0.0                   10.966       0.001416    0.014157   \n",
       "min                 5.0                    2.000       0.005000    0.050000   \n",
       "25%                 5.0                   10.000       0.006000    0.060000   \n",
       "50%                 5.0                   20.000       0.007000    0.070000   \n",
       "75%                 5.0                   30.000       0.008000    0.080000   \n",
       "max                 5.0                   38.000       0.009000    0.090000   \n",
       "\n",
       "         alpha0_1    alpha0_2    alpha1_1    alpha1_2    alpha2_1    alpha2_2  \\\n",
       "count  475.000000  475.000000  475.000000  475.000000  475.000000  475.000000   \n",
       "mean   113.757879  118.257236   -3.470978   -3.699648  109.017676  114.461372   \n",
       "std      7.197941   18.694420    0.284560    0.764378    7.725566   20.343442   \n",
       "min    101.210185   97.028772   -4.149646   -5.853067   95.806122   91.751952   \n",
       "25%    108.486387  105.693772   -3.634716   -4.052920  104.102567  100.630894   \n",
       "50%    113.837463  112.565299   -3.447280   -3.434677  108.769624  107.658676   \n",
       "75%    118.130341  126.483307   -3.292540   -3.179258  113.281405  123.668910   \n",
       "max    131.180255  171.336254   -2.996616   -2.871157  127.471357  172.229066   \n",
       "\n",
       "         alpha3_1    alpha3_2  Bias Ratio  \n",
       "count  475.000000  475.000000  475.000000  \n",
       "mean  -199.609237 -208.632460    0.975197  \n",
       "std     13.384610   35.040578    0.087413  \n",
       "min   -231.712818 -308.248897    0.732893  \n",
       "25%   -207.259792 -224.509410    0.930474  \n",
       "50%   -199.420592 -197.276481    1.027122  \n",
       "75%   -190.694314 -184.873533    1.035254  \n",
       "max   -176.537592 -169.205054    1.046877  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Non-Normalized Training data\n",
    "interpolated_training_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting only on both WV\n",
    "\n",
    "# Create Features & Normalize the fitting params\n",
    "# interpolated_training_data['Bias Ratio'] = interpolated_training_data['alpha0_1'] / interpolated_training_data['alpha0_2']\n",
    "\n",
    "# X = interpolated_training_data[['Bias Ratio', 'alpha1_1', 'alpha1_2', 'alpha2_1', 'alpha2_2', 'alpha3_1', 'alpha3_2']].to_numpy()\n",
    "X = interpolated_training_data[['alpha0_1', 'alpha0_2', 'alpha1_1', 'alpha1_2', 'alpha2_1', 'alpha2_2', 'alpha3_1', 'alpha3_2']].to_numpy()\n",
    "alpha_scaler = preprocessing.StandardScaler().fit(X)\n",
    "X = alpha_scaler.transform(X)\n",
    "\n",
    "y = interpolated_training_data[['Fetal Mu_a']].to_numpy().flatten()\n",
    "# y = interpolated_training_data[['Maternal Mu_a']].to_numpy().flatten()\n",
    "y_scaler = preprocessing.StandardScaler().fit(y.reshape(-1, 1))\n",
    "y = y_scaler.transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "random_indices = rng.choice(np.arange(y.size), size=y.size, replace=False)\n",
    "training_count = int(y.size * 1)  # 80% Training Data\n",
    "training_indices = random_indices[:training_count]\n",
    "test_indices = random_indices[training_count:]\n",
    "\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "X_test, y_test = X[test_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.03**2 * Matern(length_scale=5.04e-05, nu=1.5)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kernel = 1 * kernels.RBF(length_scale=1.0, length_scale_bounds=(1e-4, 1e1))\n",
    "kernel = 1 * kernels.Matern(length_scale=1.0, length_scale_bounds=(1e-6, 1e-2))\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "gp.fit(X_train, y_train)\n",
    "gp.kernel_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PRedict on Simulation\n",
    "# X_test = X_train\n",
    "# y_test = y_train\n",
    "# mean_prediction, std_prediction = gp.predict(X_test, return_std=True)\n",
    "# mae = np.abs(mean_prediction - y_test)\n",
    "# mse = np.square(mean_prediction - y_test)\n",
    "# df = pd.DataFrame(\n",
    "#     {\n",
    "#         'True a0' : X_test[:, 0],\n",
    "#         'True a1' : X_test[:, 1],\n",
    "#         'True a2' : X_test[:, 2],\n",
    "#         'True a3' : X_test[:, 3],\n",
    "#         'True y'  : y_test,\n",
    "#         'Prediction' : mean_prediction,\n",
    "#         'Confidence' : std_prediction,\n",
    "#         'MAE(%)' : mae * 100,\n",
    "#         'MSE(%)' : mse * 100,\n",
    "#     }\n",
    "# )\n",
    "# pd.set_option('display.max_rows', 1200)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['MAE(%)'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_patient_ppg(ppg_data : pd.DataFrame, sample_number : Union[int, List], SDD = [15, 30, 45, 70, 100]) -> np.ndarray:\n",
    "    \"\"\"Prepare PPG data to be used directly into the GPR prediction.\n",
    "\n",
    "    Args:\n",
    "        ppg_data (pd.DataFrame): PPG data Dataframe. You can feed data directly from the the TFO_dataset package.\n",
    "        (Note: This should ideally be the optically normalized data)\n",
    "        sample_number (int): which sample to choose. You can either pass a single integer or an array\n",
    "        SDD (_type_, optional): Detector distances in TFO device(in mm). Defaults to SDD=[15, 30, 45, 70, 100].\n",
    "    \"\"\"\n",
    "    # The code is generalized to run on any array. make necessary conversions \n",
    "    if isinstance(sample_number, int):\n",
    "        sample_number = [sample_number]\n",
    "    \n",
    "    patient_features = []\n",
    "    for sample_point in sample_number:\n",
    "        # Pick a point in time\n",
    "        spatial_intensity = ppg_data.iloc[sample_point].copy() \n",
    "        spatial_intensity *=  pi * 4   # from unit area -> pi r^2 area -> match simulation\n",
    "        # Reshape ppg data to fit the format\n",
    "        spatial_intensity_wv1 = pd.DataFrame(data={\n",
    "            'SDD' : SDD,\n",
    "            'Intensity' : spatial_intensity.to_numpy()[:5]\n",
    "        })\n",
    "        spatial_intensity_wv2 = pd.DataFrame(data={\n",
    "            'SDD' : SDD,\n",
    "            'Intensity' : spatial_intensity.to_numpy()[5:]\n",
    "        })\n",
    "        alpha_wv1 = interpolate_exp_chunk(spatial_intensity_wv1, weights=[1.0, -1.0], return_alpha=True).flatten()\n",
    "        alpha_wv2 = interpolate_exp_chunk(spatial_intensity_wv2, weights=[1.0, -1.0], return_alpha=True).flatten()\n",
    "        patient_features.append([alpha_wv1[0], alpha_wv2[0], alpha_wv1[1], alpha_wv2[1], alpha_wv1[2], alpha_wv2[2], alpha_wv1[3], alpha_wv2[3]])\n",
    "        \n",
    "    return np.array(patient_features)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Part of the Data\n",
    "Everything above only needs to be run once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Length : 217120\n",
      "Non- normalized Features\n",
      "           f1        f2         f3        f4         f5        f6         f7  \\\n",
      "0  -22.592250 -8.590743  14.447283  5.615544 -18.680464 -7.176588  20.051632   \n",
      "1  -21.557080 -8.056010  13.981027  5.385607 -17.941689 -6.801068  19.215122   \n",
      "2  -22.336084 -8.488919  14.343910  5.579962 -18.506633 -7.110346  19.850355   \n",
      "3  -21.960485 -8.322790  14.172400  5.507752 -18.237542 -6.993676  19.546692   \n",
      "4  -20.744321 -7.514747  13.627232  5.164015 -17.371986 -6.430865  18.567612   \n",
      "5  -22.052601 -8.363931  14.207150  5.519520 -18.299443 -7.018544  19.618696   \n",
      "6  -21.781283 -8.234477  14.092928  5.469957 -18.109868 -6.931818  19.402170   \n",
      "7  -22.582861 -8.763239  14.448131  5.697629 -18.676211 -7.302305  20.044599   \n",
      "8  -22.247459 -8.552147  14.298128  5.606241 -18.438763 -7.154437  19.775424   \n",
      "9  -23.432404 -9.114990  14.843910  5.856150 -19.292872 -7.554085  20.738030   \n",
      "10 -22.136948 -8.353189  14.253825  5.529985 -18.363609 -7.021503  19.689029   \n",
      "11 -13.039730 -0.736152  10.264977  2.443728 -11.939792 -1.817344  12.398514   \n",
      "12 -11.305534  2.169620   9.490198  1.270492 -10.703519  0.169537  10.999158   \n",
      "13 -17.471589 -4.167298  12.150817  3.809264 -15.028019 -4.145001  15.920032   \n",
      "14 -20.815544 -7.208551  13.652188  5.052117 -17.413543 -6.230406  18.617515   \n",
      "15 -21.151661 -7.878027  13.801969  5.323886 -17.654230 -6.687867  18.889091   \n",
      "16 -21.707168 -8.220264  14.042578  5.472553 -18.044555 -6.928395  19.332352   \n",
      "17 -21.220159 -7.930781  13.827453  5.352903 -17.699899 -6.728852  18.941130   \n",
      "18 -22.274545 -8.546014  14.292952  5.614935 -18.445217 -7.158094  19.786818   \n",
      "19 -21.631863 -8.209440  13.981869  5.455275 -17.946382 -6.895003  19.219032   \n",
      "\n",
      "          f8  \n",
      "0   7.673751  \n",
      "1   7.245468  \n",
      "2   7.595337  \n",
      "3   7.462546  \n",
      "4   6.820054  \n",
      "5   7.492307  \n",
      "6   7.391620  \n",
      "7   7.813529  \n",
      "8   7.645092  \n",
      "9   8.098069  \n",
      "10  7.490940  \n",
      "11  1.507658  \n",
      "12 -0.776651  \n",
      "13  4.190905  \n",
      "14  6.585300  \n",
      "15  7.111112  \n",
      "16  7.384805  \n",
      "17  7.156059  \n",
      "18  7.645771  \n",
      "19  7.345180  \n"
     ]
    }
   ],
   "source": [
    "# Predict on reallife data\n",
    "tag = {'experiment_number': 11, 'experiment_round': 1, 'experiment_year_prefix': 'sp2022',\n",
    "       'additional_info': '', 'data_version': 'iq_demod_optical'}\n",
    "data = SheepData('iq_demod_optical').get_data_from_tag(tag)\n",
    "print(f'Sample Length : {len(data)}')\n",
    "\n",
    "# Pick 20 equidistance points within the length\n",
    "point_count = 20\n",
    "sample_numbers = np.linspace(100, len(data) - 1, point_count)\n",
    "sample_numbers = [int(x) for x in sample_numbers]\n",
    "features = prepare_patient_ppg(data, sample_numbers)\n",
    "\n",
    "# Create a DF for better viz.\n",
    "print('Non- normalized Features')\n",
    "feature_names = [f'f{i + 1}' for i in range(8)]\n",
    "features = alpha_scaler.transform(features)\n",
    "print(pd.DataFrame(features, columns=feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07]\n",
      " [0.07]\n",
      " [0.07]\n",
      " [0.07]\n",
      " [0.07]\n",
      " [0.07]\n",
      " [0.07]\n",
      " [0.07]\n",
      " [0.07]\n",
      " [0.07]\n",
      " [0.07]\n",
      " [0.07]\n",
      " [0.07]\n",
      " [0.07]\n",
      " [0.07]\n",
      " [0.07]\n",
      " [0.07]\n",
      " [0.07]\n",
      " [0.07]\n",
      " [0.07]]\n",
      "          0         1         2         3         4         5         6   \\\n",
      "0   1.061228  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1   0.000000  1.061228  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "2   0.000000  0.000000  1.061228  0.000000  0.000000  0.000000  0.000000   \n",
      "3   0.000000  0.000000  0.000000  1.061228  0.000000  0.000000  0.000000   \n",
      "4   0.000000  0.000000  0.000000  0.000000  1.061228  0.000000  0.000000   \n",
      "5   0.000000  0.000000  0.000000  0.000000  0.000000  1.061228  0.000000   \n",
      "6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  1.061228   \n",
      "7   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "8   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "9   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "11  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "12  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "13  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "14  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "15  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "16  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "17  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "18  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "19  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "          7         8         9         10        11        12        13  \\\n",
      "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "2   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "3   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "7   1.061228  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "8   0.000000  1.061228  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "9   0.000000  0.000000  1.061228  0.000000  0.000000  0.000000  0.000000   \n",
      "10  0.000000  0.000000  0.000000  1.061228  0.000000  0.000000  0.000000   \n",
      "11  0.000000  0.000000  0.000000  0.000000  1.061228  0.000000  0.000000   \n",
      "12  0.000000  0.000000  0.000000  0.000000  0.000000  1.061228  0.000000   \n",
      "13  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  1.061228   \n",
      "14  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "15  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "16  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "17  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "18  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "19  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "          14        15        16        17        18        19  \n",
      "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "2   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "3   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "7   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "8   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "9   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "11  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "12  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "13  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "14  1.061228  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "15  0.000000  1.061228  0.000000  0.000000  0.000000  0.000000  \n",
      "16  0.000000  0.000000  1.061228  0.000000  0.000000  0.000000  \n",
      "17  0.000000  0.000000  0.000000  1.061228  0.000000  0.000000  \n",
      "18  0.000000  0.000000  0.000000  0.000000  1.061228  0.000000  \n",
      "19  0.000000  0.000000  0.000000  0.000000  0.000000  1.061228  \n"
     ]
    }
   ],
   "source": [
    "estimate, confidence = gp.predict(features, return_cov=True)\n",
    "# estimate, confidence = gp.predict(X_train[0, :].reshape(1, -1), return_std=True)\n",
    "print(y_scaler.inverse_transform(np.array(estimate).reshape(-1, 1)))\n",
    "print(pd.DataFrame(confidence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha means (From training) : [ 113.75787911  118.25723638   -3.47097802   -3.69964811  109.01767627\n",
      "  114.46137244 -199.60923674 -208.6324601 ]\n",
      "alpha variance (From training) : [5.17012756e+01 3.48745582e+02 8.08040191e-02 5.83043774e-01\n",
      " 5.95587113e+01 4.12984369e+02 1.78770619e+02 1.22525721e+03]\n"
     ]
    }
   ],
   "source": [
    "print(f'alpha means (From training) : {alpha_scaler.mean_}')\n",
    "print(f'alpha variance (From training) : {alpha_scaler.var_}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cybercat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
