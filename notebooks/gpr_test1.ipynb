{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TFO_dataset\n",
    "from math import pi\n",
    "from sklearn.gaussian_process import *\n",
    "from inverse_modelling_tfo.data.intensity_interpolation import interpolate_exp_chunk, get_interpolate_fit_params\n",
    "from inverse_modelling_tfo.data import normalize_zero_mean \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from typing import Union, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Uterus Thickness</th>\n",
       "      <th>Maternal Wall Thickness</th>\n",
       "      <th>Maternal Mu_a</th>\n",
       "      <th>Fetal Mu_a</th>\n",
       "      <th>alpha0_1</th>\n",
       "      <th>alpha0_2</th>\n",
       "      <th>alpha1_1</th>\n",
       "      <th>alpha1_2</th>\n",
       "      <th>alpha2_1</th>\n",
       "      <th>alpha2_2</th>\n",
       "      <th>alpha3_1</th>\n",
       "      <th>alpha3_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-12.592608</td>\n",
       "      <td>3.395675</td>\n",
       "      <td>0.373749</td>\n",
       "      <td>0.413286</td>\n",
       "      <td>-14.666515</td>\n",
       "      <td>-10.940015</td>\n",
       "      <td>23.954485</td>\n",
       "      <td>10.568332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-13.102171</td>\n",
       "      <td>2.622967</td>\n",
       "      <td>0.388989</td>\n",
       "      <td>0.431987</td>\n",
       "      <td>-15.231653</td>\n",
       "      <td>-11.677655</td>\n",
       "      <td>24.949038</td>\n",
       "      <td>11.923494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-13.573916</td>\n",
       "      <td>1.912768</td>\n",
       "      <td>0.402962</td>\n",
       "      <td>0.449012</td>\n",
       "      <td>-15.751852</td>\n",
       "      <td>-12.352166</td>\n",
       "      <td>25.866022</td>\n",
       "      <td>13.164688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.065</td>\n",
       "      <td>-14.013569</td>\n",
       "      <td>1.251053</td>\n",
       "      <td>0.415860</td>\n",
       "      <td>0.464721</td>\n",
       "      <td>-16.233896</td>\n",
       "      <td>-12.977252</td>\n",
       "      <td>26.717161</td>\n",
       "      <td>14.316873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-14.425410</td>\n",
       "      <td>0.628060</td>\n",
       "      <td>0.427827</td>\n",
       "      <td>0.479371</td>\n",
       "      <td>-16.682951</td>\n",
       "      <td>-13.562583</td>\n",
       "      <td>27.511341</td>\n",
       "      <td>15.397590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Uterus Thickness  Maternal Wall Thickness  Maternal Mu_a  Fetal Mu_a  \\\n",
       "0               5.0                      2.0          0.005       0.050   \n",
       "1               5.0                      2.0          0.005       0.055   \n",
       "2               5.0                      2.0          0.005       0.060   \n",
       "3               5.0                      2.0          0.005       0.065   \n",
       "4               5.0                      2.0          0.005       0.070   \n",
       "\n",
       "    alpha0_1  alpha0_2  alpha1_1  alpha1_2   alpha2_1   alpha2_2   alpha3_1  \\\n",
       "0 -12.592608  3.395675  0.373749  0.413286 -14.666515 -10.940015  23.954485   \n",
       "1 -13.102171  2.622967  0.388989  0.431987 -15.231653 -11.677655  24.949038   \n",
       "2 -13.573916  1.912768  0.402962  0.449012 -15.751852 -12.352166  25.866022   \n",
       "3 -14.013569  1.251053  0.415860  0.464721 -16.233896 -12.977252  26.717161   \n",
       "4 -14.425410  0.628060  0.427827  0.479371 -16.682951 -13.562583  27.511341   \n",
       "\n",
       "    alpha3_2  \n",
       "0  10.568332  \n",
       "1  11.923494  \n",
       "2  13.164688  \n",
       "3  14.316873  \n",
       "4  15.397590  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_pickle(r'/home/rraiyan/personal_projects/tfo_inverse_modelling/data/intensity/intensity_averaged_sim_data.pkl')\n",
    "train_data['Intensity'] /= 20   # Normalize by the number of detectors per ring\n",
    "\n",
    "# Make the TMPs zero mean\n",
    "\n",
    "train_data.head()\n",
    "interpolated_training_data = get_interpolate_fit_params(train_data, weights=[1, -2])\n",
    "\n",
    "# Incorporate both wavelengths by moving to a Wide Format from Long Format\n",
    "interpolated_training_data = interpolated_training_data.pivot_table(index=['Uterus Thickness', 'Maternal Wall Thickness', 'Maternal Mu_a', 'Fetal Mu_a'], columns='Wave Int', values=['alpha0', 'alpha1', 'alpha2', 'alpha3']).reset_index()\n",
    "def _renaming_func(x, y):\n",
    "    if y == '':\n",
    "        return f'{x}'\n",
    "    else:\n",
    "        return f'{x}_{int(y)}'\n",
    "interpolated_training_data.columns = [_renaming_func(x, y) for x,y in interpolated_training_data.columns]\n",
    "interpolated_training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting only on both WV\n",
    "\n",
    "# Create Features & Normalize the fitting params\n",
    "interpolated_training_data['Bias Ratio'] = interpolated_training_data['alpha0_1'] / interpolated_training_data['alpha0_2'] \n",
    "\n",
    "X = interpolated_training_data[['Bias Ratio', 'alpha1_1', 'alpha1_2', 'alpha2_1', 'alpha2_2', 'alpha3_1', 'alpha3_2']].to_numpy()\n",
    "alpha_scaler = preprocessing.StandardScaler().fit(X)\n",
    "X = alpha_scaler.transform(X)\n",
    "\n",
    "y = interpolated_training_data[['Maternal Wall Thickness']].to_numpy().flatten()\n",
    "# y = interpolated_training_data[['Maternal Mu_a']].to_numpy().flatten()\n",
    "y_scaler = preprocessing.StandardScaler().fit(y.reshape(-1, 1))\n",
    "y = y_scaler.transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "random_indices = rng.choice(np.arange(y.size), size=y.size, replace=False)\n",
    "training_count = int(y.size * 1)  # 80% Training Data\n",
    "training_indices = random_indices[:training_count]\n",
    "test_indices = random_indices[training_count:]\n",
    "\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "X_test, y_test = X[test_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.5**2 * Matern(length_scale=6.47, nu=1.5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kernel = 1 * kernels.RBF(length_scale=1.0, length_scale_bounds=(1e-4, 1e1))\n",
    "kernel = 1 * kernels.Matern(length_scale=1.0, length_scale_bounds=(1e-4, 1e1))\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "gp.fit(X_train, y_train)\n",
    "gp.kernel_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PRedict on Simulation\n",
    "# X_test = X_train\n",
    "# y_test = y_train\n",
    "# mean_prediction, std_prediction = gp.predict(X_test, return_std=True)\n",
    "# mae = np.abs(mean_prediction - y_test)\n",
    "# mse = np.square(mean_prediction - y_test)\n",
    "# df = pd.DataFrame(\n",
    "#     {\n",
    "#         'True a0' : X_test[:, 0],\n",
    "#         'True a1' : X_test[:, 1],\n",
    "#         'True a2' : X_test[:, 2],\n",
    "#         'True a3' : X_test[:, 3],\n",
    "#         'True y'  : y_test,\n",
    "#         'Prediction' : mean_prediction,\n",
    "#         'Confidence' : std_prediction,\n",
    "#         'MAE(%)' : mae * 100,\n",
    "#         'MSE(%)' : mse * 100,\n",
    "#     }\n",
    "# )\n",
    "# pd.set_option('display.max_rows', 1200)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['MAE(%)'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_patient_ppg(ppg_data : pd.DataFrame, sample_number : Union[int, List], SDD = [15, 30, 45, 70, 100]) -> np.ndarray:\n",
    "    \"\"\"Prepare PPG data to be used directly into the GPR prediction.\n",
    "\n",
    "    Args:\n",
    "        ppg_data (pd.DataFrame): PPG data Dataframe. You can feed data directly from the the TFO_dataset package.\n",
    "        (Note: This should ideally be the optically normalized data)\n",
    "        sample_number (int): which sample to choose. You can either pass a single integer or an array\n",
    "        SDD (_type_, optional): Detector distances in TFO device(in mm). Defaults to SDD=[15, 30, 45, 70, 100].\n",
    "    \"\"\"\n",
    "    # The code is generalized to run on any array. make necessary conversions \n",
    "    if isinstance(sample_number, int):\n",
    "        sample_number = [sample_number]\n",
    "    \n",
    "    patient_features = []\n",
    "    for sample_point in sample_number:\n",
    "        # Pick a point in time\n",
    "        spatial_intensity = ppg_data.iloc[sample_point].copy()  # at 300s with 80Hz sampling freq.\n",
    "        spatial_intensity *=  pi * 4   # from unit area -> pi r^2 area -> match simulation\n",
    "        # Reshape ppg data to fit the format\n",
    "        spatial_intensity_wv1 = pd.DataFrame(data={\n",
    "            'SDD' : SDD,\n",
    "            'Intensity' : spatial_intensity.to_numpy()[:5]\n",
    "        })\n",
    "        spatial_intensity_wv2 = pd.DataFrame(data={\n",
    "            'SDD' : SDD,\n",
    "            'Intensity' : spatial_intensity.to_numpy()[5:]\n",
    "        })\n",
    "        alpha_wv1 = interpolate_exp_chunk(spatial_intensity_wv1, weights=[1.0, -2.0], return_alpha=True).flatten()\n",
    "        alpha_wv2 = interpolate_exp_chunk(spatial_intensity_wv2, weights=[1.0, -2.0], return_alpha=True).flatten()\n",
    "        patient_features.append([alpha_wv1[0]/alpha_wv2[0], alpha_wv1[1], alpha_wv2[1], alpha_wv1[2], alpha_wv2[2], alpha_wv1[3], alpha_wv2[3]])\n",
    "        \n",
    "    return np.array(patient_features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 'sp2022')\n",
      "Non- normalized Features\n",
      "         f1        f2        f3         f4        f5         f6         f7\n",
      "0  1.804691 -0.388345 -0.222053  13.789534  6.878564 -29.756532 -16.434083\n",
      "1  1.812935 -0.381013 -0.217476  13.375781  6.627063 -28.893555 -15.907345\n",
      "2  1.879090 -0.389864 -0.223130  13.761774  6.786435 -29.636649 -16.093875\n",
      "3  2.215944 -0.382401 -0.193924  13.267262  5.322043 -28.588720 -13.226143\n"
     ]
    }
   ],
   "source": [
    "# Predict on reallife data\n",
    "sheep_id = 21\n",
    "data = TFO_dataset.SheepData('iq_demod_optical').get(sheep_id)\n",
    "print(TFO_dataset.SheepData('iq_demod_optical').get_tuple(sheep_id))\n",
    "\n",
    "features = prepare_patient_ppg(data, [100 * 80, 200 * 80, 1000 * 80, 2000 * 80])\n",
    "\n",
    "# Create a DF for better viz.\n",
    "print('Non- normalized Features')\n",
    "feature_names = [f'f{i + 1}' for i in range(7)]\n",
    "feature_df = pd.DataFrame(columns=feature_names, data=features)\n",
    "print(feature_df)\n",
    "features = alpha_scaler.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[106.67518907]\n",
      " [107.54110061]\n",
      " [104.0299961 ]\n",
      " [109.51145205]]\n",
      "[[141.64484029 134.21474545 140.86727447 130.61922064]\n",
      " [134.21474545 127.47718692 133.50134026 124.27737817]\n",
      " [140.86727447 133.50134026 140.112953   130.01853325]\n",
      " [130.61922064 124.27737817 130.01853325 122.32902487]]\n"
     ]
    }
   ],
   "source": [
    "estimate, confidence = gp.predict(features, return_cov=True)\n",
    "# estimate, confidence = gp.predict(X_train[0, :].reshape(1, -1), return_std=True)\n",
    "print(y_scaler.inverse_transform(np.array(estimate).reshape(-1, 1)))\n",
    "print(confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha means (From training) : [ 1.04939786  0.01790294 -0.03600841 -6.99034763 -6.16144278 13.70085319\n",
      " 12.28273509]\n",
      "alpha variance (From training) : [6.57791855e+03 9.80856727e-02 3.21627942e-01 1.40932083e+02\n",
      " 4.86134570e+02 4.77191920e+02 1.64217933e+03]\n"
     ]
    }
   ],
   "source": [
    "print(f'alpha means (From training) : {alpha_scaler.mean_}')\n",
    "print(f'alpha variance (From training) : {alpha_scaler.var_}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cybercat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
