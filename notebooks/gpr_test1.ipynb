{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TFO_dataset\n",
    "from math import pi\n",
    "from sklearn.gaussian_process import *\n",
    "from inverse_modelling_tfo.data.intensity_interpolation import interpolate_exp_chunk, get_interpolate_fit_params\n",
    "from inverse_modelling_tfo.data import normalize_zero_mean \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from typing import Union, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wave Int</th>\n",
       "      <th>Uterus Thickness</th>\n",
       "      <th>Maternal Wall Thickness</th>\n",
       "      <th>Maternal Mu_a</th>\n",
       "      <th>Fetal Mu_a</th>\n",
       "      <th>alpha0</th>\n",
       "      <th>alpha1</th>\n",
       "      <th>alpha2</th>\n",
       "      <th>alpha3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-16.959114</td>\n",
       "      <td>1.036984</td>\n",
       "      <td>-29.212411</td>\n",
       "      <td>42.008351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-17.818238</td>\n",
       "      <td>1.068003</td>\n",
       "      <td>-30.353526</td>\n",
       "      <td>43.906572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-18.635541</td>\n",
       "      <td>1.097803</td>\n",
       "      <td>-31.440929</td>\n",
       "      <td>45.713937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-19.415485</td>\n",
       "      <td>1.126506</td>\n",
       "      <td>-32.480971</td>\n",
       "      <td>47.441001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-20.161950</td>\n",
       "      <td>1.154220</td>\n",
       "      <td>-33.478903</td>\n",
       "      <td>49.096596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Wave Int  Uterus Thickness  Maternal Wall Thickness  Maternal Mu_a  \\\n",
       "0       2.0               5.0                     26.0          0.005   \n",
       "1       2.0               5.0                     26.0          0.006   \n",
       "2       2.0               5.0                     26.0          0.007   \n",
       "3       2.0               5.0                     26.0          0.008   \n",
       "4       2.0               5.0                     26.0          0.009   \n",
       "\n",
       "   Fetal Mu_a     alpha0    alpha1     alpha2     alpha3  \n",
       "0        0.05 -16.959114  1.036984 -29.212411  42.008351  \n",
       "1        0.05 -17.818238  1.068003 -30.353526  43.906572  \n",
       "2        0.05 -18.635541  1.097803 -31.440929  45.713937  \n",
       "3        0.05 -19.415485  1.126506 -32.480971  47.441001  \n",
       "4        0.05 -20.161950  1.154220 -33.478903  49.096596  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_data = pd.read_pickle(r'/home/rraiyan/personal_projects/tfo_inverse_modelling/data/intensity/intensity_averaged_sim_data.pkl')\n",
    "train_data = pd.read_pickle(\n",
    "    r'/home/rraiyan/personal_projects/tfo_inverse_modelling/data/intensity/intensity_summed_sim_data_equidistance_detector2.pkl')\n",
    "# Only for intensity_summed_sim_data_equidistance_detector.pkl\n",
    "sdd = train_data['SDD'].to_numpy()[:20]\n",
    "detector_count = [11, 16, 22, 27, 32, 38, 43, 48, 53,\n",
    "                  59, 64, 69, 75, 80, 85, 90, 96, 101, 106, 111]\n",
    "sdd_to_detector_count_map = {\n",
    "    dist: count for dist, count in zip(sdd, detector_count)}\n",
    "train_data['Intensity'] /= train_data['SDD'].map(\n",
    "    sdd_to_detector_count_map)\n",
    "\n",
    "# For the other cases\n",
    "# train_data['Intensity'] /= 20   # Normalize by the number of detectors per ring\n",
    "\n",
    "train_data['Intensity'] /= 1e9  # Photon count/Initial intensity\n",
    "\n",
    "\n",
    "\n",
    "interpolated_training_data = get_interpolate_fit_params(\n",
    "    train_data, weights=[1, -2])\n",
    "\n",
    "interpolated_training_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiIndex([(       'Uterus Thickness',  ''),\n",
      "            ('Maternal Wall Thickness',  ''),\n",
      "            (          'Maternal Mu_a',  ''),\n",
      "            (             'Fetal Mu_a',  ''),\n",
      "            (                 'alpha0', 1.0),\n",
      "            (                 'alpha0', 2.0),\n",
      "            (                 'alpha1', 1.0),\n",
      "            (                 'alpha1', 2.0),\n",
      "            (                 'alpha2', 1.0),\n",
      "            (                 'alpha2', 2.0),\n",
      "            (                 'alpha3', 1.0),\n",
      "            (                 'alpha3', 2.0)],\n",
      "           names=[None, 'Wave Int'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Uterus Thickness</th>\n",
       "      <th>Maternal Wall Thickness</th>\n",
       "      <th>Maternal Mu_a</th>\n",
       "      <th>Fetal Mu_a</th>\n",
       "      <th>alpha0_1</th>\n",
       "      <th>alpha0_2</th>\n",
       "      <th>alpha1_1</th>\n",
       "      <th>alpha1_2</th>\n",
       "      <th>alpha2_1</th>\n",
       "      <th>alpha2_2</th>\n",
       "      <th>alpha3_1</th>\n",
       "      <th>alpha3_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-18.731813</td>\n",
       "      <td>-20.444467</td>\n",
       "      <td>1.136306</td>\n",
       "      <td>1.165762</td>\n",
       "      <td>-33.474455</td>\n",
       "      <td>-38.806152</td>\n",
       "      <td>48.387194</td>\n",
       "      <td>56.459243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-19.395468</td>\n",
       "      <td>-22.200493</td>\n",
       "      <td>1.137524</td>\n",
       "      <td>1.206672</td>\n",
       "      <td>-33.947994</td>\n",
       "      <td>-40.409166</td>\n",
       "      <td>49.381108</td>\n",
       "      <td>59.436939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-19.993693</td>\n",
       "      <td>-23.640858</td>\n",
       "      <td>1.140056</td>\n",
       "      <td>1.240522</td>\n",
       "      <td>-34.389651</td>\n",
       "      <td>-41.728256</td>\n",
       "      <td>50.292328</td>\n",
       "      <td>61.884206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-20.539531</td>\n",
       "      <td>-24.856917</td>\n",
       "      <td>1.143432</td>\n",
       "      <td>1.269353</td>\n",
       "      <td>-34.803629</td>\n",
       "      <td>-42.845508</td>\n",
       "      <td>51.135119</td>\n",
       "      <td>63.954438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-21.041806</td>\n",
       "      <td>-25.905711</td>\n",
       "      <td>1.147346</td>\n",
       "      <td>1.294433</td>\n",
       "      <td>-35.192921</td>\n",
       "      <td>-43.812067</td>\n",
       "      <td>51.919289</td>\n",
       "      <td>65.743294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Uterus Thickness  Maternal Wall Thickness  Maternal Mu_a  Fetal Mu_a  \\\n",
       "0               5.0                      2.0          0.005        0.05   \n",
       "1               5.0                      2.0          0.005        0.06   \n",
       "2               5.0                      2.0          0.005        0.07   \n",
       "3               5.0                      2.0          0.005        0.08   \n",
       "4               5.0                      2.0          0.005        0.09   \n",
       "\n",
       "    alpha0_1   alpha0_2  alpha1_1  alpha1_2   alpha2_1   alpha2_2   alpha3_1  \\\n",
       "0 -18.731813 -20.444467  1.136306  1.165762 -33.474455 -38.806152  48.387194   \n",
       "1 -19.395468 -22.200493  1.137524  1.206672 -33.947994 -40.409166  49.381108   \n",
       "2 -19.993693 -23.640858  1.140056  1.240522 -34.389651 -41.728256  50.292328   \n",
       "3 -20.539531 -24.856917  1.143432  1.269353 -34.803629 -42.845508  51.135119   \n",
       "4 -21.041806 -25.905711  1.147346  1.294433 -35.192921 -43.812067  51.919289   \n",
       "\n",
       "    alpha3_2  \n",
       "0  56.459243  \n",
       "1  59.436939  \n",
       "2  61.884206  \n",
       "3  63.954438  \n",
       "4  65.743294  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Incorporate both wavelengths by moving to a Wide Format from Long Format\n",
    "interpolated_training_data = interpolated_training_data.pivot_table(\n",
    "    index=['Uterus Thickness', 'Maternal Wall Thickness', 'Maternal Mu_a', 'Fetal Mu_a'], columns='Wave Int', values=['alpha0', 'alpha1', 'alpha2', 'alpha3']).reset_index()\n",
    "\n",
    "\n",
    "print(interpolated_training_data.columns)\n",
    "\n",
    "def _renaming_func(x, y):\n",
    "    if y == '':\n",
    "        return f'{x}'\n",
    "    else:\n",
    "        return f'{x}_{int(y)}'\n",
    "\n",
    "\n",
    "interpolated_training_data.columns = [_renaming_func(\n",
    "    x, y) for x, y in interpolated_training_data.columns]\n",
    "interpolated_training_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting only on both WV\n",
    "\n",
    "# Create Features & Normalize the fitting params\n",
    "interpolated_training_data['Bias Ratio'] = interpolated_training_data['alpha0_1'] / interpolated_training_data['alpha0_2'] \n",
    "\n",
    "X = interpolated_training_data[['Bias Ratio', 'alpha1_1', 'alpha1_2', 'alpha2_1', 'alpha2_2', 'alpha3_1', 'alpha3_2']].to_numpy()\n",
    "alpha_scaler = preprocessing.StandardScaler().fit(X)\n",
    "X = alpha_scaler.transform(X)\n",
    "\n",
    "y = interpolated_training_data[['Fetal Mu_a']].to_numpy().flatten()\n",
    "# y = interpolated_training_data[['Maternal Mu_a']].to_numpy().flatten()\n",
    "y_scaler = preprocessing.StandardScaler().fit(y.reshape(-1, 1))\n",
    "y = y_scaler.transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "random_indices = rng.choice(np.arange(y.size), size=y.size, replace=False)\n",
    "training_count = int(y.size * 1)  # 80% Training Data\n",
    "training_indices = random_indices[:training_count]\n",
    "test_indices = random_indices[training_count:]\n",
    "\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "X_test, y_test = X[test_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rraiyan/cybercat/lib/python3.8/site-packages/sklearn/gaussian_process/_gpr.py:629: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.03**2 * Matern(length_scale=5.21e-05, nu=1.5)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kernel = 1 * kernels.RBF(length_scale=1.0, length_scale_bounds=(1e-4, 1e1))\n",
    "kernel = 1 * kernels.Matern(length_scale=1.0, length_scale_bounds=(1e-6, 1e-2))\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "gp.fit(X_train, y_train)\n",
    "gp.kernel_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PRedict on Simulation\n",
    "# X_test = X_train\n",
    "# y_test = y_train\n",
    "# mean_prediction, std_prediction = gp.predict(X_test, return_std=True)\n",
    "# mae = np.abs(mean_prediction - y_test)\n",
    "# mse = np.square(mean_prediction - y_test)\n",
    "# df = pd.DataFrame(\n",
    "#     {\n",
    "#         'True a0' : X_test[:, 0],\n",
    "#         'True a1' : X_test[:, 1],\n",
    "#         'True a2' : X_test[:, 2],\n",
    "#         'True a3' : X_test[:, 3],\n",
    "#         'True y'  : y_test,\n",
    "#         'Prediction' : mean_prediction,\n",
    "#         'Confidence' : std_prediction,\n",
    "#         'MAE(%)' : mae * 100,\n",
    "#         'MSE(%)' : mse * 100,\n",
    "#     }\n",
    "# )\n",
    "# pd.set_option('display.max_rows', 1200)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['MAE(%)'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_patient_ppg(ppg_data : pd.DataFrame, sample_number : Union[int, List], SDD = [15, 30, 45, 70, 100]) -> np.ndarray:\n",
    "    \"\"\"Prepare PPG data to be used directly into the GPR prediction.\n",
    "\n",
    "    Args:\n",
    "        ppg_data (pd.DataFrame): PPG data Dataframe. You can feed data directly from the the TFO_dataset package.\n",
    "        (Note: This should ideally be the optically normalized data)\n",
    "        sample_number (int): which sample to choose. You can either pass a single integer or an array\n",
    "        SDD (_type_, optional): Detector distances in TFO device(in mm). Defaults to SDD=[15, 30, 45, 70, 100].\n",
    "    \"\"\"\n",
    "    # The code is generalized to run on any array. make necessary conversions \n",
    "    if isinstance(sample_number, int):\n",
    "        sample_number = [sample_number]\n",
    "    \n",
    "    patient_features = []\n",
    "    for sample_point in sample_number:\n",
    "        # Pick a point in time\n",
    "        spatial_intensity = ppg_data.iloc[sample_point].copy()  # at 300s with 80Hz sampling freq.\n",
    "        spatial_intensity *=  pi * 4   # from unit area -> pi r^2 area -> match simulation\n",
    "        # Reshape ppg data to fit the format\n",
    "        spatial_intensity_wv1 = pd.DataFrame(data={\n",
    "            'SDD' : SDD,\n",
    "            'Intensity' : spatial_intensity.to_numpy()[:5]\n",
    "        })\n",
    "        spatial_intensity_wv2 = pd.DataFrame(data={\n",
    "            'SDD' : SDD,\n",
    "            'Intensity' : spatial_intensity.to_numpy()[5:]\n",
    "        })\n",
    "        alpha_wv1 = interpolate_exp_chunk(spatial_intensity_wv1, weights=[1.0, -2.0], return_alpha=True).flatten()\n",
    "        alpha_wv2 = interpolate_exp_chunk(spatial_intensity_wv2, weights=[1.0, -2.0], return_alpha=True).flatten()\n",
    "        patient_features.append([alpha_wv1[0]/alpha_wv2[0], alpha_wv1[1], alpha_wv2[1], alpha_wv1[2], alpha_wv2[2], alpha_wv1[3], alpha_wv2[3]])\n",
    "        \n",
    "    return np.array(patient_features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 'sp2021', 'Recovery')\n",
      "Non- normalized Features\n",
      "         f1        f2        f3          f4          f5          f6  \\\n",
      "0  0.923122 -7.762932 -8.427821  432.893302  469.738907 -918.699976   \n",
      "1  0.952615 -8.271182 -8.689905  460.316041  483.668010 -976.018981   \n",
      "2  0.913762 -7.339342 -8.054755  409.465326  449.041880 -869.461025   \n",
      "\n",
      "            f7  \n",
      "0  -996.130024  \n",
      "1 -1025.140742  \n",
      "2  -952.596061  \n"
     ]
    }
   ],
   "source": [
    "# Predict on reallife data\n",
    "sheep_id = 23\n",
    "data = TFO_dataset.SheepData('iq_demod_optical').get(sheep_id)\n",
    "print(TFO_dataset.SheepData('iq_demod_optical').get_tuple(sheep_id))\n",
    "\n",
    "features = prepare_patient_ppg(data, [1000, 2000, 3000])\n",
    "\n",
    "# Create a DF for better viz.\n",
    "print('Non- normalized Features')\n",
    "feature_names = [f'f{i + 1}' for i in range(7)]\n",
    "feature_df = pd.DataFrame(columns=feature_names, data=features)\n",
    "print(feature_df)\n",
    "features = alpha_scaler.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07]\n",
      " [0.07]\n",
      " [0.07]]\n",
      "[[1.06042771 0.         0.        ]\n",
      " [0.         1.06042771 0.        ]\n",
      " [0.         0.         1.06042771]]\n"
     ]
    }
   ],
   "source": [
    "estimate, confidence = gp.predict(features, return_cov=True)\n",
    "# estimate, confidence = gp.predict(X_train[0, :].reshape(1, -1), return_std=True)\n",
    "print(y_scaler.inverse_transform(np.array(estimate).reshape(-1, 1)))\n",
    "print(confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha means (From training) : [  0.94525161   1.16190502   1.16155528 -33.70954645 -35.10835354\n",
      "  49.53349681  52.43872191]\n",
      "alpha variance (From training) : [1.35868092e-02 2.25335223e-03 1.01410463e-02 4.60485405e+00\n",
      " 3.14067449e+01 1.46363908e+01 1.03779584e+02]\n"
     ]
    }
   ],
   "source": [
    "print(f'alpha means (From training) : {alpha_scaler.mean_}')\n",
    "print(f'alpha variance (From training) : {alpha_scaler.var_}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cybercat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
