{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set my GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35929014, 47)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Maternal Wall Thickness</th>\n",
       "      <th>Fetal Radius</th>\n",
       "      <th>Fetal Displacement</th>\n",
       "      <th>Maternal Hb Concentration</th>\n",
       "      <th>Maternal Saturation</th>\n",
       "      <th>Fetal Hb Concentration</th>\n",
       "      <th>Fetal Saturation</th>\n",
       "      <th>10.0_1.0</th>\n",
       "      <th>15.0_1.0</th>\n",
       "      <th>19.0_1.0</th>\n",
       "      <th>...</th>\n",
       "      <th>55.0_2.0</th>\n",
       "      <th>59.0_2.0</th>\n",
       "      <th>64.0_2.0</th>\n",
       "      <th>68.0_2.0</th>\n",
       "      <th>72.0_2.0</th>\n",
       "      <th>77.0_2.0</th>\n",
       "      <th>81.0_2.0</th>\n",
       "      <th>86.0_2.0</th>\n",
       "      <th>90.0_2.0</th>\n",
       "      <th>94.0_2.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43923</th>\n",
       "      <td>2.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10.725</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>6.001589e-15</td>\n",
       "      <td>5.848743e-15</td>\n",
       "      <td>2.763929e-16</td>\n",
       "      <td>1.500358e-17</td>\n",
       "      <td>3.398607e-18</td>\n",
       "      <td>1.015295e-18</td>\n",
       "      <td>7.130597e-20</td>\n",
       "      <td>8.710054e-22</td>\n",
       "      <td>4.012456e-20</td>\n",
       "      <td>2.355748e-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43924</th>\n",
       "      <td>2.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10.725</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>5.893694e-15</td>\n",
       "      <td>5.768395e-15</td>\n",
       "      <td>2.710312e-16</td>\n",
       "      <td>1.463642e-17</td>\n",
       "      <td>3.212955e-18</td>\n",
       "      <td>9.575111e-19</td>\n",
       "      <td>6.831444e-20</td>\n",
       "      <td>8.155644e-22</td>\n",
       "      <td>3.835888e-20</td>\n",
       "      <td>2.151092e-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43925</th>\n",
       "      <td>2.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10.725</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>5.789363e-15</td>\n",
       "      <td>5.689761e-15</td>\n",
       "      <td>2.658178e-16</td>\n",
       "      <td>1.428727e-17</td>\n",
       "      <td>3.039417e-18</td>\n",
       "      <td>9.030257e-19</td>\n",
       "      <td>6.545149e-20</td>\n",
       "      <td>7.646231e-22</td>\n",
       "      <td>3.667090e-20</td>\n",
       "      <td>1.964215e-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43926</th>\n",
       "      <td>2.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10.725</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>5.688417e-15</td>\n",
       "      <td>5.612798e-15</td>\n",
       "      <td>2.607455e-16</td>\n",
       "      <td>1.395518e-17</td>\n",
       "      <td>2.877101e-18</td>\n",
       "      <td>8.516513e-19</td>\n",
       "      <td>6.271132e-20</td>\n",
       "      <td>7.177746e-22</td>\n",
       "      <td>3.505719e-20</td>\n",
       "      <td>1.793575e-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43927</th>\n",
       "      <td>2.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10.725</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>5.590720e-15</td>\n",
       "      <td>5.537444e-15</td>\n",
       "      <td>2.558081e-16</td>\n",
       "      <td>1.363931e-17</td>\n",
       "      <td>2.725183e-18</td>\n",
       "      <td>8.032090e-19</td>\n",
       "      <td>6.008842e-20</td>\n",
       "      <td>6.746510e-22</td>\n",
       "      <td>3.351450e-20</td>\n",
       "      <td>1.637759e-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Maternal Wall Thickness  Fetal Radius  Fetal Displacement  \\\n",
       "43923                      2.0          50.0                 5.0   \n",
       "43924                      2.0          50.0                 5.0   \n",
       "43925                      2.0          50.0                 5.0   \n",
       "43926                      2.0          50.0                 5.0   \n",
       "43927                      2.0          50.0                 5.0   \n",
       "\n",
       "       Maternal Hb Concentration  Maternal Saturation  Fetal Hb Concentration  \\\n",
       "43923                       11.0                  0.9                  10.725   \n",
       "43924                       11.0                  0.9                  10.725   \n",
       "43925                       11.0                  0.9                  10.725   \n",
       "43926                       11.0                  0.9                  10.725   \n",
       "43927                       11.0                  0.9                  10.725   \n",
       "\n",
       "       Fetal Saturation  10.0_1.0  15.0_1.0  19.0_1.0  ...      55.0_2.0  \\\n",
       "43923              0.10  0.000041  0.000005  0.000001  ...  6.001589e-15   \n",
       "43924              0.15  0.000041  0.000005  0.000001  ...  5.893694e-15   \n",
       "43925              0.20  0.000041  0.000005  0.000001  ...  5.789363e-15   \n",
       "43926              0.25  0.000041  0.000005  0.000001  ...  5.688417e-15   \n",
       "43927              0.30  0.000041  0.000005  0.000001  ...  5.590720e-15   \n",
       "\n",
       "           59.0_2.0      64.0_2.0      68.0_2.0      72.0_2.0      77.0_2.0  \\\n",
       "43923  5.848743e-15  2.763929e-16  1.500358e-17  3.398607e-18  1.015295e-18   \n",
       "43924  5.768395e-15  2.710312e-16  1.463642e-17  3.212955e-18  9.575111e-19   \n",
       "43925  5.689761e-15  2.658178e-16  1.428727e-17  3.039417e-18  9.030257e-19   \n",
       "43926  5.612798e-15  2.607455e-16  1.395518e-17  2.877101e-18  8.516513e-19   \n",
       "43927  5.537444e-15  2.558081e-16  1.363931e-17  2.725183e-18  8.032090e-19   \n",
       "\n",
       "           81.0_2.0      86.0_2.0      90.0_2.0      94.0_2.0  \n",
       "43923  7.130597e-20  8.710054e-22  4.012456e-20  2.355748e-22  \n",
       "43924  6.831444e-20  8.155644e-22  3.835888e-20  2.151092e-22  \n",
       "43925  6.545149e-20  7.646231e-22  3.667090e-20  1.964215e-22  \n",
       "43926  6.271132e-20  7.177746e-22  3.505719e-20  1.793575e-22  \n",
       "43927  6.008842e-20  6.746510e-22  3.351450e-20  1.637759e-22  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = r'/home/rlfowler/Documents/research/tfo_inverse_modelling/Randalls Folder/data/randall_data_intensities.pkl'\n",
    "#CONFIG_PATH = r'/home/rlfowler/Documents/research/tfo_sim/data/compiled_intensity/randall_data.json'\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_pickle(DATA_PATH)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scale y\n",
    "from sklearn import preprocessing\n",
    "\n",
    "y_columns = data.columns[:7]#[\"Fetal Saturation\"]\n",
    "x_columns = data.columns[7:]\n",
    "\n",
    "y_scaler = preprocessing.StandardScaler()\n",
    "data[y_columns] = y_scaler.fit_transform(data[y_columns])\n",
    "\n",
    "x_scaler = preprocessing.StandardScaler()\n",
    "data[x_columns] = x_scaler.fit_transform(data[x_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (5748642, 47)\n",
      "Test data shape: (1437161, 47)\n",
      "x_train columns: ['10.0_1.0', '15.0_1.0', '19.0_1.0', '24.0_1.0', '28.0_1.0', '33.0_1.0', '37.0_1.0', '41.0_1.0', '46.0_1.0', '50.0_1.0', '55.0_1.0', '59.0_1.0', '64.0_1.0', '68.0_1.0', '72.0_1.0', '77.0_1.0', '81.0_1.0', '86.0_1.0', '90.0_1.0', '94.0_1.0', '10.0_2.0', '15.0_2.0', '19.0_2.0', '24.0_2.0', '28.0_2.0', '33.0_2.0', '37.0_2.0', '41.0_2.0', '46.0_2.0', '50.0_2.0', '55.0_2.0', '59.0_2.0', '64.0_2.0', '68.0_2.0', '72.0_2.0', '77.0_2.0', '81.0_2.0', '86.0_2.0', '90.0_2.0', '94.0_2.0']\n",
      "y_train columns: ['Maternal Wall Thickness', 'Fetal Radius', 'Fetal Displacement', 'Maternal Hb Concentration', 'Maternal Saturation', 'Fetal Hb Concentration', 'Fetal Saturation']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "RANDOM_SEED = 42    # Set the random seed for reproducibility\n",
    "SAMPLE_SIZE = 0.2   # Set the sample size from the data\n",
    "TEST_SIZE = 0.2     # Set the test size for the train/test split\n",
    "\n",
    "# Split the data into training and testing sets # Might want to remove randomness for param selection\n",
    "train_data, test_data = train_test_split(data.sample(frac=SAMPLE_SIZE, random_state=RANDOM_SEED) , test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "# Split the data into input and output\n",
    "x_train = train_data.iloc[:, 7:]\n",
    "x_test = test_data.iloc[:, 7:]\n",
    "y_train = train_data.iloc[:,:7]\n",
    "y_test = test_data.iloc[:,:7]\n",
    "print(f\"x_train columns: {x_train.columns.tolist()}\")\n",
    "print(f\"y_train columns: {y_train.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "from typing import Optional, List\n",
    "from torch.optim import SGD, Optimizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "EPOCH = 1 #100\n",
    "BATCH_SIZE = 32 #128, 32\n",
    "LR = 0.001\n",
    "\n",
    "# Create a dataloader\n",
    "train_dataset = TensorDataset(torch.tensor(x_train.values, dtype=torch.float32), torch.tensor(y_train.values, dtype=torch.float32))\n",
    "test_dataset = TensorDataset(torch.tensor(x_test.values, dtype=torch.float32), torch.tensor(y_test.values, dtype=torch.float32))\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Create a neural network\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    layers: List[nn.Module] = []     # List of layers\n",
    "    model: nn.Module            # The model\n",
    "    training_loss: List[float] = []  # Training loss\n",
    "    validation_loss: List[float] = []# Validation loss\n",
    "\n",
    "    def __init__(self, node_count: List[int], dropout_rates: Optional[List[float]]):\n",
    "        super(Net, self).__init__()\n",
    "        #self.layers = []\n",
    "        for indx, n_nodes in enumerate(node_count[:-2]):\n",
    "            self.layers.append(nn.Linear(n_nodes, node_count[indx+1]))\n",
    "            self.layers.append(nn.BatchNorm1d(node_count[indx+1]))\n",
    "            if dropout_rates:\n",
    "                self.layers.append(nn.Dropout(dropout_rates[indx]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Linear(node_count[-2], node_count[-1]))   # Output layer\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "        self.model.cuda()\n",
    "\n",
    "    def _default_optimizer(self):\n",
    "        return SGD(self.model.parameters(), lr=LR, momentum=0.9)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def train(self, train_loader, criterion:nn.Module = nn.MSELoss(), optimizer:Optimizer = None, verbose=False):\n",
    "        if optimizer is None:\n",
    "            optimizer = self._default_optimizer()\n",
    "        self.model.train()  # Set the model to training mode\n",
    "        print(\"Starting training...\")\n",
    "        print(f\"Length of train_loader: {len(train_loader)}\")\n",
    "        for epoch in range(EPOCH):\n",
    "            running_loss = 0.0\n",
    "            for step, (batch_x, batch_y) in enumerate(train_loader):\n",
    "                batch_x = batch_x.cuda()    # Send the data to the GPU\n",
    "                batch_y = batch_y.cuda()    # Send the data to the GPU\n",
    "\n",
    "                output = self.forward(batch_x)                      # Forward pass\n",
    "                loss = criterion(output, batch_y)                   # Compute the loss\n",
    "                optimizer.zero_grad()                               # Zero the gradients to prevent accumulation\n",
    "                loss.backward()                                     # Backpropagation to compute the gradients                   \n",
    "                optimizer.step()                                    # Update the weights, apply gradients        \n",
    "\n",
    "                running_loss += loss.item()                         # Add the loss to the running loss\n",
    "\n",
    "                if verbose and step % BATCH_SIZE == 0:                    # Print stats every 100 steps\n",
    "                    print(f\"Epoch: {epoch}, Step: {step}, Loss: {loss.item()}\")\n",
    "                    # accuracy = torch.sum(torch.abs(output - batch_y) < 0.1).item() / len(batch_y)\n",
    "                    # print(f\"Epoch: {epoch}, Loss: {loss.item()}, Accuracy: {accuracy}\")\n",
    "            self.training_loss.append(running_loss/len(train_loader))  # Append the average loss to the training loss list\n",
    "\n",
    "    def predict(self, test_loader, criterion:nn.Module = nn.MSELoss(), verbose=False):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        accuracy = 0.0\n",
    "        print(\"Starting prediction...\")\n",
    "        for step, (batch_x, batch_y) in enumerate(test_loader):\n",
    "            # Send the data to the GPU\n",
    "            batch_x = batch_x.cuda()\n",
    "            batch_y = batch_y.cuda()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = self.model(batch_x)\n",
    "                loss = criterion(output, batch_y)\n",
    "\n",
    "            # acc = torch.sum(torch.abs(output - batch_y) < 0.1).item() / len(batch_y)\n",
    "            # accuracy += acc\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Loss: {loss.item()}\")#, Accuracy: {acc}\")\n",
    "\n",
    "        accuracy /= len(test_loader)\n",
    "        running_loss /= len(test_loader)\n",
    "        self.validation_loss.append(running_loss)\n",
    "        return running_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Length of train_loader: 179646\n",
      "Epoch: 0, Step: 0, Loss: 1.4797519445419312\n",
      "Epoch: 0, Step: 32, Loss: 1.2493771314620972\n",
      "Epoch: 0, Step: 64, Loss: 1.1113189458847046\n",
      "Epoch: 0, Step: 96, Loss: 1.0035979747772217\n",
      "Epoch: 0, Step: 128, Loss: 1.0755716562271118\n",
      "Epoch: 0, Step: 160, Loss: 1.1304600238800049\n",
      "Epoch: 0, Step: 192, Loss: 0.9769256711006165\n",
      "Epoch: 0, Step: 224, Loss: 0.9063306450843811\n",
      "Epoch: 0, Step: 256, Loss: 0.9072474241256714\n",
      "Epoch: 0, Step: 288, Loss: 0.9785010814666748\n",
      "Epoch: 0, Step: 320, Loss: 0.8803388476371765\n",
      "Epoch: 0, Step: 352, Loss: 1.03053617477417\n",
      "Epoch: 0, Step: 384, Loss: 0.989343523979187\n",
      "Epoch: 0, Step: 416, Loss: 0.9920421838760376\n",
      "Epoch: 0, Step: 448, Loss: 1.0441521406173706\n",
      "Epoch: 0, Step: 480, Loss: 0.9219022989273071\n",
      "Epoch: 0, Step: 512, Loss: 1.061523675918579\n",
      "Epoch: 0, Step: 544, Loss: 0.9827539920806885\n",
      "Epoch: 0, Step: 576, Loss: 0.9734272956848145\n",
      "Epoch: 0, Step: 608, Loss: 0.8508872389793396\n",
      "Epoch: 0, Step: 640, Loss: 0.9465962052345276\n",
      "Epoch: 0, Step: 672, Loss: 0.8760151863098145\n",
      "Epoch: 0, Step: 704, Loss: 1.0441802740097046\n",
      "Epoch: 0, Step: 736, Loss: 1.0526347160339355\n",
      "Epoch: 0, Step: 768, Loss: 0.9878810048103333\n",
      "Epoch: 0, Step: 800, Loss: 0.8409008383750916\n",
      "Epoch: 0, Step: 832, Loss: 0.9815312623977661\n",
      "Epoch: 0, Step: 864, Loss: 0.8586794137954712\n",
      "Epoch: 0, Step: 896, Loss: 0.9304983615875244\n",
      "Epoch: 0, Step: 928, Loss: 1.037825107574463\n",
      "Epoch: 0, Step: 960, Loss: 0.8838188052177429\n",
      "Epoch: 0, Step: 992, Loss: 1.0102237462997437\n",
      "Epoch: 0, Step: 1024, Loss: 0.9010770320892334\n",
      "Epoch: 0, Step: 1056, Loss: 0.9341526627540588\n",
      "Epoch: 0, Step: 1088, Loss: 0.9473360776901245\n",
      "Epoch: 0, Step: 1120, Loss: 0.9078116416931152\n",
      "Epoch: 0, Step: 1152, Loss: 0.9320359230041504\n",
      "Epoch: 0, Step: 1184, Loss: 0.8798823952674866\n",
      "Epoch: 0, Step: 1216, Loss: 0.9321311116218567\n",
      "Epoch: 0, Step: 1248, Loss: 1.0232751369476318\n",
      "Epoch: 0, Step: 1280, Loss: 0.8924272060394287\n",
      "Epoch: 0, Step: 1312, Loss: 0.9438881874084473\n",
      "Epoch: 0, Step: 1344, Loss: 0.9218465685844421\n",
      "Epoch: 0, Step: 1376, Loss: 0.9532071948051453\n",
      "Epoch: 0, Step: 1408, Loss: 0.9167338013648987\n",
      "Epoch: 0, Step: 1440, Loss: 0.9525537490844727\n",
      "Epoch: 0, Step: 1472, Loss: 0.8404404520988464\n",
      "Epoch: 0, Step: 1504, Loss: 0.9219375848770142\n",
      "Epoch: 0, Step: 1536, Loss: 0.8314610123634338\n",
      "Epoch: 0, Step: 1568, Loss: 0.8667116761207581\n",
      "Epoch: 0, Step: 1600, Loss: 0.9315597414970398\n",
      "Epoch: 0, Step: 1632, Loss: 0.9089290499687195\n",
      "Epoch: 0, Step: 1664, Loss: 0.8337633013725281\n",
      "Epoch: 0, Step: 1696, Loss: 0.9098749160766602\n",
      "Epoch: 0, Step: 1728, Loss: 0.8006020188331604\n",
      "Epoch: 0, Step: 1760, Loss: 0.8275411128997803\n",
      "Epoch: 0, Step: 1792, Loss: 0.9979982972145081\n",
      "Epoch: 0, Step: 1824, Loss: 0.950778067111969\n",
      "Epoch: 0, Step: 1856, Loss: 0.8369730114936829\n",
      "Epoch: 0, Step: 1888, Loss: 0.950478732585907\n",
      "Epoch: 0, Step: 1920, Loss: 0.8474534153938293\n",
      "Epoch: 0, Step: 1952, Loss: 0.9326817989349365\n",
      "Epoch: 0, Step: 1984, Loss: 0.9089360237121582\n",
      "Epoch: 0, Step: 2016, Loss: 0.8255969882011414\n",
      "Epoch: 0, Step: 2048, Loss: 0.885261595249176\n",
      "Epoch: 0, Step: 2080, Loss: 0.8495670557022095\n",
      "Epoch: 0, Step: 2112, Loss: 0.9536738991737366\n",
      "Epoch: 0, Step: 2144, Loss: 0.8856058716773987\n",
      "Epoch: 0, Step: 2176, Loss: 1.005839228630066\n",
      "Epoch: 0, Step: 2208, Loss: 0.8182572722434998\n",
      "Epoch: 0, Step: 2240, Loss: 0.8918451070785522\n",
      "Epoch: 0, Step: 2272, Loss: 0.8927297592163086\n",
      "Epoch: 0, Step: 2304, Loss: 1.0088750123977661\n",
      "Epoch: 0, Step: 2336, Loss: 0.8555309176445007\n",
      "Epoch: 0, Step: 2368, Loss: 0.8947966694831848\n",
      "Epoch: 0, Step: 2400, Loss: 0.8271640539169312\n",
      "Epoch: 0, Step: 2432, Loss: 0.8939367532730103\n",
      "Epoch: 0, Step: 2464, Loss: 0.8188251852989197\n",
      "Epoch: 0, Step: 2496, Loss: 0.9177659153938293\n",
      "Epoch: 0, Step: 2528, Loss: 0.820006787776947\n",
      "Epoch: 0, Step: 2560, Loss: 0.8945329785346985\n",
      "Epoch: 0, Step: 2592, Loss: 0.7576786279678345\n",
      "Epoch: 0, Step: 2624, Loss: 0.9393568634986877\n",
      "Epoch: 0, Step: 2656, Loss: 0.8065991997718811\n",
      "Epoch: 0, Step: 2688, Loss: 0.8750585317611694\n",
      "Epoch: 0, Step: 2720, Loss: 0.9493204951286316\n",
      "Epoch: 0, Step: 2752, Loss: 0.835023820400238\n",
      "Epoch: 0, Step: 2784, Loss: 0.8645389080047607\n",
      "Epoch: 0, Step: 2816, Loss: 0.9329760670661926\n",
      "Epoch: 0, Step: 2848, Loss: 0.8615487217903137\n",
      "Epoch: 0, Step: 2880, Loss: 0.9441892504692078\n",
      "Epoch: 0, Step: 2912, Loss: 0.964425265789032\n",
      "Epoch: 0, Step: 2944, Loss: 0.8442035913467407\n",
      "Epoch: 0, Step: 2976, Loss: 0.8891372680664062\n",
      "Epoch: 0, Step: 3008, Loss: 0.8108956813812256\n",
      "Epoch: 0, Step: 3040, Loss: 0.7665164470672607\n",
      "Epoch: 0, Step: 3072, Loss: 0.8098322153091431\n",
      "Epoch: 0, Step: 3104, Loss: 1.001556634902954\n",
      "Epoch: 0, Step: 3136, Loss: 0.82249915599823\n",
      "Epoch: 0, Step: 3168, Loss: 0.9149782657623291\n",
      "Epoch: 0, Step: 3200, Loss: 0.8564006090164185\n",
      "Epoch: 0, Step: 3232, Loss: 0.8807073831558228\n",
      "Epoch: 0, Step: 3264, Loss: 0.8743816614151001\n",
      "Epoch: 0, Step: 3296, Loss: 0.9236748814582825\n",
      "Epoch: 0, Step: 3328, Loss: 1.0252524614334106\n",
      "Epoch: 0, Step: 3360, Loss: 0.7544133067131042\n",
      "Epoch: 0, Step: 3392, Loss: 0.963845431804657\n",
      "Epoch: 0, Step: 3424, Loss: 0.998369574546814\n",
      "Epoch: 0, Step: 3456, Loss: 0.9678923487663269\n",
      "Epoch: 0, Step: 3488, Loss: 0.825509250164032\n",
      "Epoch: 0, Step: 3520, Loss: 0.8833930492401123\n",
      "Epoch: 0, Step: 3552, Loss: 0.8613389730453491\n",
      "Epoch: 0, Step: 3584, Loss: 0.9567116498947144\n",
      "Epoch: 0, Step: 3616, Loss: 0.7971593737602234\n",
      "Epoch: 0, Step: 3648, Loss: 0.9036117792129517\n",
      "Epoch: 0, Step: 3680, Loss: 0.89319908618927\n",
      "Epoch: 0, Step: 3712, Loss: 0.9448599815368652\n",
      "Epoch: 0, Step: 3744, Loss: 0.8817040920257568\n",
      "Epoch: 0, Step: 3776, Loss: 0.8909474015235901\n",
      "Epoch: 0, Step: 3808, Loss: 0.8209272623062134\n",
      "Epoch: 0, Step: 3840, Loss: 0.9157517552375793\n",
      "Epoch: 0, Step: 3872, Loss: 0.8333652019500732\n",
      "Epoch: 0, Step: 3904, Loss: 0.8552592396736145\n",
      "Epoch: 0, Step: 3936, Loss: 0.8104807138442993\n",
      "Epoch: 0, Step: 3968, Loss: 0.8549953103065491\n",
      "Epoch: 0, Step: 4000, Loss: 0.8196712732315063\n",
      "Epoch: 0, Step: 4032, Loss: 0.9791290760040283\n",
      "Epoch: 0, Step: 4064, Loss: 0.9407862424850464\n",
      "Epoch: 0, Step: 4096, Loss: 0.8244145512580872\n",
      "Epoch: 0, Step: 4128, Loss: 0.9332320690155029\n",
      "Epoch: 0, Step: 4160, Loss: 0.8285518884658813\n",
      "Epoch: 0, Step: 4192, Loss: 0.9575750827789307\n",
      "Epoch: 0, Step: 4224, Loss: 0.8346386551856995\n",
      "Epoch: 0, Step: 4256, Loss: 0.8245856761932373\n",
      "Epoch: 0, Step: 4288, Loss: 1.0053961277008057\n",
      "Epoch: 0, Step: 4320, Loss: 0.810590922832489\n",
      "Epoch: 0, Step: 4352, Loss: 0.9388028383255005\n",
      "Epoch: 0, Step: 4384, Loss: 0.8212268352508545\n",
      "Epoch: 0, Step: 4416, Loss: 0.8399505019187927\n",
      "Epoch: 0, Step: 4448, Loss: 0.8856385350227356\n",
      "Epoch: 0, Step: 4480, Loss: 0.8897354006767273\n",
      "Epoch: 0, Step: 4512, Loss: 0.8452451825141907\n",
      "Epoch: 0, Step: 4544, Loss: 0.834091305732727\n",
      "Epoch: 0, Step: 4576, Loss: 0.8576620817184448\n",
      "Epoch: 0, Step: 4608, Loss: 0.9360830187797546\n",
      "Epoch: 0, Step: 4640, Loss: 0.979357898235321\n",
      "Epoch: 0, Step: 4672, Loss: 0.7772047519683838\n",
      "Epoch: 0, Step: 4704, Loss: 0.859695553779602\n",
      "Epoch: 0, Step: 4736, Loss: 0.8838556408882141\n",
      "Epoch: 0, Step: 4768, Loss: 0.9395659565925598\n",
      "Epoch: 0, Step: 4800, Loss: 0.9082114100456238\n",
      "Epoch: 0, Step: 4832, Loss: 0.929265558719635\n",
      "Epoch: 0, Step: 4864, Loss: 0.802743673324585\n",
      "Epoch: 0, Step: 4896, Loss: 0.7971303462982178\n",
      "Epoch: 0, Step: 4928, Loss: 0.8453918695449829\n",
      "Epoch: 0, Step: 4960, Loss: 0.9413123726844788\n",
      "Epoch: 0, Step: 4992, Loss: 0.8466542363166809\n",
      "Epoch: 0, Step: 5024, Loss: 0.9074755907058716\n",
      "Epoch: 0, Step: 5056, Loss: 0.9029315114021301\n",
      "Epoch: 0, Step: 5088, Loss: 0.7938492894172668\n",
      "Epoch: 0, Step: 5120, Loss: 0.9775839447975159\n",
      "Epoch: 0, Step: 5152, Loss: 0.8898739814758301\n",
      "Epoch: 0, Step: 5184, Loss: 0.8782162666320801\n",
      "Epoch: 0, Step: 5216, Loss: 0.8407928347587585\n",
      "Epoch: 0, Step: 5248, Loss: 0.7764739394187927\n",
      "Epoch: 0, Step: 5280, Loss: 0.7146759629249573\n",
      "Epoch: 0, Step: 5312, Loss: 0.9669660329818726\n",
      "Epoch: 0, Step: 5344, Loss: 0.8164700269699097\n",
      "Epoch: 0, Step: 5376, Loss: 0.765120267868042\n",
      "Epoch: 0, Step: 5408, Loss: 0.9603447914123535\n",
      "Epoch: 0, Step: 5440, Loss: 0.9216207265853882\n",
      "Epoch: 0, Step: 5472, Loss: 0.869205117225647\n",
      "Epoch: 0, Step: 5504, Loss: 0.9148880243301392\n",
      "Epoch: 0, Step: 5536, Loss: 0.8352669477462769\n",
      "Epoch: 0, Step: 5568, Loss: 0.9060775637626648\n",
      "Epoch: 0, Step: 5600, Loss: 0.8425421118736267\n",
      "Epoch: 0, Step: 5632, Loss: 0.8140751123428345\n",
      "Epoch: 0, Step: 5664, Loss: 0.8633508682250977\n",
      "Epoch: 0, Step: 5696, Loss: 0.929939329624176\n",
      "Epoch: 0, Step: 5728, Loss: 0.7946622371673584\n",
      "Epoch: 0, Step: 5760, Loss: 0.9258330464363098\n",
      "Epoch: 0, Step: 5792, Loss: 0.8564227819442749\n",
      "Epoch: 0, Step: 5824, Loss: 0.7753958106040955\n",
      "Epoch: 0, Step: 5856, Loss: 0.839986264705658\n",
      "Epoch: 0, Step: 5888, Loss: 0.9542840123176575\n",
      "Epoch: 0, Step: 5920, Loss: 0.8229448795318604\n",
      "Epoch: 0, Step: 5952, Loss: 0.7463281750679016\n",
      "Epoch: 0, Step: 5984, Loss: 0.9482491612434387\n",
      "Epoch: 0, Step: 6016, Loss: 0.9692862033843994\n",
      "Epoch: 0, Step: 6048, Loss: 0.8914052844047546\n",
      "Epoch: 0, Step: 6080, Loss: 0.8636859059333801\n",
      "Epoch: 0, Step: 6112, Loss: 0.866971492767334\n",
      "Epoch: 0, Step: 6144, Loss: 0.900887131690979\n",
      "Epoch: 0, Step: 6176, Loss: 0.8716825246810913\n",
      "Epoch: 0, Step: 6208, Loss: 0.8469601273536682\n",
      "Epoch: 0, Step: 6240, Loss: 0.922283947467804\n",
      "Epoch: 0, Step: 6272, Loss: 0.8862642049789429\n",
      "Epoch: 0, Step: 6304, Loss: 0.9215090870857239\n",
      "Epoch: 0, Step: 6336, Loss: 0.8224765658378601\n",
      "Epoch: 0, Step: 6368, Loss: 0.972197413444519\n",
      "Epoch: 0, Step: 6400, Loss: 0.8638612627983093\n",
      "Epoch: 0, Step: 6432, Loss: 0.9646156430244446\n",
      "Epoch: 0, Step: 6464, Loss: 0.9131066203117371\n",
      "Epoch: 0, Step: 6496, Loss: 0.7978668808937073\n",
      "Epoch: 0, Step: 6528, Loss: 0.7160140872001648\n",
      "Epoch: 0, Step: 6560, Loss: 0.8865411877632141\n",
      "Epoch: 0, Step: 6592, Loss: 0.8786965012550354\n",
      "Epoch: 0, Step: 6624, Loss: 0.8430391550064087\n",
      "Epoch: 0, Step: 6656, Loss: 0.8104929327964783\n",
      "Epoch: 0, Step: 6688, Loss: 0.9362499117851257\n",
      "Epoch: 0, Step: 6720, Loss: 0.9240310192108154\n",
      "Epoch: 0, Step: 6752, Loss: 0.9273374080657959\n",
      "Epoch: 0, Step: 6784, Loss: 0.7903975248336792\n",
      "Epoch: 0, Step: 6816, Loss: 0.8677908182144165\n",
      "Epoch: 0, Step: 6848, Loss: 0.7952010035514832\n",
      "Epoch: 0, Step: 6880, Loss: 0.9246314167976379\n",
      "Epoch: 0, Step: 6912, Loss: 0.9534476399421692\n",
      "Epoch: 0, Step: 6944, Loss: 0.828597366809845\n",
      "Epoch: 0, Step: 6976, Loss: 0.7856485843658447\n",
      "Epoch: 0, Step: 7008, Loss: 0.8308443427085876\n",
      "Epoch: 0, Step: 7040, Loss: 0.8768435120582581\n",
      "Epoch: 0, Step: 7072, Loss: 0.7663862109184265\n",
      "Epoch: 0, Step: 7104, Loss: 0.890730619430542\n",
      "Epoch: 0, Step: 7136, Loss: 0.8480662107467651\n",
      "Epoch: 0, Step: 7168, Loss: 0.845361590385437\n",
      "Epoch: 0, Step: 7200, Loss: 0.909966766834259\n",
      "Epoch: 0, Step: 7232, Loss: 0.8100236058235168\n",
      "Epoch: 0, Step: 7264, Loss: 0.7815917134284973\n",
      "Epoch: 0, Step: 7296, Loss: 0.8902166485786438\n",
      "Epoch: 0, Step: 7328, Loss: 0.8220794796943665\n",
      "Epoch: 0, Step: 7360, Loss: 0.8582678437232971\n",
      "Epoch: 0, Step: 7392, Loss: 0.8917528986930847\n",
      "Epoch: 0, Step: 7424, Loss: 0.9106718897819519\n",
      "Epoch: 0, Step: 7456, Loss: 0.8506655097007751\n",
      "Epoch: 0, Step: 7488, Loss: 0.7704653143882751\n",
      "Epoch: 0, Step: 7520, Loss: 0.8370174765586853\n",
      "Epoch: 0, Step: 7552, Loss: 0.8834711313247681\n",
      "Epoch: 0, Step: 7584, Loss: 0.8989730477333069\n",
      "Epoch: 0, Step: 7616, Loss: 0.9490952491760254\n",
      "Epoch: 0, Step: 7648, Loss: 0.8632031083106995\n",
      "Epoch: 0, Step: 7680, Loss: 0.8826203346252441\n",
      "Epoch: 0, Step: 7712, Loss: 0.9496700167655945\n",
      "Epoch: 0, Step: 7744, Loss: 0.8909511566162109\n",
      "Epoch: 0, Step: 7776, Loss: 0.8650537133216858\n",
      "Epoch: 0, Step: 7808, Loss: 0.6925831437110901\n",
      "Epoch: 0, Step: 7840, Loss: 0.9188544750213623\n",
      "Epoch: 0, Step: 7872, Loss: 0.8095665574073792\n",
      "Epoch: 0, Step: 7904, Loss: 0.942179799079895\n",
      "Epoch: 0, Step: 7936, Loss: 0.8935797810554504\n",
      "Epoch: 0, Step: 7968, Loss: 0.8534062504768372\n",
      "Epoch: 0, Step: 8000, Loss: 0.8339148163795471\n",
      "Epoch: 0, Step: 8032, Loss: 0.8004677891731262\n",
      "Epoch: 0, Step: 8064, Loss: 0.9436748623847961\n",
      "Epoch: 0, Step: 8096, Loss: 0.8187840580940247\n",
      "Epoch: 0, Step: 8128, Loss: 0.8361225724220276\n",
      "Epoch: 0, Step: 8160, Loss: 0.828481137752533\n",
      "Epoch: 0, Step: 8192, Loss: 0.8819825053215027\n",
      "Epoch: 0, Step: 8224, Loss: 0.8381253480911255\n",
      "Epoch: 0, Step: 8256, Loss: 0.8676327466964722\n",
      "Epoch: 0, Step: 8288, Loss: 0.9382637143135071\n",
      "Epoch: 0, Step: 8320, Loss: 0.717268705368042\n",
      "Epoch: 0, Step: 8352, Loss: 0.8336682915687561\n",
      "Epoch: 0, Step: 8384, Loss: 0.8890701532363892\n",
      "Epoch: 0, Step: 8416, Loss: 0.8351249098777771\n",
      "Epoch: 0, Step: 8448, Loss: 0.8553836345672607\n",
      "Epoch: 0, Step: 8480, Loss: 0.894922137260437\n",
      "Epoch: 0, Step: 8512, Loss: 0.8757884502410889\n",
      "Epoch: 0, Step: 8544, Loss: 0.8671278953552246\n",
      "Epoch: 0, Step: 8576, Loss: 0.8204758167266846\n",
      "Epoch: 0, Step: 8608, Loss: 0.8428844809532166\n",
      "Epoch: 0, Step: 8640, Loss: 0.89642733335495\n",
      "Epoch: 0, Step: 8672, Loss: 0.855959951877594\n",
      "Epoch: 0, Step: 8704, Loss: 0.8446214199066162\n",
      "Epoch: 0, Step: 8736, Loss: 0.8775697946548462\n",
      "Epoch: 0, Step: 8768, Loss: 0.8571627736091614\n",
      "Epoch: 0, Step: 8800, Loss: 0.7600694894790649\n",
      "Epoch: 0, Step: 8832, Loss: 0.7923201322555542\n",
      "Epoch: 0, Step: 8864, Loss: 0.8969160318374634\n",
      "Epoch: 0, Step: 8896, Loss: 0.8523314595222473\n",
      "Epoch: 0, Step: 8928, Loss: 0.6867640614509583\n",
      "Epoch: 0, Step: 8960, Loss: 0.9233036041259766\n",
      "Epoch: 0, Step: 8992, Loss: 0.90948885679245\n",
      "Epoch: 0, Step: 9024, Loss: 0.8336345553398132\n",
      "Epoch: 0, Step: 9056, Loss: 0.8475021719932556\n",
      "Epoch: 0, Step: 9088, Loss: 0.82900470495224\n",
      "Epoch: 0, Step: 9120, Loss: 0.6812557578086853\n",
      "Epoch: 0, Step: 9152, Loss: 0.9022191166877747\n",
      "Epoch: 0, Step: 9184, Loss: 0.7727572917938232\n",
      "Epoch: 0, Step: 9216, Loss: 0.8148423433303833\n",
      "Epoch: 0, Step: 9248, Loss: 0.8943146467208862\n",
      "Epoch: 0, Step: 9280, Loss: 0.810804009437561\n",
      "Epoch: 0, Step: 9312, Loss: 0.84841388463974\n",
      "Epoch: 0, Step: 9344, Loss: 0.8432639241218567\n",
      "Epoch: 0, Step: 9376, Loss: 0.9261174201965332\n",
      "Epoch: 0, Step: 9408, Loss: 0.8838807344436646\n",
      "Epoch: 0, Step: 9440, Loss: 0.8762009739875793\n",
      "Epoch: 0, Step: 9472, Loss: 0.8912560939788818\n",
      "Epoch: 0, Step: 9504, Loss: 0.8926336169242859\n",
      "Epoch: 0, Step: 9536, Loss: 0.94926917552948\n",
      "Epoch: 0, Step: 9568, Loss: 0.8667062520980835\n",
      "Epoch: 0, Step: 9600, Loss: 0.9111970067024231\n",
      "Epoch: 0, Step: 9632, Loss: 0.8399650454521179\n",
      "Epoch: 0, Step: 9664, Loss: 0.9046247601509094\n",
      "Epoch: 0, Step: 9696, Loss: 0.7788377404212952\n",
      "Epoch: 0, Step: 9728, Loss: 0.9022033214569092\n",
      "Epoch: 0, Step: 9760, Loss: 0.954433023929596\n",
      "Epoch: 0, Step: 9792, Loss: 0.8136317133903503\n",
      "Epoch: 0, Step: 9824, Loss: 0.834826648235321\n",
      "Epoch: 0, Step: 9856, Loss: 0.8553158044815063\n",
      "Epoch: 0, Step: 9888, Loss: 0.8664868474006653\n",
      "Epoch: 0, Step: 9920, Loss: 0.7764512896537781\n",
      "Epoch: 0, Step: 9952, Loss: 0.810825526714325\n",
      "Epoch: 0, Step: 9984, Loss: 0.799374520778656\n",
      "Epoch: 0, Step: 10016, Loss: 0.9287016987800598\n",
      "Epoch: 0, Step: 10048, Loss: 0.8212920427322388\n",
      "Epoch: 0, Step: 10080, Loss: 0.7444639801979065\n",
      "Epoch: 0, Step: 10112, Loss: 0.741435706615448\n",
      "Epoch: 0, Step: 10144, Loss: 0.8028285503387451\n",
      "Epoch: 0, Step: 10176, Loss: 0.9172794222831726\n",
      "Epoch: 0, Step: 10208, Loss: 0.804798424243927\n",
      "Epoch: 0, Step: 10240, Loss: 0.839978814125061\n",
      "Epoch: 0, Step: 10272, Loss: 0.7683994174003601\n",
      "Epoch: 0, Step: 10304, Loss: 0.8244289755821228\n",
      "Epoch: 0, Step: 10336, Loss: 0.8160375952720642\n",
      "Epoch: 0, Step: 10368, Loss: 0.8923419117927551\n",
      "Epoch: 0, Step: 10400, Loss: 0.8628183007240295\n",
      "Epoch: 0, Step: 10432, Loss: 0.7978238463401794\n",
      "Epoch: 0, Step: 10464, Loss: 0.8670316934585571\n",
      "Epoch: 0, Step: 10496, Loss: 0.7664032578468323\n",
      "Epoch: 0, Step: 10528, Loss: 0.8152912855148315\n",
      "Epoch: 0, Step: 10560, Loss: 0.8126534223556519\n",
      "Epoch: 0, Step: 10592, Loss: 0.836665689945221\n",
      "Epoch: 0, Step: 10624, Loss: 0.8828890919685364\n",
      "Epoch: 0, Step: 10656, Loss: 0.8781166672706604\n",
      "Epoch: 0, Step: 10688, Loss: 0.7965862154960632\n",
      "Epoch: 0, Step: 10720, Loss: 0.7589565515518188\n",
      "Epoch: 0, Step: 10752, Loss: 0.8525719046592712\n",
      "Epoch: 0, Step: 10784, Loss: 0.8281299471855164\n",
      "Epoch: 0, Step: 10816, Loss: 0.7704342603683472\n",
      "Epoch: 0, Step: 10848, Loss: 0.8149734735488892\n",
      "Epoch: 0, Step: 10880, Loss: 0.9117332696914673\n",
      "Epoch: 0, Step: 10912, Loss: 0.9326574802398682\n",
      "Epoch: 0, Step: 10944, Loss: 0.9077260494232178\n",
      "Epoch: 0, Step: 10976, Loss: 0.8547099232673645\n",
      "Epoch: 0, Step: 11008, Loss: 0.8228772282600403\n",
      "Epoch: 0, Step: 11040, Loss: 0.800993025302887\n",
      "Epoch: 0, Step: 11072, Loss: 0.9113057255744934\n",
      "Epoch: 0, Step: 11104, Loss: 0.8709644079208374\n",
      "Epoch: 0, Step: 11136, Loss: 0.8265682458877563\n",
      "Epoch: 0, Step: 11168, Loss: 0.8670913577079773\n",
      "Epoch: 0, Step: 11200, Loss: 0.7451941967010498\n",
      "Epoch: 0, Step: 11232, Loss: 0.9138582944869995\n",
      "Epoch: 0, Step: 11264, Loss: 0.9320552945137024\n",
      "Epoch: 0, Step: 11296, Loss: 0.7994367480278015\n",
      "Epoch: 0, Step: 11328, Loss: 0.8119996190071106\n",
      "Epoch: 0, Step: 11360, Loss: 0.8171441555023193\n",
      "Epoch: 0, Step: 11392, Loss: 0.9379239082336426\n",
      "Epoch: 0, Step: 11424, Loss: 0.8953884840011597\n",
      "Epoch: 0, Step: 11456, Loss: 0.899621307849884\n",
      "Epoch: 0, Step: 11488, Loss: 0.9018093943595886\n",
      "Epoch: 0, Step: 11520, Loss: 0.8111019730567932\n",
      "Epoch: 0, Step: 11552, Loss: 0.8906112909317017\n",
      "Epoch: 0, Step: 11584, Loss: 0.8849831223487854\n",
      "Epoch: 0, Step: 11616, Loss: 0.8969565033912659\n",
      "Epoch: 0, Step: 11648, Loss: 0.900942862033844\n",
      "Epoch: 0, Step: 11680, Loss: 0.8394249677658081\n",
      "Epoch: 0, Step: 11712, Loss: 0.8100993633270264\n",
      "Epoch: 0, Step: 11744, Loss: 0.6919122338294983\n",
      "Epoch: 0, Step: 11776, Loss: 0.804986298084259\n",
      "Epoch: 0, Step: 11808, Loss: 0.775327742099762\n",
      "Epoch: 0, Step: 11840, Loss: 0.7959532141685486\n",
      "Epoch: 0, Step: 11872, Loss: 0.827810525894165\n",
      "Epoch: 0, Step: 11904, Loss: 0.7216312885284424\n",
      "Epoch: 0, Step: 11936, Loss: 0.8364973664283752\n",
      "Epoch: 0, Step: 11968, Loss: 0.7071134448051453\n",
      "Epoch: 0, Step: 12000, Loss: 0.7801797389984131\n",
      "Epoch: 0, Step: 12032, Loss: 0.8394814133644104\n",
      "Epoch: 0, Step: 12064, Loss: 0.8774116039276123\n",
      "Epoch: 0, Step: 12096, Loss: 0.8845292329788208\n",
      "Epoch: 0, Step: 12128, Loss: 0.793749988079071\n",
      "Epoch: 0, Step: 12160, Loss: 0.8814512491226196\n",
      "Epoch: 0, Step: 12192, Loss: 0.7891247868537903\n",
      "Epoch: 0, Step: 12224, Loss: 0.8349395990371704\n",
      "Epoch: 0, Step: 12256, Loss: 0.8040149211883545\n",
      "Epoch: 0, Step: 12288, Loss: 0.8663958311080933\n",
      "Epoch: 0, Step: 12320, Loss: 0.8595643043518066\n",
      "Epoch: 0, Step: 12352, Loss: 0.7172127366065979\n",
      "Epoch: 0, Step: 12384, Loss: 0.851796567440033\n",
      "Epoch: 0, Step: 12416, Loss: 0.7740644812583923\n",
      "Epoch: 0, Step: 12448, Loss: 0.8202913999557495\n",
      "Epoch: 0, Step: 12480, Loss: 0.831918478012085\n",
      "Epoch: 0, Step: 12512, Loss: 0.7266618609428406\n",
      "Epoch: 0, Step: 12544, Loss: 0.7182310223579407\n",
      "Epoch: 0, Step: 12576, Loss: 0.7994357347488403\n",
      "Epoch: 0, Step: 12608, Loss: 0.8887852430343628\n",
      "Epoch: 0, Step: 12640, Loss: 0.7962501049041748\n",
      "Epoch: 0, Step: 12672, Loss: 0.8847348690032959\n",
      "Epoch: 0, Step: 12704, Loss: 0.8657848238945007\n",
      "Epoch: 0, Step: 12736, Loss: 0.9790831804275513\n",
      "Epoch: 0, Step: 12768, Loss: 0.8549004197120667\n",
      "Epoch: 0, Step: 12800, Loss: 0.9296315312385559\n",
      "Epoch: 0, Step: 12832, Loss: 0.838214635848999\n",
      "Epoch: 0, Step: 12864, Loss: 0.9133639335632324\n",
      "Epoch: 0, Step: 12896, Loss: 0.8372524976730347\n",
      "Epoch: 0, Step: 12928, Loss: 0.9311612844467163\n",
      "Epoch: 0, Step: 12960, Loss: 0.9552123546600342\n",
      "Epoch: 0, Step: 12992, Loss: 0.8985705375671387\n",
      "Epoch: 0, Step: 13024, Loss: 0.8276927471160889\n",
      "Epoch: 0, Step: 13056, Loss: 0.8718152046203613\n",
      "Epoch: 0, Step: 13088, Loss: 0.9120038151741028\n",
      "Epoch: 0, Step: 13120, Loss: 0.8829067349433899\n",
      "Epoch: 0, Step: 13152, Loss: 0.7645224332809448\n",
      "Epoch: 0, Step: 13184, Loss: 0.948524534702301\n",
      "Epoch: 0, Step: 13216, Loss: 0.803770124912262\n",
      "Epoch: 0, Step: 13248, Loss: 0.8899973630905151\n",
      "Epoch: 0, Step: 13280, Loss: 0.7879236936569214\n",
      "Epoch: 0, Step: 13312, Loss: 0.9191744923591614\n",
      "Epoch: 0, Step: 13344, Loss: 0.775979220867157\n",
      "Epoch: 0, Step: 13376, Loss: 0.8484230637550354\n",
      "Epoch: 0, Step: 13408, Loss: 0.8514835834503174\n",
      "Epoch: 0, Step: 13440, Loss: 0.8674359321594238\n",
      "Epoch: 0, Step: 13472, Loss: 0.7210116982460022\n",
      "Epoch: 0, Step: 13504, Loss: 0.8554456830024719\n",
      "Epoch: 0, Step: 13536, Loss: 0.7872995138168335\n",
      "Epoch: 0, Step: 13568, Loss: 0.7627284526824951\n",
      "Epoch: 0, Step: 13600, Loss: 0.8508659601211548\n",
      "Epoch: 0, Step: 13632, Loss: 0.8889676332473755\n",
      "Epoch: 0, Step: 13664, Loss: 0.9255979657173157\n",
      "Epoch: 0, Step: 13696, Loss: 0.9174152612686157\n",
      "Epoch: 0, Step: 13728, Loss: 0.9073625802993774\n",
      "Epoch: 0, Step: 13760, Loss: 0.974941074848175\n",
      "Epoch: 0, Step: 13792, Loss: 0.8613213300704956\n",
      "Epoch: 0, Step: 13824, Loss: 0.9366587400436401\n",
      "Epoch: 0, Step: 13856, Loss: 0.8398005366325378\n",
      "Epoch: 0, Step: 13888, Loss: 0.7774166464805603\n",
      "Epoch: 0, Step: 13920, Loss: 0.7728003263473511\n",
      "Epoch: 0, Step: 13952, Loss: 0.9053554534912109\n",
      "Epoch: 0, Step: 13984, Loss: 0.8137925267219543\n",
      "Epoch: 0, Step: 14016, Loss: 0.6908786296844482\n",
      "Epoch: 0, Step: 14048, Loss: 0.7746918201446533\n",
      "Epoch: 0, Step: 14080, Loss: 0.7987318634986877\n",
      "Epoch: 0, Step: 14112, Loss: 0.9043886065483093\n",
      "Epoch: 0, Step: 14144, Loss: 0.804273784160614\n",
      "Epoch: 0, Step: 14176, Loss: 0.852964460849762\n",
      "Epoch: 0, Step: 14208, Loss: 0.9046772718429565\n",
      "Epoch: 0, Step: 14240, Loss: 0.8444083333015442\n",
      "Epoch: 0, Step: 14272, Loss: 0.82560795545578\n",
      "Epoch: 0, Step: 14304, Loss: 0.8882980346679688\n",
      "Epoch: 0, Step: 14336, Loss: 0.8593427538871765\n",
      "Epoch: 0, Step: 14368, Loss: 0.7623181939125061\n",
      "Epoch: 0, Step: 14400, Loss: 0.9219627976417542\n",
      "Epoch: 0, Step: 14432, Loss: 0.7223962545394897\n",
      "Epoch: 0, Step: 14464, Loss: 0.7918978333473206\n",
      "Epoch: 0, Step: 14496, Loss: 0.7945517301559448\n",
      "Epoch: 0, Step: 14528, Loss: 0.9000688791275024\n",
      "Epoch: 0, Step: 14560, Loss: 0.8666417598724365\n",
      "Epoch: 0, Step: 14592, Loss: 0.8161783814430237\n",
      "Epoch: 0, Step: 14624, Loss: 0.7731078267097473\n",
      "Epoch: 0, Step: 14656, Loss: 0.8861499428749084\n",
      "Epoch: 0, Step: 14688, Loss: 0.8418976068496704\n",
      "Epoch: 0, Step: 14720, Loss: 0.930884838104248\n",
      "Epoch: 0, Step: 14752, Loss: 0.772051990032196\n",
      "Epoch: 0, Step: 14784, Loss: 0.8874217867851257\n",
      "Epoch: 0, Step: 14816, Loss: 0.9343022108078003\n",
      "Epoch: 0, Step: 14848, Loss: 0.8334071040153503\n",
      "Epoch: 0, Step: 14880, Loss: 0.8947829008102417\n",
      "Epoch: 0, Step: 14912, Loss: 0.9304516911506653\n",
      "Epoch: 0, Step: 14944, Loss: 0.7772015929222107\n",
      "Epoch: 0, Step: 14976, Loss: 0.8441820740699768\n",
      "Epoch: 0, Step: 15008, Loss: 0.8191258311271667\n",
      "Epoch: 0, Step: 15040, Loss: 0.9025067090988159\n",
      "Epoch: 0, Step: 15072, Loss: 0.8720987439155579\n",
      "Epoch: 0, Step: 15104, Loss: 0.8173738718032837\n",
      "Epoch: 0, Step: 15136, Loss: 0.9096301198005676\n",
      "Epoch: 0, Step: 15168, Loss: 0.9493716359138489\n",
      "Epoch: 0, Step: 15200, Loss: 0.8264400959014893\n",
      "Epoch: 0, Step: 15232, Loss: 0.7433463931083679\n",
      "Epoch: 0, Step: 15264, Loss: 0.9022939205169678\n",
      "Epoch: 0, Step: 15296, Loss: 0.8696349263191223\n",
      "Epoch: 0, Step: 15328, Loss: 0.8401925563812256\n",
      "Epoch: 0, Step: 15360, Loss: 0.7215779423713684\n",
      "Epoch: 0, Step: 15392, Loss: 1.0027281045913696\n",
      "Epoch: 0, Step: 15424, Loss: 0.8815479278564453\n",
      "Epoch: 0, Step: 15456, Loss: 0.8516336679458618\n",
      "Epoch: 0, Step: 15488, Loss: 0.8261253237724304\n",
      "Epoch: 0, Step: 15520, Loss: 0.8792169690132141\n",
      "Epoch: 0, Step: 15552, Loss: 0.7255151271820068\n",
      "Epoch: 0, Step: 15584, Loss: 0.781329333782196\n",
      "Epoch: 0, Step: 15616, Loss: 0.8198664784431458\n",
      "Epoch: 0, Step: 15648, Loss: 0.8723101615905762\n",
      "Epoch: 0, Step: 15680, Loss: 0.8545883893966675\n",
      "Epoch: 0, Step: 15712, Loss: 0.8144226670265198\n",
      "Epoch: 0, Step: 15744, Loss: 0.935991108417511\n",
      "Epoch: 0, Step: 15776, Loss: 0.8163715600967407\n",
      "Epoch: 0, Step: 15808, Loss: 0.8313400745391846\n",
      "Epoch: 0, Step: 15840, Loss: 0.8425958156585693\n",
      "Epoch: 0, Step: 15872, Loss: 0.8533428907394409\n",
      "Epoch: 0, Step: 15904, Loss: 0.8733789324760437\n",
      "Epoch: 0, Step: 15936, Loss: 0.8616694211959839\n",
      "Epoch: 0, Step: 15968, Loss: 0.881557822227478\n",
      "Epoch: 0, Step: 16000, Loss: 0.7719884514808655\n",
      "Epoch: 0, Step: 16032, Loss: 0.9482100605964661\n",
      "Epoch: 0, Step: 16064, Loss: 0.8190568089485168\n",
      "Epoch: 0, Step: 16096, Loss: 0.8123747706413269\n",
      "Epoch: 0, Step: 16128, Loss: 0.8074296116828918\n",
      "Epoch: 0, Step: 16160, Loss: 0.8175268769264221\n",
      "Epoch: 0, Step: 16192, Loss: 0.903060793876648\n",
      "Epoch: 0, Step: 16224, Loss: 0.8131839632987976\n",
      "Epoch: 0, Step: 16256, Loss: 0.9342153668403625\n",
      "Epoch: 0, Step: 16288, Loss: 0.8641532063484192\n",
      "Epoch: 0, Step: 16320, Loss: 0.7794108986854553\n",
      "Epoch: 0, Step: 16352, Loss: 0.8719038963317871\n",
      "Epoch: 0, Step: 16384, Loss: 0.6675133109092712\n",
      "Epoch: 0, Step: 16416, Loss: 0.8341964483261108\n",
      "Epoch: 0, Step: 16448, Loss: 0.8705964684486389\n",
      "Epoch: 0, Step: 16480, Loss: 0.8565558195114136\n",
      "Epoch: 0, Step: 16512, Loss: 0.8380760550498962\n",
      "Epoch: 0, Step: 16544, Loss: 0.7299582958221436\n",
      "Epoch: 0, Step: 16576, Loss: 0.7391632795333862\n",
      "Epoch: 0, Step: 16608, Loss: 0.9058605432510376\n",
      "Epoch: 0, Step: 16640, Loss: 0.8232942819595337\n",
      "Epoch: 0, Step: 16672, Loss: 0.890868067741394\n",
      "Epoch: 0, Step: 16704, Loss: 0.8728657364845276\n",
      "Epoch: 0, Step: 16736, Loss: 0.9920505285263062\n",
      "Epoch: 0, Step: 16768, Loss: 0.8190180659294128\n",
      "Epoch: 0, Step: 16800, Loss: 0.8095821738243103\n",
      "Epoch: 0, Step: 16832, Loss: 0.8171004056930542\n",
      "Epoch: 0, Step: 16864, Loss: 0.884200394153595\n",
      "Epoch: 0, Step: 16896, Loss: 0.7566468715667725\n",
      "Epoch: 0, Step: 16928, Loss: 0.8399893045425415\n",
      "Epoch: 0, Step: 16960, Loss: 0.8122606873512268\n",
      "Epoch: 0, Step: 16992, Loss: 0.9819614291191101\n",
      "Epoch: 0, Step: 17024, Loss: 0.8501332998275757\n",
      "Epoch: 0, Step: 17056, Loss: 0.9064027070999146\n",
      "Epoch: 0, Step: 17088, Loss: 0.8206194043159485\n",
      "Epoch: 0, Step: 17120, Loss: 0.8501906991004944\n",
      "Epoch: 0, Step: 17152, Loss: 0.8880336284637451\n",
      "Epoch: 0, Step: 17184, Loss: 0.8442870378494263\n",
      "Epoch: 0, Step: 17216, Loss: 0.9025877118110657\n",
      "Epoch: 0, Step: 17248, Loss: 0.8278616666793823\n",
      "Epoch: 0, Step: 17280, Loss: 0.8387743234634399\n",
      "Epoch: 0, Step: 17312, Loss: 0.8994843363761902\n",
      "Epoch: 0, Step: 17344, Loss: 0.8078948259353638\n",
      "Epoch: 0, Step: 17376, Loss: 0.9443114399909973\n",
      "Epoch: 0, Step: 17408, Loss: 0.8352130055427551\n",
      "Epoch: 0, Step: 17440, Loss: 0.8566915988922119\n",
      "Epoch: 0, Step: 17472, Loss: 0.7745229601860046\n",
      "Epoch: 0, Step: 17504, Loss: 0.860339879989624\n",
      "Epoch: 0, Step: 17536, Loss: 0.7410332560539246\n",
      "Epoch: 0, Step: 17568, Loss: 0.9155395030975342\n",
      "Epoch: 0, Step: 17600, Loss: 0.9431958198547363\n",
      "Epoch: 0, Step: 17632, Loss: 0.8027467131614685\n",
      "Epoch: 0, Step: 17664, Loss: 0.8603882789611816\n",
      "Epoch: 0, Step: 17696, Loss: 0.9572015404701233\n",
      "Epoch: 0, Step: 17728, Loss: 0.7084416747093201\n",
      "Epoch: 0, Step: 17760, Loss: 0.7975374460220337\n",
      "Epoch: 0, Step: 17792, Loss: 0.8604447841644287\n",
      "Epoch: 0, Step: 17824, Loss: 0.8801635503768921\n",
      "Epoch: 0, Step: 17856, Loss: 0.8307225704193115\n",
      "Epoch: 0, Step: 17888, Loss: 0.8775863647460938\n",
      "Epoch: 0, Step: 17920, Loss: 0.9198099374771118\n",
      "Epoch: 0, Step: 17952, Loss: 0.7678417563438416\n",
      "Epoch: 0, Step: 17984, Loss: 0.8104904294013977\n",
      "Epoch: 0, Step: 18016, Loss: 0.8560479283332825\n",
      "Epoch: 0, Step: 18048, Loss: 0.8180339336395264\n",
      "Epoch: 0, Step: 18080, Loss: 0.8604145050048828\n",
      "Epoch: 0, Step: 18112, Loss: 0.8821256756782532\n",
      "Epoch: 0, Step: 18144, Loss: 0.7387627363204956\n",
      "Epoch: 0, Step: 18176, Loss: 0.8025699853897095\n",
      "Epoch: 0, Step: 18208, Loss: 0.7821472883224487\n",
      "Epoch: 0, Step: 18240, Loss: 0.8088292479515076\n",
      "Epoch: 0, Step: 18272, Loss: 0.840697705745697\n",
      "Epoch: 0, Step: 18304, Loss: 0.8932929635047913\n",
      "Epoch: 0, Step: 18336, Loss: 0.8659048080444336\n",
      "Epoch: 0, Step: 18368, Loss: 0.8935770988464355\n",
      "Epoch: 0, Step: 18400, Loss: 0.7351199984550476\n",
      "Epoch: 0, Step: 18432, Loss: 0.8328985571861267\n",
      "Epoch: 0, Step: 18464, Loss: 0.7723304629325867\n",
      "Epoch: 0, Step: 18496, Loss: 0.7695905566215515\n",
      "Epoch: 0, Step: 18528, Loss: 0.9227900505065918\n",
      "Epoch: 0, Step: 18560, Loss: 0.760224461555481\n",
      "Epoch: 0, Step: 18592, Loss: 0.7895756363868713\n",
      "Epoch: 0, Step: 18624, Loss: 0.9295970797538757\n",
      "Epoch: 0, Step: 18656, Loss: 0.8112360239028931\n",
      "Epoch: 0, Step: 18688, Loss: 0.7825709581375122\n",
      "Epoch: 0, Step: 18720, Loss: 0.6804842352867126\n",
      "Epoch: 0, Step: 18752, Loss: 0.8385130167007446\n",
      "Epoch: 0, Step: 18784, Loss: 0.7655378580093384\n",
      "Epoch: 0, Step: 18816, Loss: 0.8064808249473572\n",
      "Epoch: 0, Step: 18848, Loss: 0.83637535572052\n",
      "Epoch: 0, Step: 18880, Loss: 0.8331887722015381\n",
      "Epoch: 0, Step: 18912, Loss: 0.8384899497032166\n",
      "Epoch: 0, Step: 18944, Loss: 0.817350447177887\n",
      "Epoch: 0, Step: 18976, Loss: 0.7991324067115784\n",
      "Epoch: 0, Step: 19008, Loss: 0.8003261089324951\n",
      "Epoch: 0, Step: 19040, Loss: 0.8318873047828674\n",
      "Epoch: 0, Step: 19072, Loss: 0.837331235408783\n",
      "Epoch: 0, Step: 19104, Loss: 0.880225658416748\n",
      "Epoch: 0, Step: 19136, Loss: 0.9431069493293762\n",
      "Epoch: 0, Step: 19168, Loss: 0.8369241952896118\n",
      "Epoch: 0, Step: 19200, Loss: 0.8054827451705933\n",
      "Epoch: 0, Step: 19232, Loss: 0.7955143451690674\n",
      "Epoch: 0, Step: 19264, Loss: 0.777114748954773\n",
      "Epoch: 0, Step: 19296, Loss: 0.7815688252449036\n",
      "Epoch: 0, Step: 19328, Loss: 0.8773231506347656\n",
      "Epoch: 0, Step: 19360, Loss: 0.8181211352348328\n",
      "Epoch: 0, Step: 19392, Loss: 0.7970924973487854\n",
      "Epoch: 0, Step: 19424, Loss: 0.8940308094024658\n",
      "Epoch: 0, Step: 19456, Loss: 0.8063234686851501\n",
      "Epoch: 0, Step: 19488, Loss: 0.8122161030769348\n",
      "Epoch: 0, Step: 19520, Loss: 0.7679280042648315\n",
      "Epoch: 0, Step: 19552, Loss: 0.8804091215133667\n",
      "Epoch: 0, Step: 19584, Loss: 0.9166190028190613\n",
      "Epoch: 0, Step: 19616, Loss: 0.8151392340660095\n",
      "Epoch: 0, Step: 19648, Loss: 0.9086697697639465\n",
      "Epoch: 0, Step: 19680, Loss: 0.7936041951179504\n",
      "Epoch: 0, Step: 19712, Loss: 0.8703649044036865\n",
      "Epoch: 0, Step: 19744, Loss: 0.881980299949646\n",
      "Epoch: 0, Step: 19776, Loss: 0.83136385679245\n",
      "Epoch: 0, Step: 19808, Loss: 0.9070436954498291\n",
      "Epoch: 0, Step: 19840, Loss: 0.7768713235855103\n",
      "Epoch: 0, Step: 19872, Loss: 0.8877388834953308\n",
      "Epoch: 0, Step: 19904, Loss: 0.8397194743156433\n",
      "Epoch: 0, Step: 19936, Loss: 0.8416624665260315\n",
      "Epoch: 0, Step: 19968, Loss: 0.8285347819328308\n",
      "Epoch: 0, Step: 20000, Loss: 0.7779313325881958\n",
      "Epoch: 0, Step: 20032, Loss: 0.860430896282196\n",
      "Epoch: 0, Step: 20064, Loss: 0.8331413865089417\n",
      "Epoch: 0, Step: 20096, Loss: 0.8496491312980652\n",
      "Epoch: 0, Step: 20128, Loss: 0.7588059306144714\n",
      "Epoch: 0, Step: 20160, Loss: 0.9105426669120789\n",
      "Epoch: 0, Step: 20192, Loss: 0.8402582406997681\n",
      "Epoch: 0, Step: 20224, Loss: 0.811659038066864\n",
      "Epoch: 0, Step: 20256, Loss: 0.7803467512130737\n",
      "Epoch: 0, Step: 20288, Loss: 0.8874269723892212\n",
      "Epoch: 0, Step: 20320, Loss: 0.8127230405807495\n",
      "Epoch: 0, Step: 20352, Loss: 0.8414726853370667\n",
      "Epoch: 0, Step: 20384, Loss: 0.6840455532073975\n",
      "Epoch: 0, Step: 20416, Loss: 0.859873354434967\n",
      "Epoch: 0, Step: 20448, Loss: 0.7894409894943237\n",
      "Epoch: 0, Step: 20480, Loss: 0.8221094012260437\n",
      "Epoch: 0, Step: 20512, Loss: 0.8630744814872742\n",
      "Epoch: 0, Step: 20544, Loss: 0.8333120942115784\n",
      "Epoch: 0, Step: 20576, Loss: 0.7657672762870789\n",
      "Epoch: 0, Step: 20608, Loss: 0.870450496673584\n",
      "Epoch: 0, Step: 20640, Loss: 0.7304448485374451\n",
      "Epoch: 0, Step: 20672, Loss: 0.7940707802772522\n",
      "Epoch: 0, Step: 20704, Loss: 0.8166443109512329\n",
      "Epoch: 0, Step: 20736, Loss: 0.8205417990684509\n",
      "Epoch: 0, Step: 20768, Loss: 0.9140475392341614\n",
      "Epoch: 0, Step: 20800, Loss: 0.8267969489097595\n",
      "Epoch: 0, Step: 20832, Loss: 0.868559718132019\n",
      "Epoch: 0, Step: 20864, Loss: 0.8299911022186279\n",
      "Epoch: 0, Step: 20896, Loss: 0.803318202495575\n",
      "Epoch: 0, Step: 20928, Loss: 0.7823601365089417\n",
      "Epoch: 0, Step: 20960, Loss: 0.8006383180618286\n",
      "Epoch: 0, Step: 20992, Loss: 0.7390463948249817\n",
      "Epoch: 0, Step: 21024, Loss: 0.9302204251289368\n",
      "Epoch: 0, Step: 21056, Loss: 0.8433095812797546\n",
      "Epoch: 0, Step: 21088, Loss: 0.8507795929908752\n",
      "Epoch: 0, Step: 21120, Loss: 0.8603876829147339\n",
      "Epoch: 0, Step: 21152, Loss: 0.8001515865325928\n",
      "Epoch: 0, Step: 21184, Loss: 0.9429886341094971\n",
      "Epoch: 0, Step: 21216, Loss: 0.795642614364624\n",
      "Epoch: 0, Step: 21248, Loss: 0.8257277011871338\n",
      "Epoch: 0, Step: 21280, Loss: 0.8159353137016296\n",
      "Epoch: 0, Step: 21312, Loss: 0.7853748202323914\n",
      "Epoch: 0, Step: 21344, Loss: 0.8329638242721558\n",
      "Epoch: 0, Step: 21376, Loss: 0.793222188949585\n",
      "Epoch: 0, Step: 21408, Loss: 0.900162398815155\n",
      "Epoch: 0, Step: 21440, Loss: 0.7882567048072815\n",
      "Epoch: 0, Step: 21472, Loss: 0.8934252858161926\n",
      "Epoch: 0, Step: 21504, Loss: 0.8021466732025146\n",
      "Epoch: 0, Step: 21536, Loss: 0.8364912867546082\n",
      "Epoch: 0, Step: 21568, Loss: 0.8225700259208679\n",
      "Epoch: 0, Step: 21600, Loss: 0.8240157961845398\n",
      "Epoch: 0, Step: 21632, Loss: 0.8392239212989807\n",
      "Epoch: 0, Step: 21664, Loss: 0.8875996470451355\n",
      "Epoch: 0, Step: 21696, Loss: 0.7716436982154846\n",
      "Epoch: 0, Step: 21728, Loss: 0.8627192974090576\n",
      "Epoch: 0, Step: 21760, Loss: 0.8919912576675415\n",
      "Epoch: 0, Step: 21792, Loss: 0.8651331067085266\n",
      "Epoch: 0, Step: 21824, Loss: 0.8203620314598083\n",
      "Epoch: 0, Step: 21856, Loss: 0.7824982404708862\n",
      "Epoch: 0, Step: 21888, Loss: 0.7998785376548767\n",
      "Epoch: 0, Step: 21920, Loss: 0.7228663563728333\n",
      "Epoch: 0, Step: 21952, Loss: 0.8422102332115173\n",
      "Epoch: 0, Step: 21984, Loss: 0.7285334467887878\n",
      "Epoch: 0, Step: 22016, Loss: 0.93439120054245\n",
      "Epoch: 0, Step: 22048, Loss: 0.8994556069374084\n",
      "Epoch: 0, Step: 22080, Loss: 0.8878327012062073\n",
      "Epoch: 0, Step: 22112, Loss: 0.8072038292884827\n",
      "Epoch: 0, Step: 22144, Loss: 0.730461835861206\n",
      "Epoch: 0, Step: 22176, Loss: 0.8184911608695984\n",
      "Epoch: 0, Step: 22208, Loss: 0.8371022343635559\n",
      "Epoch: 0, Step: 22240, Loss: 0.7827659249305725\n",
      "Epoch: 0, Step: 22272, Loss: 0.7914533019065857\n",
      "Epoch: 0, Step: 22304, Loss: 0.84376460313797\n",
      "Epoch: 0, Step: 22336, Loss: 0.8726428151130676\n",
      "Epoch: 0, Step: 22368, Loss: 0.7995220422744751\n",
      "Epoch: 0, Step: 22400, Loss: 0.8776407837867737\n",
      "Epoch: 0, Step: 22432, Loss: 0.8495977520942688\n",
      "Epoch: 0, Step: 22464, Loss: 0.8678921461105347\n",
      "Epoch: 0, Step: 22496, Loss: 0.7285204529762268\n",
      "Epoch: 0, Step: 22528, Loss: 0.8887796401977539\n",
      "Epoch: 0, Step: 22560, Loss: 0.8566234707832336\n",
      "Epoch: 0, Step: 22592, Loss: 0.8502967953681946\n",
      "Epoch: 0, Step: 22624, Loss: 0.7861728072166443\n",
      "Epoch: 0, Step: 22656, Loss: 0.9170947074890137\n",
      "Epoch: 0, Step: 22688, Loss: 0.8195016384124756\n",
      "Epoch: 0, Step: 22720, Loss: 0.7351405620574951\n",
      "Epoch: 0, Step: 22752, Loss: 0.7580799460411072\n",
      "Epoch: 0, Step: 22784, Loss: 0.8027897477149963\n",
      "Epoch: 0, Step: 22816, Loss: 0.9087274074554443\n",
      "Epoch: 0, Step: 22848, Loss: 0.886202871799469\n",
      "Epoch: 0, Step: 22880, Loss: 0.796390175819397\n",
      "Epoch: 0, Step: 22912, Loss: 0.795342743396759\n",
      "Epoch: 0, Step: 22944, Loss: 0.8722141981124878\n",
      "Epoch: 0, Step: 22976, Loss: 0.8741534352302551\n",
      "Epoch: 0, Step: 23008, Loss: 0.7812831997871399\n",
      "Epoch: 0, Step: 23040, Loss: 0.8746575117111206\n",
      "Epoch: 0, Step: 23072, Loss: 0.9112130999565125\n",
      "Epoch: 0, Step: 23104, Loss: 0.8219946026802063\n",
      "Epoch: 0, Step: 23136, Loss: 0.8365166187286377\n",
      "Epoch: 0, Step: 23168, Loss: 0.8232107758522034\n",
      "Epoch: 0, Step: 23200, Loss: 0.8868961334228516\n",
      "Epoch: 0, Step: 23232, Loss: 0.818810224533081\n",
      "Epoch: 0, Step: 23264, Loss: 0.8922351598739624\n",
      "Epoch: 0, Step: 23296, Loss: 0.778700590133667\n",
      "Epoch: 0, Step: 23328, Loss: 0.7863990068435669\n",
      "Epoch: 0, Step: 23360, Loss: 0.8462091684341431\n",
      "Epoch: 0, Step: 23392, Loss: 0.8016424775123596\n",
      "Epoch: 0, Step: 23424, Loss: 0.809950590133667\n",
      "Epoch: 0, Step: 23456, Loss: 0.8950105309486389\n",
      "Epoch: 0, Step: 23488, Loss: 0.8726639747619629\n",
      "Epoch: 0, Step: 23520, Loss: 0.8308735489845276\n",
      "Epoch: 0, Step: 23552, Loss: 0.778938353061676\n",
      "Epoch: 0, Step: 23584, Loss: 0.8347103595733643\n",
      "Epoch: 0, Step: 23616, Loss: 0.912706732749939\n",
      "Epoch: 0, Step: 23648, Loss: 0.7749535441398621\n",
      "Epoch: 0, Step: 23680, Loss: 0.8062615990638733\n",
      "Epoch: 0, Step: 23712, Loss: 0.9087126851081848\n",
      "Epoch: 0, Step: 23744, Loss: 0.8160682320594788\n",
      "Epoch: 0, Step: 23776, Loss: 0.847908079624176\n",
      "Epoch: 0, Step: 23808, Loss: 0.8371793627738953\n",
      "Epoch: 0, Step: 23840, Loss: 0.8359302282333374\n",
      "Epoch: 0, Step: 23872, Loss: 0.9279548525810242\n",
      "Epoch: 0, Step: 23904, Loss: 0.8679180145263672\n",
      "Epoch: 0, Step: 23936, Loss: 0.8213679194450378\n",
      "Epoch: 0, Step: 23968, Loss: 1.016998529434204\n",
      "Epoch: 0, Step: 24000, Loss: 0.8041937947273254\n",
      "Epoch: 0, Step: 24032, Loss: 0.9026955366134644\n",
      "Epoch: 0, Step: 24064, Loss: 0.8544539213180542\n",
      "Epoch: 0, Step: 24096, Loss: 0.8309186100959778\n",
      "Epoch: 0, Step: 24128, Loss: 0.8322341442108154\n",
      "Epoch: 0, Step: 24160, Loss: 0.7961015105247498\n",
      "Epoch: 0, Step: 24192, Loss: 0.8562518358230591\n",
      "Epoch: 0, Step: 24224, Loss: 0.8619738221168518\n",
      "Epoch: 0, Step: 24256, Loss: 0.8392215967178345\n",
      "Epoch: 0, Step: 24288, Loss: 0.7516099810600281\n",
      "Epoch: 0, Step: 24320, Loss: 0.8074037432670593\n",
      "Epoch: 0, Step: 24352, Loss: 0.7696632742881775\n",
      "Epoch: 0, Step: 24384, Loss: 0.7549875378608704\n",
      "Epoch: 0, Step: 24416, Loss: 0.8515278697013855\n",
      "Epoch: 0, Step: 24448, Loss: 0.7732762098312378\n",
      "Epoch: 0, Step: 24480, Loss: 0.816521942615509\n",
      "Epoch: 0, Step: 24512, Loss: 0.7533237338066101\n",
      "Epoch: 0, Step: 24544, Loss: 0.8384575247764587\n",
      "Epoch: 0, Step: 24576, Loss: 0.782570481300354\n",
      "Epoch: 0, Step: 24608, Loss: 0.8184787631034851\n",
      "Epoch: 0, Step: 24640, Loss: 0.8856015801429749\n",
      "Epoch: 0, Step: 24672, Loss: 0.7967072129249573\n",
      "Epoch: 0, Step: 24704, Loss: 0.8234526515007019\n",
      "Epoch: 0, Step: 24736, Loss: 0.8434960842132568\n",
      "Epoch: 0, Step: 24768, Loss: 0.9175365567207336\n",
      "Epoch: 0, Step: 24800, Loss: 0.7597761750221252\n",
      "Epoch: 0, Step: 24832, Loss: 0.8542093634605408\n",
      "Epoch: 0, Step: 24864, Loss: 0.7232888340950012\n",
      "Epoch: 0, Step: 24896, Loss: 0.909706711769104\n",
      "Epoch: 0, Step: 24928, Loss: 0.7622781991958618\n",
      "Epoch: 0, Step: 24960, Loss: 0.8933984637260437\n",
      "Epoch: 0, Step: 24992, Loss: 0.8678103089332581\n",
      "Epoch: 0, Step: 25024, Loss: 0.8929497599601746\n",
      "Epoch: 0, Step: 25056, Loss: 0.883034884929657\n",
      "Epoch: 0, Step: 25088, Loss: 0.768464207649231\n",
      "Epoch: 0, Step: 25120, Loss: 0.8937020301818848\n",
      "Epoch: 0, Step: 25152, Loss: 0.8415514230728149\n",
      "Epoch: 0, Step: 25184, Loss: 0.868821382522583\n",
      "Epoch: 0, Step: 25216, Loss: 0.7778139710426331\n",
      "Epoch: 0, Step: 25248, Loss: 0.8853514790534973\n",
      "Epoch: 0, Step: 25280, Loss: 0.7918892502784729\n",
      "Epoch: 0, Step: 25312, Loss: 0.8689729571342468\n",
      "Epoch: 0, Step: 25344, Loss: 0.8507073521614075\n",
      "Epoch: 0, Step: 25376, Loss: 0.8710644841194153\n",
      "Epoch: 0, Step: 25408, Loss: 0.7778579592704773\n",
      "Epoch: 0, Step: 25440, Loss: 0.7969443798065186\n",
      "Epoch: 0, Step: 25472, Loss: 0.8783619403839111\n",
      "Epoch: 0, Step: 25504, Loss: 0.8996627330780029\n",
      "Epoch: 0, Step: 25536, Loss: 0.8096333146095276\n",
      "Epoch: 0, Step: 25568, Loss: 0.8451125025749207\n",
      "Epoch: 0, Step: 25600, Loss: 0.8537752032279968\n",
      "Epoch: 0, Step: 25632, Loss: 0.8179200291633606\n",
      "Epoch: 0, Step: 25664, Loss: 0.9229695796966553\n",
      "Epoch: 0, Step: 25696, Loss: 0.9557392001152039\n",
      "Epoch: 0, Step: 25728, Loss: 0.8019341826438904\n",
      "Epoch: 0, Step: 25760, Loss: 0.8766940236091614\n",
      "Epoch: 0, Step: 25792, Loss: 0.7281438708305359\n",
      "Epoch: 0, Step: 25824, Loss: 0.8109815716743469\n",
      "Epoch: 0, Step: 25856, Loss: 0.739419162273407\n",
      "Epoch: 0, Step: 25888, Loss: 0.7380822896957397\n",
      "Epoch: 0, Step: 25920, Loss: 0.893328070640564\n",
      "Epoch: 0, Step: 25952, Loss: 0.8241590857505798\n",
      "Epoch: 0, Step: 25984, Loss: 0.7745962738990784\n",
      "Epoch: 0, Step: 26016, Loss: 0.7829002737998962\n",
      "Epoch: 0, Step: 26048, Loss: 0.8926407098770142\n",
      "Epoch: 0, Step: 26080, Loss: 0.8543463349342346\n",
      "Epoch: 0, Step: 26112, Loss: 0.7797170877456665\n",
      "Epoch: 0, Step: 26144, Loss: 0.877526044845581\n",
      "Epoch: 0, Step: 26176, Loss: 0.8301729559898376\n",
      "Epoch: 0, Step: 26208, Loss: 0.8396115899085999\n",
      "Epoch: 0, Step: 26240, Loss: 0.8185344338417053\n",
      "Epoch: 0, Step: 26272, Loss: 0.8666893243789673\n",
      "Epoch: 0, Step: 26304, Loss: 0.9243680834770203\n",
      "Epoch: 0, Step: 26336, Loss: 0.9102246165275574\n",
      "Epoch: 0, Step: 26368, Loss: 0.887134313583374\n",
      "Epoch: 0, Step: 26400, Loss: 0.8613448143005371\n",
      "Epoch: 0, Step: 26432, Loss: 0.8549836874008179\n",
      "Epoch: 0, Step: 26464, Loss: 0.9345455765724182\n",
      "Epoch: 0, Step: 26496, Loss: 0.8610468506813049\n",
      "Epoch: 0, Step: 26528, Loss: 0.8473938703536987\n",
      "Epoch: 0, Step: 26560, Loss: 0.8399713039398193\n",
      "Epoch: 0, Step: 26592, Loss: 0.910679042339325\n",
      "Epoch: 0, Step: 26624, Loss: 0.7627461552619934\n",
      "Epoch: 0, Step: 26656, Loss: 0.8532353043556213\n",
      "Epoch: 0, Step: 26688, Loss: 0.8208106160163879\n",
      "Epoch: 0, Step: 26720, Loss: 0.9067314267158508\n",
      "Epoch: 0, Step: 26752, Loss: 0.7883411645889282\n",
      "Epoch: 0, Step: 26784, Loss: 0.9540764093399048\n",
      "Epoch: 0, Step: 26816, Loss: 0.9324999451637268\n",
      "Epoch: 0, Step: 26848, Loss: 0.866749107837677\n",
      "Epoch: 0, Step: 26880, Loss: 0.8721534609794617\n",
      "Epoch: 0, Step: 26912, Loss: 0.8691047430038452\n",
      "Epoch: 0, Step: 26944, Loss: 0.8580796122550964\n",
      "Epoch: 0, Step: 26976, Loss: 0.8463855981826782\n",
      "Epoch: 0, Step: 27008, Loss: 0.8815466165542603\n",
      "Epoch: 0, Step: 27040, Loss: 0.7552806735038757\n",
      "Epoch: 0, Step: 27072, Loss: 0.8285785913467407\n",
      "Epoch: 0, Step: 27104, Loss: 0.8467997312545776\n",
      "Epoch: 0, Step: 27136, Loss: 0.7600387930870056\n",
      "Epoch: 0, Step: 27168, Loss: 0.87642502784729\n",
      "Epoch: 0, Step: 27200, Loss: 0.8958393335342407\n",
      "Epoch: 0, Step: 27232, Loss: 0.8962050676345825\n",
      "Epoch: 0, Step: 27264, Loss: 0.8817174434661865\n",
      "Epoch: 0, Step: 27296, Loss: 0.8081090450286865\n",
      "Epoch: 0, Step: 27328, Loss: 0.9598743915557861\n",
      "Epoch: 0, Step: 27360, Loss: 0.8194027543067932\n",
      "Epoch: 0, Step: 27392, Loss: 0.8324856162071228\n",
      "Epoch: 0, Step: 27424, Loss: 0.8557949662208557\n",
      "Epoch: 0, Step: 27456, Loss: 0.7728319764137268\n",
      "Epoch: 0, Step: 27488, Loss: 0.8293415904045105\n",
      "Epoch: 0, Step: 27520, Loss: 0.8750338554382324\n",
      "Epoch: 0, Step: 27552, Loss: 0.8946211934089661\n",
      "Epoch: 0, Step: 27584, Loss: 0.7532551288604736\n",
      "Epoch: 0, Step: 27616, Loss: 0.7710439562797546\n",
      "Epoch: 0, Step: 27648, Loss: 0.8499693274497986\n",
      "Epoch: 0, Step: 27680, Loss: 0.820077657699585\n",
      "Epoch: 0, Step: 27712, Loss: 0.8955732583999634\n",
      "Epoch: 0, Step: 27744, Loss: 0.8688197135925293\n",
      "Epoch: 0, Step: 27776, Loss: 0.7647727727890015\n",
      "Epoch: 0, Step: 27808, Loss: 0.9038098454475403\n",
      "Epoch: 0, Step: 27840, Loss: 0.834333598613739\n",
      "Epoch: 0, Step: 27872, Loss: 0.9262240529060364\n",
      "Epoch: 0, Step: 27904, Loss: 0.8520720601081848\n",
      "Epoch: 0, Step: 27936, Loss: 0.8292515873908997\n",
      "Epoch: 0, Step: 27968, Loss: 0.7989262342453003\n",
      "Epoch: 0, Step: 28000, Loss: 0.8761553764343262\n",
      "Epoch: 0, Step: 28032, Loss: 0.7576998472213745\n",
      "Epoch: 0, Step: 28064, Loss: 0.8047939538955688\n",
      "Epoch: 0, Step: 28096, Loss: 0.844353973865509\n",
      "Epoch: 0, Step: 28128, Loss: 0.8713863492012024\n",
      "Epoch: 0, Step: 28160, Loss: 0.8600281476974487\n",
      "Epoch: 0, Step: 28192, Loss: 0.8876913785934448\n",
      "Epoch: 0, Step: 28224, Loss: 0.943580150604248\n",
      "Epoch: 0, Step: 28256, Loss: 0.7603124380111694\n",
      "Epoch: 0, Step: 28288, Loss: 0.805351197719574\n",
      "Epoch: 0, Step: 28320, Loss: 0.7960672974586487\n",
      "Epoch: 0, Step: 28352, Loss: 0.7946456074714661\n",
      "Epoch: 0, Step: 28384, Loss: 0.88069748878479\n",
      "Epoch: 0, Step: 28416, Loss: 0.880707323551178\n",
      "Epoch: 0, Step: 28448, Loss: 0.8636741638183594\n",
      "Epoch: 0, Step: 28480, Loss: 0.8065764904022217\n",
      "Epoch: 0, Step: 28512, Loss: 0.8546239137649536\n",
      "Epoch: 0, Step: 28544, Loss: 0.8125019669532776\n",
      "Epoch: 0, Step: 28576, Loss: 0.7661470174789429\n",
      "Epoch: 0, Step: 28608, Loss: 0.9450502395629883\n",
      "Epoch: 0, Step: 28640, Loss: 0.7127610445022583\n",
      "Epoch: 0, Step: 28672, Loss: 0.8312599062919617\n",
      "Epoch: 0, Step: 28704, Loss: 0.807578980922699\n",
      "Epoch: 0, Step: 28736, Loss: 0.8372094631195068\n",
      "Epoch: 0, Step: 28768, Loss: 0.7972919344902039\n",
      "Epoch: 0, Step: 28800, Loss: 0.870597243309021\n",
      "Epoch: 0, Step: 28832, Loss: 0.770999550819397\n",
      "Epoch: 0, Step: 28864, Loss: 0.7848815321922302\n",
      "Epoch: 0, Step: 28896, Loss: 0.7958294749259949\n",
      "Epoch: 0, Step: 28928, Loss: 0.8885242938995361\n",
      "Epoch: 0, Step: 28960, Loss: 0.8852952122688293\n",
      "Epoch: 0, Step: 28992, Loss: 0.884323000907898\n",
      "Epoch: 0, Step: 29024, Loss: 0.8242076635360718\n",
      "Epoch: 0, Step: 29056, Loss: 0.8791837692260742\n",
      "Epoch: 0, Step: 29088, Loss: 0.8584820032119751\n",
      "Epoch: 0, Step: 29120, Loss: 0.8656582832336426\n",
      "Epoch: 0, Step: 29152, Loss: 0.8243860006332397\n",
      "Epoch: 0, Step: 29184, Loss: 0.7414047122001648\n",
      "Epoch: 0, Step: 29216, Loss: 0.8478735685348511\n",
      "Epoch: 0, Step: 29248, Loss: 0.7078969478607178\n",
      "Epoch: 0, Step: 29280, Loss: 0.7811024785041809\n",
      "Epoch: 0, Step: 29312, Loss: 0.8296814560890198\n",
      "Epoch: 0, Step: 29344, Loss: 0.7592422962188721\n",
      "Epoch: 0, Step: 29376, Loss: 0.7596474289894104\n",
      "Epoch: 0, Step: 29408, Loss: 0.8391210436820984\n",
      "Epoch: 0, Step: 29440, Loss: 0.9274932742118835\n",
      "Epoch: 0, Step: 29472, Loss: 0.8204501271247864\n",
      "Epoch: 0, Step: 29504, Loss: 0.8151724934577942\n",
      "Epoch: 0, Step: 29536, Loss: 0.8600085973739624\n",
      "Epoch: 0, Step: 29568, Loss: 0.8453854918479919\n",
      "Epoch: 0, Step: 29600, Loss: 0.849975049495697\n",
      "Epoch: 0, Step: 29632, Loss: 0.8677638173103333\n",
      "Epoch: 0, Step: 29664, Loss: 0.7934845685958862\n",
      "Epoch: 0, Step: 29696, Loss: 0.8120693564414978\n",
      "Epoch: 0, Step: 29728, Loss: 0.9555969834327698\n",
      "Epoch: 0, Step: 29760, Loss: 0.9224951863288879\n",
      "Epoch: 0, Step: 29792, Loss: 0.8150357007980347\n",
      "Epoch: 0, Step: 29824, Loss: 0.8765884041786194\n",
      "Epoch: 0, Step: 29856, Loss: 0.8635842800140381\n",
      "Epoch: 0, Step: 29888, Loss: 0.9579485058784485\n",
      "Epoch: 0, Step: 29920, Loss: 0.8640082478523254\n",
      "Epoch: 0, Step: 29952, Loss: 0.8116541504859924\n",
      "Epoch: 0, Step: 29984, Loss: 0.8565052151679993\n",
      "Epoch: 0, Step: 30016, Loss: 0.9464302062988281\n",
      "Epoch: 0, Step: 30048, Loss: 0.9999580383300781\n",
      "Epoch: 0, Step: 30080, Loss: 0.8040772080421448\n",
      "Epoch: 0, Step: 30112, Loss: 0.9120571613311768\n",
      "Epoch: 0, Step: 30144, Loss: 0.8642604351043701\n",
      "Epoch: 0, Step: 30176, Loss: 0.7471649050712585\n",
      "Epoch: 0, Step: 30208, Loss: 0.7813695073127747\n",
      "Epoch: 0, Step: 30240, Loss: 0.7834500670433044\n",
      "Epoch: 0, Step: 30272, Loss: 0.8530542254447937\n",
      "Epoch: 0, Step: 30304, Loss: 0.827340304851532\n",
      "Epoch: 0, Step: 30336, Loss: 0.7643584609031677\n",
      "Epoch: 0, Step: 30368, Loss: 0.8048673272132874\n",
      "Epoch: 0, Step: 30400, Loss: 0.8161281943321228\n",
      "Epoch: 0, Step: 30432, Loss: 0.8267814517021179\n",
      "Epoch: 0, Step: 30464, Loss: 0.8061872720718384\n",
      "Epoch: 0, Step: 30496, Loss: 0.8583434224128723\n",
      "Epoch: 0, Step: 30528, Loss: 0.9236913323402405\n",
      "Epoch: 0, Step: 30560, Loss: 0.9402041435241699\n",
      "Epoch: 0, Step: 30592, Loss: 0.7311903238296509\n",
      "Epoch: 0, Step: 30624, Loss: 0.7551834583282471\n",
      "Epoch: 0, Step: 30656, Loss: 0.8335859775543213\n",
      "Epoch: 0, Step: 30688, Loss: 0.8241296410560608\n",
      "Epoch: 0, Step: 30720, Loss: 0.9432986974716187\n",
      "Epoch: 0, Step: 30752, Loss: 0.8746933937072754\n",
      "Epoch: 0, Step: 30784, Loss: 0.8114840984344482\n",
      "Epoch: 0, Step: 30816, Loss: 0.7638759016990662\n",
      "Epoch: 0, Step: 30848, Loss: 0.8971577882766724\n",
      "Epoch: 0, Step: 30880, Loss: 0.8617833852767944\n",
      "Epoch: 0, Step: 30912, Loss: 0.9307457804679871\n",
      "Epoch: 0, Step: 30944, Loss: 0.8836261630058289\n",
      "Epoch: 0, Step: 30976, Loss: 0.9094655513763428\n",
      "Epoch: 0, Step: 31008, Loss: 0.8314961194992065\n",
      "Epoch: 0, Step: 31040, Loss: 0.8499643802642822\n",
      "Epoch: 0, Step: 31072, Loss: 0.8047656416893005\n",
      "Epoch: 0, Step: 31104, Loss: 0.8693004846572876\n",
      "Epoch: 0, Step: 31136, Loss: 0.9274055361747742\n",
      "Epoch: 0, Step: 31168, Loss: 0.8558633327484131\n",
      "Epoch: 0, Step: 31200, Loss: 0.8596957325935364\n",
      "Epoch: 0, Step: 31232, Loss: 0.9037295579910278\n",
      "Epoch: 0, Step: 31264, Loss: 0.812930166721344\n",
      "Epoch: 0, Step: 31296, Loss: 0.7261536121368408\n",
      "Epoch: 0, Step: 31328, Loss: 0.9437603950500488\n",
      "Epoch: 0, Step: 31360, Loss: 0.8214907050132751\n",
      "Epoch: 0, Step: 31392, Loss: 1.0323150157928467\n",
      "Epoch: 0, Step: 31424, Loss: 0.7806758284568787\n",
      "Epoch: 0, Step: 31456, Loss: 0.898420512676239\n",
      "Epoch: 0, Step: 31488, Loss: 0.8620741367340088\n",
      "Epoch: 0, Step: 31520, Loss: 0.8465925455093384\n",
      "Epoch: 0, Step: 31552, Loss: 0.7489568591117859\n",
      "Epoch: 0, Step: 31584, Loss: 0.8682816028594971\n",
      "Epoch: 0, Step: 31616, Loss: 0.934222400188446\n",
      "Epoch: 0, Step: 31648, Loss: 0.7680885791778564\n",
      "Epoch: 0, Step: 31680, Loss: 0.8409121632575989\n",
      "Epoch: 0, Step: 31712, Loss: 0.8991695642471313\n",
      "Epoch: 0, Step: 31744, Loss: 0.8247852921485901\n",
      "Epoch: 0, Step: 31776, Loss: 0.895542323589325\n",
      "Epoch: 0, Step: 31808, Loss: 0.8112510442733765\n",
      "Epoch: 0, Step: 31840, Loss: 0.796375036239624\n",
      "Epoch: 0, Step: 31872, Loss: 0.8583180904388428\n",
      "Epoch: 0, Step: 31904, Loss: 0.8215109705924988\n",
      "Epoch: 0, Step: 31936, Loss: 0.8079678416252136\n",
      "Epoch: 0, Step: 31968, Loss: 0.8540278077125549\n",
      "Epoch: 0, Step: 32000, Loss: 0.8784002065658569\n",
      "Epoch: 0, Step: 32032, Loss: 0.7577501535415649\n",
      "Epoch: 0, Step: 32064, Loss: 0.858832836151123\n",
      "Epoch: 0, Step: 32096, Loss: 0.7826429605484009\n",
      "Epoch: 0, Step: 32128, Loss: 0.8312721848487854\n",
      "Epoch: 0, Step: 32160, Loss: 0.7827120423316956\n",
      "Epoch: 0, Step: 32192, Loss: 0.9696922898292542\n",
      "Epoch: 0, Step: 32224, Loss: 0.834105372428894\n",
      "Epoch: 0, Step: 32256, Loss: 0.8529149293899536\n",
      "Epoch: 0, Step: 32288, Loss: 0.8295864462852478\n",
      "Epoch: 0, Step: 32320, Loss: 0.7701427340507507\n",
      "Epoch: 0, Step: 32352, Loss: 0.8398383855819702\n",
      "Epoch: 0, Step: 32384, Loss: 0.8009529709815979\n",
      "Epoch: 0, Step: 32416, Loss: 0.8936463594436646\n",
      "Epoch: 0, Step: 32448, Loss: 0.8828938007354736\n",
      "Epoch: 0, Step: 32480, Loss: 0.8991479873657227\n",
      "Epoch: 0, Step: 32512, Loss: 0.8271772265434265\n",
      "Epoch: 0, Step: 32544, Loss: 0.8175329566001892\n",
      "Epoch: 0, Step: 32576, Loss: 0.8885504007339478\n",
      "Epoch: 0, Step: 32608, Loss: 0.7323687672615051\n",
      "Epoch: 0, Step: 32640, Loss: 0.8596709370613098\n",
      "Epoch: 0, Step: 32672, Loss: 0.7924813032150269\n",
      "Epoch: 0, Step: 32704, Loss: 0.858199417591095\n",
      "Epoch: 0, Step: 32736, Loss: 0.89207524061203\n",
      "Epoch: 0, Step: 32768, Loss: 0.860349714756012\n",
      "Epoch: 0, Step: 32800, Loss: 0.7725241780281067\n",
      "Epoch: 0, Step: 32832, Loss: 0.7713660001754761\n",
      "Epoch: 0, Step: 32864, Loss: 0.8499786257743835\n",
      "Epoch: 0, Step: 32896, Loss: 0.7907750606536865\n",
      "Epoch: 0, Step: 32928, Loss: 0.8707605600357056\n",
      "Epoch: 0, Step: 32960, Loss: 0.8799183964729309\n",
      "Epoch: 0, Step: 32992, Loss: 0.8488962054252625\n",
      "Epoch: 0, Step: 33024, Loss: 0.7404201030731201\n",
      "Epoch: 0, Step: 33056, Loss: 0.7849393486976624\n",
      "Epoch: 0, Step: 33088, Loss: 0.9018663167953491\n",
      "Epoch: 0, Step: 33120, Loss: 0.7788326144218445\n",
      "Epoch: 0, Step: 33152, Loss: 0.8436101078987122\n",
      "Epoch: 0, Step: 33184, Loss: 0.8354827761650085\n",
      "Epoch: 0, Step: 33216, Loss: 0.7359146475791931\n",
      "Epoch: 0, Step: 33248, Loss: 0.9215068817138672\n",
      "Epoch: 0, Step: 33280, Loss: 0.8069372773170471\n",
      "Epoch: 0, Step: 33312, Loss: 0.8533328175544739\n",
      "Epoch: 0, Step: 33344, Loss: 0.8734461069107056\n",
      "Epoch: 0, Step: 33376, Loss: 0.804873526096344\n",
      "Epoch: 0, Step: 33408, Loss: 0.8337370157241821\n",
      "Epoch: 0, Step: 33440, Loss: 0.8399331569671631\n",
      "Epoch: 0, Step: 33472, Loss: 0.73345547914505\n",
      "Epoch: 0, Step: 33504, Loss: 0.9058884382247925\n",
      "Epoch: 0, Step: 33536, Loss: 0.8252401947975159\n",
      "Epoch: 0, Step: 33568, Loss: 0.905983030796051\n",
      "Epoch: 0, Step: 33600, Loss: 0.882373571395874\n",
      "Epoch: 0, Step: 33632, Loss: 0.9068036675453186\n",
      "Epoch: 0, Step: 33664, Loss: 0.7643841505050659\n",
      "Epoch: 0, Step: 33696, Loss: 0.8451908826828003\n",
      "Epoch: 0, Step: 33728, Loss: 0.8324301838874817\n",
      "Epoch: 0, Step: 33760, Loss: 0.7433434128761292\n",
      "Epoch: 0, Step: 33792, Loss: 0.9146790504455566\n",
      "Epoch: 0, Step: 33824, Loss: 0.7962185144424438\n",
      "Epoch: 0, Step: 33856, Loss: 0.7547038197517395\n",
      "Epoch: 0, Step: 33888, Loss: 0.741977334022522\n",
      "Epoch: 0, Step: 33920, Loss: 0.959514856338501\n",
      "Epoch: 0, Step: 33952, Loss: 0.7443796992301941\n",
      "Epoch: 0, Step: 33984, Loss: 0.8566142320632935\n",
      "Epoch: 0, Step: 34016, Loss: 0.8520296812057495\n",
      "Epoch: 0, Step: 34048, Loss: 0.8837240934371948\n",
      "Epoch: 0, Step: 34080, Loss: 0.8494823575019836\n",
      "Epoch: 0, Step: 34112, Loss: 0.7635478377342224\n",
      "Epoch: 0, Step: 34144, Loss: 0.8781335949897766\n",
      "Epoch: 0, Step: 34176, Loss: 0.841045081615448\n",
      "Epoch: 0, Step: 34208, Loss: 0.8062573075294495\n",
      "Epoch: 0, Step: 34240, Loss: 0.8042517304420471\n",
      "Epoch: 0, Step: 34272, Loss: 0.8017942309379578\n",
      "Epoch: 0, Step: 34304, Loss: 0.8703694343566895\n",
      "Epoch: 0, Step: 34336, Loss: 0.7623829245567322\n",
      "Epoch: 0, Step: 34368, Loss: 0.9592487215995789\n",
      "Epoch: 0, Step: 34400, Loss: 0.756550133228302\n",
      "Epoch: 0, Step: 34432, Loss: 0.8124670386314392\n",
      "Epoch: 0, Step: 34464, Loss: 0.9762611985206604\n",
      "Epoch: 0, Step: 34496, Loss: 0.8491734266281128\n",
      "Epoch: 0, Step: 34528, Loss: 0.9152486324310303\n",
      "Epoch: 0, Step: 34560, Loss: 0.7929047346115112\n",
      "Epoch: 0, Step: 34592, Loss: 0.9993283748626709\n",
      "Epoch: 0, Step: 34624, Loss: 0.7354196906089783\n",
      "Epoch: 0, Step: 34656, Loss: 0.7842420339584351\n",
      "Epoch: 0, Step: 34688, Loss: 0.8477944135665894\n",
      "Epoch: 0, Step: 34720, Loss: 0.7869721055030823\n",
      "Epoch: 0, Step: 34752, Loss: 0.8548292517662048\n",
      "Epoch: 0, Step: 34784, Loss: 0.8367556929588318\n",
      "Epoch: 0, Step: 34816, Loss: 0.7728182077407837\n",
      "Epoch: 0, Step: 34848, Loss: 0.8287206888198853\n",
      "Epoch: 0, Step: 34880, Loss: 0.8846852779388428\n",
      "Epoch: 0, Step: 34912, Loss: 0.7641044855117798\n",
      "Epoch: 0, Step: 34944, Loss: 0.8029096126556396\n",
      "Epoch: 0, Step: 34976, Loss: 0.8538933992385864\n",
      "Epoch: 0, Step: 35008, Loss: 0.8011863231658936\n",
      "Epoch: 0, Step: 35040, Loss: 0.8469123244285583\n",
      "Epoch: 0, Step: 35072, Loss: 0.7928099036216736\n",
      "Epoch: 0, Step: 35104, Loss: 0.7918365001678467\n",
      "Epoch: 0, Step: 35136, Loss: 0.8731934428215027\n",
      "Epoch: 0, Step: 35168, Loss: 0.9094640612602234\n",
      "Epoch: 0, Step: 35200, Loss: 0.8543774485588074\n",
      "Epoch: 0, Step: 35232, Loss: 0.773776650428772\n",
      "Epoch: 0, Step: 35264, Loss: 0.7098258137702942\n",
      "Epoch: 0, Step: 35296, Loss: 0.8225919604301453\n",
      "Epoch: 0, Step: 35328, Loss: 0.967170238494873\n",
      "Epoch: 0, Step: 35360, Loss: 0.8255136013031006\n",
      "Epoch: 0, Step: 35392, Loss: 0.9053711891174316\n",
      "Epoch: 0, Step: 35424, Loss: 0.7867993116378784\n",
      "Epoch: 0, Step: 35456, Loss: 0.8982338905334473\n",
      "Epoch: 0, Step: 35488, Loss: 0.8489006757736206\n",
      "Epoch: 0, Step: 35520, Loss: 0.8837294578552246\n",
      "Epoch: 0, Step: 35552, Loss: 0.7901675701141357\n",
      "Epoch: 0, Step: 35584, Loss: 0.8598687648773193\n",
      "Epoch: 0, Step: 35616, Loss: 0.8688969612121582\n",
      "Epoch: 0, Step: 35648, Loss: 0.8615833520889282\n",
      "Epoch: 0, Step: 35680, Loss: 0.6974764466285706\n",
      "Epoch: 0, Step: 35712, Loss: 0.8214039206504822\n",
      "Epoch: 0, Step: 35744, Loss: 0.8050734996795654\n",
      "Epoch: 0, Step: 35776, Loss: 0.9436027407646179\n",
      "Epoch: 0, Step: 35808, Loss: 0.8747146725654602\n",
      "Epoch: 0, Step: 35840, Loss: 0.7980348467826843\n",
      "Epoch: 0, Step: 35872, Loss: 0.8664189577102661\n",
      "Epoch: 0, Step: 35904, Loss: 0.8810396790504456\n",
      "Epoch: 0, Step: 35936, Loss: 0.8553512096405029\n",
      "Epoch: 0, Step: 35968, Loss: 0.6710509061813354\n",
      "Epoch: 0, Step: 36000, Loss: 0.7916960120201111\n",
      "Epoch: 0, Step: 36032, Loss: 0.8349341154098511\n",
      "Epoch: 0, Step: 36064, Loss: 0.8563376069068909\n",
      "Epoch: 0, Step: 36096, Loss: 0.7656800746917725\n",
      "Epoch: 0, Step: 36128, Loss: 0.9805377125740051\n",
      "Epoch: 0, Step: 36160, Loss: 0.8113462924957275\n",
      "Epoch: 0, Step: 36192, Loss: 0.8401193022727966\n",
      "Epoch: 0, Step: 36224, Loss: 0.8859400749206543\n",
      "Epoch: 0, Step: 36256, Loss: 0.8948699235916138\n",
      "Epoch: 0, Step: 36288, Loss: 0.7171115279197693\n",
      "Epoch: 0, Step: 36320, Loss: 0.8871304392814636\n",
      "Epoch: 0, Step: 36352, Loss: 0.9110143184661865\n",
      "Epoch: 0, Step: 36384, Loss: 0.8008108735084534\n",
      "Epoch: 0, Step: 36416, Loss: 0.8324568271636963\n",
      "Epoch: 0, Step: 36448, Loss: 0.8576332330703735\n",
      "Epoch: 0, Step: 36480, Loss: 0.9930077195167542\n",
      "Epoch: 0, Step: 36512, Loss: 0.8387256264686584\n",
      "Epoch: 0, Step: 36544, Loss: 0.786508321762085\n",
      "Epoch: 0, Step: 36576, Loss: 0.7982805371284485\n",
      "Epoch: 0, Step: 36608, Loss: 0.8068262934684753\n",
      "Epoch: 0, Step: 36640, Loss: 0.8233938813209534\n",
      "Epoch: 0, Step: 36672, Loss: 0.8819649815559387\n",
      "Epoch: 0, Step: 36704, Loss: 0.8334334492683411\n",
      "Epoch: 0, Step: 36736, Loss: 0.7796619534492493\n",
      "Epoch: 0, Step: 36768, Loss: 0.8657333850860596\n",
      "Epoch: 0, Step: 36800, Loss: 0.7566439509391785\n",
      "Epoch: 0, Step: 36832, Loss: 0.8368181586265564\n",
      "Epoch: 0, Step: 36864, Loss: 0.8873170018196106\n",
      "Epoch: 0, Step: 36896, Loss: 0.7632576823234558\n",
      "Epoch: 0, Step: 36928, Loss: 0.7932367920875549\n",
      "Epoch: 0, Step: 36960, Loss: 0.747846245765686\n",
      "Epoch: 0, Step: 36992, Loss: 0.7979266047477722\n",
      "Epoch: 0, Step: 37024, Loss: 0.8585249185562134\n",
      "Epoch: 0, Step: 37056, Loss: 0.8923493027687073\n",
      "Epoch: 0, Step: 37088, Loss: 0.8264485001564026\n",
      "Epoch: 0, Step: 37120, Loss: 0.7828563451766968\n",
      "Epoch: 0, Step: 37152, Loss: 0.8483957648277283\n",
      "Epoch: 0, Step: 37184, Loss: 0.788901686668396\n",
      "Epoch: 0, Step: 37216, Loss: 0.8279575705528259\n",
      "Epoch: 0, Step: 37248, Loss: 0.8000267744064331\n",
      "Epoch: 0, Step: 37280, Loss: 0.8690192699432373\n",
      "Epoch: 0, Step: 37312, Loss: 0.8331421613693237\n",
      "Epoch: 0, Step: 37344, Loss: 0.9133657217025757\n",
      "Epoch: 0, Step: 37376, Loss: 0.7417367100715637\n",
      "Epoch: 0, Step: 37408, Loss: 0.8930561542510986\n",
      "Epoch: 0, Step: 37440, Loss: 0.7170084118843079\n",
      "Epoch: 0, Step: 37472, Loss: 0.7839979529380798\n",
      "Epoch: 0, Step: 37504, Loss: 0.8387596011161804\n",
      "Epoch: 0, Step: 37536, Loss: 0.8847103714942932\n",
      "Epoch: 0, Step: 37568, Loss: 0.8180295825004578\n",
      "Epoch: 0, Step: 37600, Loss: 0.8284897208213806\n",
      "Epoch: 0, Step: 37632, Loss: 0.7836723923683167\n",
      "Epoch: 0, Step: 37664, Loss: 0.8858206272125244\n",
      "Epoch: 0, Step: 37696, Loss: 0.7821096777915955\n",
      "Epoch: 0, Step: 37728, Loss: 0.8038026690483093\n",
      "Epoch: 0, Step: 37760, Loss: 0.9591635465621948\n",
      "Epoch: 0, Step: 37792, Loss: 0.7975999712944031\n",
      "Epoch: 0, Step: 37824, Loss: 0.825639009475708\n",
      "Epoch: 0, Step: 37856, Loss: 0.794314444065094\n",
      "Epoch: 0, Step: 37888, Loss: 0.8418285250663757\n",
      "Epoch: 0, Step: 37920, Loss: 0.7827098369598389\n",
      "Epoch: 0, Step: 37952, Loss: 0.7706812620162964\n",
      "Epoch: 0, Step: 37984, Loss: 0.7371437549591064\n",
      "Epoch: 0, Step: 38016, Loss: 0.8475726246833801\n",
      "Epoch: 0, Step: 38048, Loss: 0.8117627501487732\n",
      "Epoch: 0, Step: 38080, Loss: 0.8157471418380737\n",
      "Epoch: 0, Step: 38112, Loss: 0.9796697497367859\n",
      "Epoch: 0, Step: 38144, Loss: 0.8437080979347229\n",
      "Epoch: 0, Step: 38176, Loss: 0.7185051441192627\n",
      "Epoch: 0, Step: 38208, Loss: 0.8583797216415405\n",
      "Epoch: 0, Step: 38240, Loss: 0.8880414962768555\n",
      "Epoch: 0, Step: 38272, Loss: 0.8339773416519165\n",
      "Epoch: 0, Step: 38304, Loss: 0.8060172200202942\n",
      "Epoch: 0, Step: 38336, Loss: 0.7509512305259705\n",
      "Epoch: 0, Step: 38368, Loss: 0.8189545273780823\n",
      "Epoch: 0, Step: 38400, Loss: 0.8500029444694519\n",
      "Epoch: 0, Step: 38432, Loss: 0.8001567721366882\n",
      "Epoch: 0, Step: 38464, Loss: 0.8685009479522705\n",
      "Epoch: 0, Step: 38496, Loss: 0.8657868504524231\n",
      "Epoch: 0, Step: 38528, Loss: 0.9099845886230469\n",
      "Epoch: 0, Step: 38560, Loss: 0.901422381401062\n",
      "Epoch: 0, Step: 38592, Loss: 0.9602177143096924\n",
      "Epoch: 0, Step: 38624, Loss: 0.7583333849906921\n",
      "Epoch: 0, Step: 38656, Loss: 0.749573826789856\n",
      "Epoch: 0, Step: 38688, Loss: 0.7502281069755554\n",
      "Epoch: 0, Step: 38720, Loss: 0.8161789178848267\n",
      "Epoch: 0, Step: 38752, Loss: 0.7100714445114136\n",
      "Epoch: 0, Step: 38784, Loss: 0.8454043865203857\n",
      "Epoch: 0, Step: 38816, Loss: 0.8250926733016968\n",
      "Epoch: 0, Step: 38848, Loss: 0.8569141626358032\n",
      "Epoch: 0, Step: 38880, Loss: 0.7871778011322021\n",
      "Epoch: 0, Step: 38912, Loss: 0.8459632992744446\n",
      "Epoch: 0, Step: 38944, Loss: 0.8666735887527466\n",
      "Epoch: 0, Step: 38976, Loss: 0.8854284882545471\n",
      "Epoch: 0, Step: 39008, Loss: 0.8011439442634583\n",
      "Epoch: 0, Step: 39040, Loss: 0.8064502477645874\n",
      "Epoch: 0, Step: 39072, Loss: 0.8474436402320862\n",
      "Epoch: 0, Step: 39104, Loss: 0.8685162663459778\n",
      "Epoch: 0, Step: 39136, Loss: 0.7814404964447021\n",
      "Epoch: 0, Step: 39168, Loss: 0.7879419922828674\n",
      "Epoch: 0, Step: 39200, Loss: 0.7763530611991882\n",
      "Epoch: 0, Step: 39232, Loss: 0.8053900599479675\n",
      "Epoch: 0, Step: 39264, Loss: 0.8722679018974304\n",
      "Epoch: 0, Step: 39296, Loss: 0.6779793500900269\n",
      "Epoch: 0, Step: 39328, Loss: 0.8284160494804382\n",
      "Epoch: 0, Step: 39360, Loss: 0.8743035197257996\n",
      "Epoch: 0, Step: 39392, Loss: 0.8874221444129944\n",
      "Epoch: 0, Step: 39424, Loss: 0.859875500202179\n",
      "Epoch: 0, Step: 39456, Loss: 0.8074460625648499\n",
      "Epoch: 0, Step: 39488, Loss: 0.8013611435890198\n",
      "Epoch: 0, Step: 39520, Loss: 0.8570784330368042\n",
      "Epoch: 0, Step: 39552, Loss: 0.8209660649299622\n",
      "Epoch: 0, Step: 39584, Loss: 0.8489580750465393\n",
      "Epoch: 0, Step: 39616, Loss: 0.875978946685791\n",
      "Epoch: 0, Step: 39648, Loss: 0.8094560503959656\n",
      "Epoch: 0, Step: 39680, Loss: 0.7960406541824341\n",
      "Epoch: 0, Step: 39712, Loss: 0.8525528311729431\n",
      "Epoch: 0, Step: 39744, Loss: 0.7105064988136292\n",
      "Epoch: 0, Step: 39776, Loss: 0.8241018056869507\n",
      "Epoch: 0, Step: 39808, Loss: 0.791824221611023\n",
      "Epoch: 0, Step: 39840, Loss: 0.8762126564979553\n",
      "Epoch: 0, Step: 39872, Loss: 0.8441721796989441\n",
      "Epoch: 0, Step: 39904, Loss: 0.878188967704773\n",
      "Epoch: 0, Step: 39936, Loss: 0.9470612406730652\n",
      "Epoch: 0, Step: 39968, Loss: 0.8091213703155518\n",
      "Epoch: 0, Step: 40000, Loss: 0.8537001013755798\n",
      "Epoch: 0, Step: 40032, Loss: 0.8973788022994995\n",
      "Epoch: 0, Step: 40064, Loss: 0.7311131954193115\n",
      "Epoch: 0, Step: 40096, Loss: 0.933150053024292\n",
      "Epoch: 0, Step: 40128, Loss: 0.8680011034011841\n",
      "Epoch: 0, Step: 40160, Loss: 0.9258295297622681\n",
      "Epoch: 0, Step: 40192, Loss: 0.8029743432998657\n",
      "Epoch: 0, Step: 40224, Loss: 0.9025341272354126\n",
      "Epoch: 0, Step: 40256, Loss: 0.8974415063858032\n",
      "Epoch: 0, Step: 40288, Loss: 0.785947859287262\n",
      "Epoch: 0, Step: 40320, Loss: 0.9407733082771301\n",
      "Epoch: 0, Step: 40352, Loss: 0.8125725388526917\n",
      "Epoch: 0, Step: 40384, Loss: 0.7591068148612976\n",
      "Epoch: 0, Step: 40416, Loss: 0.8644025921821594\n",
      "Epoch: 0, Step: 40448, Loss: 0.8692318201065063\n",
      "Epoch: 0, Step: 40480, Loss: 0.7758668065071106\n",
      "Epoch: 0, Step: 40512, Loss: 0.8659656047821045\n",
      "Epoch: 0, Step: 40544, Loss: 0.8410680294036865\n",
      "Epoch: 0, Step: 40576, Loss: 0.8629241585731506\n",
      "Epoch: 0, Step: 40608, Loss: 0.9320823550224304\n",
      "Epoch: 0, Step: 40640, Loss: 0.8721495270729065\n",
      "Epoch: 0, Step: 40672, Loss: 0.8387431502342224\n",
      "Epoch: 0, Step: 40704, Loss: 0.8739894032478333\n",
      "Epoch: 0, Step: 40736, Loss: 0.8403178453445435\n",
      "Epoch: 0, Step: 40768, Loss: 0.7670924067497253\n",
      "Epoch: 0, Step: 40800, Loss: 0.804562509059906\n",
      "Epoch: 0, Step: 40832, Loss: 0.8324900269508362\n",
      "Epoch: 0, Step: 40864, Loss: 0.8368335962295532\n",
      "Epoch: 0, Step: 40896, Loss: 0.9198641777038574\n",
      "Epoch: 0, Step: 40928, Loss: 0.949048638343811\n",
      "Epoch: 0, Step: 40960, Loss: 0.8969548344612122\n",
      "Epoch: 0, Step: 40992, Loss: 0.8577516078948975\n",
      "Epoch: 0, Step: 41024, Loss: 0.910205602645874\n",
      "Epoch: 0, Step: 41056, Loss: 0.800652801990509\n",
      "Epoch: 0, Step: 41088, Loss: 0.82813560962677\n",
      "Epoch: 0, Step: 41120, Loss: 0.7307079434394836\n",
      "Epoch: 0, Step: 41152, Loss: 0.8040695786476135\n",
      "Epoch: 0, Step: 41184, Loss: 0.7586203217506409\n",
      "Epoch: 0, Step: 41216, Loss: 0.8401506543159485\n",
      "Epoch: 0, Step: 41248, Loss: 0.9095791578292847\n",
      "Epoch: 0, Step: 41280, Loss: 0.8396202921867371\n",
      "Epoch: 0, Step: 41312, Loss: 0.925433337688446\n",
      "Epoch: 0, Step: 41344, Loss: 0.8500986695289612\n",
      "Epoch: 0, Step: 41376, Loss: 0.7898510694503784\n",
      "Epoch: 0, Step: 41408, Loss: 0.7444771528244019\n",
      "Epoch: 0, Step: 41440, Loss: 0.9207115769386292\n",
      "Epoch: 0, Step: 41472, Loss: 0.8199889659881592\n",
      "Epoch: 0, Step: 41504, Loss: 0.8276646733283997\n",
      "Epoch: 0, Step: 41536, Loss: 0.9116382598876953\n",
      "Epoch: 0, Step: 41568, Loss: 0.8018097281455994\n",
      "Epoch: 0, Step: 41600, Loss: 0.8576570153236389\n",
      "Epoch: 0, Step: 41632, Loss: 0.8764790296554565\n",
      "Epoch: 0, Step: 41664, Loss: 0.804567277431488\n",
      "Epoch: 0, Step: 41696, Loss: 0.7859072089195251\n",
      "Epoch: 0, Step: 41728, Loss: 0.7345010042190552\n",
      "Epoch: 0, Step: 41760, Loss: 0.8705925941467285\n",
      "Epoch: 0, Step: 41792, Loss: 0.8209350109100342\n",
      "Epoch: 0, Step: 41824, Loss: 0.8987646698951721\n",
      "Epoch: 0, Step: 41856, Loss: 0.8104162812232971\n",
      "Epoch: 0, Step: 41888, Loss: 0.8044335246086121\n",
      "Epoch: 0, Step: 41920, Loss: 0.8570418953895569\n",
      "Epoch: 0, Step: 41952, Loss: 0.8053799271583557\n",
      "Epoch: 0, Step: 41984, Loss: 0.8465259075164795\n",
      "Epoch: 0, Step: 42016, Loss: 0.8375118374824524\n",
      "Epoch: 0, Step: 42048, Loss: 0.8902851343154907\n",
      "Epoch: 0, Step: 42080, Loss: 0.8133609890937805\n",
      "Epoch: 0, Step: 42112, Loss: 0.8365967869758606\n",
      "Epoch: 0, Step: 42144, Loss: 0.9213646650314331\n",
      "Epoch: 0, Step: 42176, Loss: 0.9158774018287659\n",
      "Epoch: 0, Step: 42208, Loss: 0.8585649132728577\n",
      "Epoch: 0, Step: 42240, Loss: 0.8056935667991638\n",
      "Epoch: 0, Step: 42272, Loss: 0.8495076894760132\n",
      "Epoch: 0, Step: 42304, Loss: 0.7969059348106384\n",
      "Epoch: 0, Step: 42336, Loss: 0.7919708490371704\n",
      "Epoch: 0, Step: 42368, Loss: 0.8137148022651672\n",
      "Epoch: 0, Step: 42400, Loss: 0.8437268733978271\n",
      "Epoch: 0, Step: 42432, Loss: 0.7326058149337769\n",
      "Epoch: 0, Step: 42464, Loss: 0.7743760943412781\n",
      "Epoch: 0, Step: 42496, Loss: 0.7513800263404846\n",
      "Epoch: 0, Step: 42528, Loss: 0.8851622343063354\n",
      "Epoch: 0, Step: 42560, Loss: 0.8486310243606567\n",
      "Epoch: 0, Step: 42592, Loss: 0.7663370370864868\n",
      "Epoch: 0, Step: 42624, Loss: 0.8587120771408081\n",
      "Epoch: 0, Step: 42656, Loss: 0.8449622988700867\n",
      "Epoch: 0, Step: 42688, Loss: 0.8522014021873474\n",
      "Epoch: 0, Step: 42720, Loss: 0.7308433055877686\n",
      "Epoch: 0, Step: 42752, Loss: 0.7915869355201721\n",
      "Epoch: 0, Step: 42784, Loss: 0.8271601796150208\n",
      "Epoch: 0, Step: 42816, Loss: 0.7277750372886658\n",
      "Epoch: 0, Step: 42848, Loss: 0.9248772263526917\n",
      "Epoch: 0, Step: 42880, Loss: 0.7651949524879456\n",
      "Epoch: 0, Step: 42912, Loss: 0.9025262594223022\n",
      "Epoch: 0, Step: 42944, Loss: 0.8203176259994507\n",
      "Epoch: 0, Step: 42976, Loss: 0.8113380074501038\n",
      "Epoch: 0, Step: 43008, Loss: 0.8515828251838684\n",
      "Epoch: 0, Step: 43040, Loss: 0.8306041955947876\n",
      "Epoch: 0, Step: 43072, Loss: 0.7983284592628479\n",
      "Epoch: 0, Step: 43104, Loss: 0.7636615633964539\n",
      "Epoch: 0, Step: 43136, Loss: 0.846841037273407\n",
      "Epoch: 0, Step: 43168, Loss: 0.7567754983901978\n",
      "Epoch: 0, Step: 43200, Loss: 0.7458673715591431\n",
      "Epoch: 0, Step: 43232, Loss: 0.8738306760787964\n",
      "Epoch: 0, Step: 43264, Loss: 0.8743230104446411\n",
      "Epoch: 0, Step: 43296, Loss: 0.7556449174880981\n",
      "Epoch: 0, Step: 43328, Loss: 0.8142254948616028\n",
      "Epoch: 0, Step: 43360, Loss: 0.7001704573631287\n",
      "Epoch: 0, Step: 43392, Loss: 0.7985900044441223\n",
      "Epoch: 0, Step: 43424, Loss: 0.7352085709571838\n",
      "Epoch: 0, Step: 43456, Loss: 0.8828644752502441\n",
      "Epoch: 0, Step: 43488, Loss: 0.8138512372970581\n",
      "Epoch: 0, Step: 43520, Loss: 0.8152645230293274\n",
      "Epoch: 0, Step: 43552, Loss: 0.7952044010162354\n",
      "Epoch: 0, Step: 43584, Loss: 0.8697764277458191\n",
      "Epoch: 0, Step: 43616, Loss: 0.9359872341156006\n",
      "Epoch: 0, Step: 43648, Loss: 0.7316201329231262\n",
      "Epoch: 0, Step: 43680, Loss: 0.9090779423713684\n",
      "Epoch: 0, Step: 43712, Loss: 0.9922205805778503\n",
      "Epoch: 0, Step: 43744, Loss: 0.8027531504631042\n",
      "Epoch: 0, Step: 43776, Loss: 0.8673778176307678\n",
      "Epoch: 0, Step: 43808, Loss: 0.795382559299469\n",
      "Epoch: 0, Step: 43840, Loss: 0.8315280079841614\n",
      "Epoch: 0, Step: 43872, Loss: 0.7496368885040283\n",
      "Epoch: 0, Step: 43904, Loss: 0.9816858768463135\n",
      "Epoch: 0, Step: 43936, Loss: 0.8247808814048767\n",
      "Epoch: 0, Step: 43968, Loss: 0.7596796154975891\n",
      "Epoch: 0, Step: 44000, Loss: 0.8828158378601074\n",
      "Epoch: 0, Step: 44032, Loss: 0.9516807794570923\n",
      "Epoch: 0, Step: 44064, Loss: 0.7717494368553162\n",
      "Epoch: 0, Step: 44096, Loss: 0.9023994207382202\n",
      "Epoch: 0, Step: 44128, Loss: 0.7151302695274353\n",
      "Epoch: 0, Step: 44160, Loss: 0.7966778874397278\n",
      "Epoch: 0, Step: 44192, Loss: 0.8853530883789062\n",
      "Epoch: 0, Step: 44224, Loss: 0.904295027256012\n",
      "Epoch: 0, Step: 44256, Loss: 0.7665110230445862\n",
      "Epoch: 0, Step: 44288, Loss: 0.7169806361198425\n",
      "Epoch: 0, Step: 44320, Loss: 0.8126452565193176\n",
      "Epoch: 0, Step: 44352, Loss: 0.7546011209487915\n",
      "Epoch: 0, Step: 44384, Loss: 0.8241116404533386\n",
      "Epoch: 0, Step: 44416, Loss: 0.7434872388839722\n",
      "Epoch: 0, Step: 44448, Loss: 0.8356403708457947\n",
      "Epoch: 0, Step: 44480, Loss: 0.8562179207801819\n",
      "Epoch: 0, Step: 44512, Loss: 0.852217972278595\n",
      "Epoch: 0, Step: 44544, Loss: 0.8487600684165955\n",
      "Epoch: 0, Step: 44576, Loss: 0.8343587517738342\n",
      "Epoch: 0, Step: 44608, Loss: 0.8007819652557373\n",
      "Epoch: 0, Step: 44640, Loss: 0.9152632355690002\n",
      "Epoch: 0, Step: 44672, Loss: 0.8508473634719849\n",
      "Epoch: 0, Step: 44704, Loss: 0.814742386341095\n",
      "Epoch: 0, Step: 44736, Loss: 0.8224409818649292\n",
      "Epoch: 0, Step: 44768, Loss: 0.7968378663063049\n",
      "Epoch: 0, Step: 44800, Loss: 0.856143057346344\n",
      "Epoch: 0, Step: 44832, Loss: 0.8441603183746338\n",
      "Epoch: 0, Step: 44864, Loss: 0.760181188583374\n",
      "Epoch: 0, Step: 44896, Loss: 0.8715792298316956\n",
      "Epoch: 0, Step: 44928, Loss: 0.8482016324996948\n",
      "Epoch: 0, Step: 44960, Loss: 0.8014296293258667\n",
      "Epoch: 0, Step: 44992, Loss: 0.8302198648452759\n",
      "Epoch: 0, Step: 45024, Loss: 0.8968822360038757\n",
      "Epoch: 0, Step: 45056, Loss: 0.7870737314224243\n",
      "Epoch: 0, Step: 45088, Loss: 0.7288110852241516\n",
      "Epoch: 0, Step: 45120, Loss: 0.8547980189323425\n",
      "Epoch: 0, Step: 45152, Loss: 0.8957058787345886\n",
      "Epoch: 0, Step: 45184, Loss: 0.7580364346504211\n",
      "Epoch: 0, Step: 45216, Loss: 0.74425208568573\n",
      "Epoch: 0, Step: 45248, Loss: 0.7753368616104126\n",
      "Epoch: 0, Step: 45280, Loss: 0.8774062991142273\n",
      "Epoch: 0, Step: 45312, Loss: 0.7867887020111084\n",
      "Epoch: 0, Step: 45344, Loss: 0.758583664894104\n",
      "Epoch: 0, Step: 45376, Loss: 0.865536093711853\n",
      "Epoch: 0, Step: 45408, Loss: 0.840692937374115\n",
      "Epoch: 0, Step: 45440, Loss: 0.7542303800582886\n",
      "Epoch: 0, Step: 45472, Loss: 0.8244166970252991\n",
      "Epoch: 0, Step: 45504, Loss: 0.8241354823112488\n",
      "Epoch: 0, Step: 45536, Loss: 0.8318853974342346\n",
      "Epoch: 0, Step: 45568, Loss: 0.8385025262832642\n",
      "Epoch: 0, Step: 45600, Loss: 0.7903795838356018\n",
      "Epoch: 0, Step: 45632, Loss: 0.7811380624771118\n",
      "Epoch: 0, Step: 45664, Loss: 0.7966898679733276\n",
      "Epoch: 0, Step: 45696, Loss: 0.825219452381134\n",
      "Epoch: 0, Step: 45728, Loss: 0.8409141898155212\n",
      "Epoch: 0, Step: 45760, Loss: 0.7669345736503601\n",
      "Epoch: 0, Step: 45792, Loss: 0.811407208442688\n",
      "Epoch: 0, Step: 45824, Loss: 0.8435543775558472\n",
      "Epoch: 0, Step: 45856, Loss: 0.8934522271156311\n",
      "Epoch: 0, Step: 45888, Loss: 0.7170635461807251\n",
      "Epoch: 0, Step: 45920, Loss: 0.6869617700576782\n",
      "Epoch: 0, Step: 45952, Loss: 0.7906064391136169\n",
      "Epoch: 0, Step: 45984, Loss: 0.7194927930831909\n",
      "Epoch: 0, Step: 46016, Loss: 0.7901844382286072\n",
      "Epoch: 0, Step: 46048, Loss: 0.8415442705154419\n",
      "Epoch: 0, Step: 46080, Loss: 0.8183363676071167\n",
      "Epoch: 0, Step: 46112, Loss: 0.8990584015846252\n",
      "Epoch: 0, Step: 46144, Loss: 0.8373051285743713\n",
      "Epoch: 0, Step: 46176, Loss: 0.7993466854095459\n",
      "Epoch: 0, Step: 46208, Loss: 0.8515655398368835\n",
      "Epoch: 0, Step: 46240, Loss: 0.8490574955940247\n",
      "Epoch: 0, Step: 46272, Loss: 0.8371914625167847\n",
      "Epoch: 0, Step: 46304, Loss: 0.8249075412750244\n",
      "Epoch: 0, Step: 46336, Loss: 0.9639221429824829\n",
      "Epoch: 0, Step: 46368, Loss: 0.8180431127548218\n",
      "Epoch: 0, Step: 46400, Loss: 0.8842079043388367\n",
      "Epoch: 0, Step: 46432, Loss: 0.7825269103050232\n",
      "Epoch: 0, Step: 46464, Loss: 0.960772693157196\n",
      "Epoch: 0, Step: 46496, Loss: 0.8885047435760498\n",
      "Epoch: 0, Step: 46528, Loss: 0.8897415995597839\n",
      "Epoch: 0, Step: 46560, Loss: 0.7975942492485046\n",
      "Epoch: 0, Step: 46592, Loss: 0.8383748531341553\n",
      "Epoch: 0, Step: 46624, Loss: 0.7591904401779175\n",
      "Epoch: 0, Step: 46656, Loss: 0.7146204710006714\n",
      "Epoch: 0, Step: 46688, Loss: 0.830575704574585\n",
      "Epoch: 0, Step: 46720, Loss: 0.9654674530029297\n",
      "Epoch: 0, Step: 46752, Loss: 0.7668192982673645\n",
      "Epoch: 0, Step: 46784, Loss: 0.7539985179901123\n",
      "Epoch: 0, Step: 46816, Loss: 0.7549678683280945\n",
      "Epoch: 0, Step: 46848, Loss: 0.8998529314994812\n",
      "Epoch: 0, Step: 46880, Loss: 0.8657203912734985\n",
      "Epoch: 0, Step: 46912, Loss: 0.8720127940177917\n",
      "Epoch: 0, Step: 46944, Loss: 0.8687267899513245\n",
      "Epoch: 0, Step: 46976, Loss: 0.775107204914093\n",
      "Epoch: 0, Step: 47008, Loss: 0.8101561665534973\n",
      "Epoch: 0, Step: 47040, Loss: 0.8416913151741028\n",
      "Epoch: 0, Step: 47072, Loss: 0.7969892024993896\n",
      "Epoch: 0, Step: 47104, Loss: 0.7789732813835144\n",
      "Epoch: 0, Step: 47136, Loss: 0.8650876879692078\n",
      "Epoch: 0, Step: 47168, Loss: 0.795621931552887\n",
      "Epoch: 0, Step: 47200, Loss: 0.8093059062957764\n",
      "Epoch: 0, Step: 47232, Loss: 0.8558965921401978\n",
      "Epoch: 0, Step: 47264, Loss: 0.8325923085212708\n",
      "Epoch: 0, Step: 47296, Loss: 0.8494793176651001\n",
      "Epoch: 0, Step: 47328, Loss: 0.7961811423301697\n",
      "Epoch: 0, Step: 47360, Loss: 0.7787296175956726\n",
      "Epoch: 0, Step: 47392, Loss: 0.7837626934051514\n",
      "Epoch: 0, Step: 47424, Loss: 0.7566254734992981\n",
      "Epoch: 0, Step: 47456, Loss: 0.7571402788162231\n",
      "Epoch: 0, Step: 47488, Loss: 0.8385512828826904\n",
      "Epoch: 0, Step: 47520, Loss: 0.7897478938102722\n",
      "Epoch: 0, Step: 47552, Loss: 0.8738277554512024\n",
      "Epoch: 0, Step: 47584, Loss: 0.8269270062446594\n",
      "Epoch: 0, Step: 47616, Loss: 0.8653360605239868\n",
      "Epoch: 0, Step: 47648, Loss: 0.8051669001579285\n",
      "Epoch: 0, Step: 47680, Loss: 0.766068696975708\n",
      "Epoch: 0, Step: 47712, Loss: 0.8077126145362854\n",
      "Epoch: 0, Step: 47744, Loss: 0.8247470259666443\n",
      "Epoch: 0, Step: 47776, Loss: 0.870247483253479\n",
      "Epoch: 0, Step: 47808, Loss: 0.9001522660255432\n",
      "Epoch: 0, Step: 47840, Loss: 0.8581224679946899\n",
      "Epoch: 0, Step: 47872, Loss: 0.9235969185829163\n",
      "Epoch: 0, Step: 47904, Loss: 0.7012827396392822\n",
      "Epoch: 0, Step: 47936, Loss: 0.7449856996536255\n",
      "Epoch: 0, Step: 47968, Loss: 0.8910006284713745\n",
      "Epoch: 0, Step: 48000, Loss: 0.8400245904922485\n",
      "Epoch: 0, Step: 48032, Loss: 0.6953065395355225\n",
      "Epoch: 0, Step: 48064, Loss: 0.7852307558059692\n",
      "Epoch: 0, Step: 48096, Loss: 0.807153582572937\n",
      "Epoch: 0, Step: 48128, Loss: 0.7807977795600891\n",
      "Epoch: 0, Step: 48160, Loss: 0.7738207578659058\n",
      "Epoch: 0, Step: 48192, Loss: 0.8913017511367798\n",
      "Epoch: 0, Step: 48224, Loss: 0.8975250124931335\n",
      "Epoch: 0, Step: 48256, Loss: 0.8030470609664917\n",
      "Epoch: 0, Step: 48288, Loss: 0.8415977358818054\n",
      "Epoch: 0, Step: 48320, Loss: 0.8942378163337708\n",
      "Epoch: 0, Step: 48352, Loss: 0.8688390851020813\n",
      "Epoch: 0, Step: 48384, Loss: 0.7206612825393677\n",
      "Epoch: 0, Step: 48416, Loss: 0.7540919780731201\n",
      "Epoch: 0, Step: 48448, Loss: 0.8002027869224548\n",
      "Epoch: 0, Step: 48480, Loss: 0.9086827039718628\n",
      "Epoch: 0, Step: 48512, Loss: 0.8368123173713684\n",
      "Epoch: 0, Step: 48544, Loss: 0.8555613160133362\n",
      "Epoch: 0, Step: 48576, Loss: 0.8081411719322205\n",
      "Epoch: 0, Step: 48608, Loss: 0.8632571697235107\n",
      "Epoch: 0, Step: 48640, Loss: 0.8573435544967651\n",
      "Epoch: 0, Step: 48672, Loss: 0.8470098376274109\n",
      "Epoch: 0, Step: 48704, Loss: 0.7590601444244385\n",
      "Epoch: 0, Step: 48736, Loss: 0.6994855403900146\n",
      "Epoch: 0, Step: 48768, Loss: 0.8067461848258972\n",
      "Epoch: 0, Step: 48800, Loss: 0.8177107572555542\n",
      "Epoch: 0, Step: 48832, Loss: 0.9611994028091431\n",
      "Epoch: 0, Step: 48864, Loss: 0.7570973038673401\n",
      "Epoch: 0, Step: 48896, Loss: 0.7896831631660461\n",
      "Epoch: 0, Step: 48928, Loss: 0.9696261286735535\n",
      "Epoch: 0, Step: 48960, Loss: 0.8028469681739807\n",
      "Epoch: 0, Step: 48992, Loss: 0.8302406668663025\n",
      "Epoch: 0, Step: 49024, Loss: 0.8346403241157532\n",
      "Epoch: 0, Step: 49056, Loss: 0.8062599897384644\n",
      "Epoch: 0, Step: 49088, Loss: 0.8219171166419983\n",
      "Epoch: 0, Step: 49120, Loss: 0.8436201214790344\n",
      "Epoch: 0, Step: 49152, Loss: 0.8448755741119385\n",
      "Epoch: 0, Step: 49184, Loss: 0.8200585842132568\n",
      "Epoch: 0, Step: 49216, Loss: 0.9262700080871582\n",
      "Epoch: 0, Step: 49248, Loss: 0.7796146273612976\n",
      "Epoch: 0, Step: 49280, Loss: 0.8012303113937378\n",
      "Epoch: 0, Step: 49312, Loss: 0.8592197895050049\n",
      "Epoch: 0, Step: 49344, Loss: 0.8209986090660095\n",
      "Epoch: 0, Step: 49376, Loss: 0.8910209536552429\n",
      "Epoch: 0, Step: 49408, Loss: 0.7501078248023987\n",
      "Epoch: 0, Step: 49440, Loss: 0.8065915703773499\n",
      "Epoch: 0, Step: 49472, Loss: 0.8447472453117371\n",
      "Epoch: 0, Step: 49504, Loss: 0.8365268111228943\n",
      "Epoch: 0, Step: 49536, Loss: 0.8581575155258179\n",
      "Epoch: 0, Step: 49568, Loss: 0.8093113303184509\n",
      "Epoch: 0, Step: 49600, Loss: 0.8400812745094299\n",
      "Epoch: 0, Step: 49632, Loss: 0.6582692265510559\n",
      "Epoch: 0, Step: 49664, Loss: 0.7910044193267822\n",
      "Epoch: 0, Step: 49696, Loss: 0.7399853467941284\n",
      "Epoch: 0, Step: 49728, Loss: 0.9690553545951843\n",
      "Epoch: 0, Step: 49760, Loss: 0.8472363948822021\n",
      "Epoch: 0, Step: 49792, Loss: 0.7368273138999939\n",
      "Epoch: 0, Step: 49824, Loss: 0.8414380550384521\n",
      "Epoch: 0, Step: 49856, Loss: 0.7733760476112366\n",
      "Epoch: 0, Step: 49888, Loss: 0.8863063454627991\n",
      "Epoch: 0, Step: 49920, Loss: 0.835895299911499\n",
      "Epoch: 0, Step: 49952, Loss: 0.8641693592071533\n",
      "Epoch: 0, Step: 49984, Loss: 0.7629796862602234\n",
      "Epoch: 0, Step: 50016, Loss: 0.7862393260002136\n",
      "Epoch: 0, Step: 50048, Loss: 0.8235204815864563\n",
      "Epoch: 0, Step: 50080, Loss: 0.7709510922431946\n",
      "Epoch: 0, Step: 50112, Loss: 0.7779631018638611\n",
      "Epoch: 0, Step: 50144, Loss: 0.8927609324455261\n",
      "Epoch: 0, Step: 50176, Loss: 0.7501305341720581\n",
      "Epoch: 0, Step: 50208, Loss: 0.8418971300125122\n",
      "Epoch: 0, Step: 50240, Loss: 0.8379300236701965\n",
      "Epoch: 0, Step: 50272, Loss: 0.87660813331604\n",
      "Epoch: 0, Step: 50304, Loss: 0.8409603834152222\n",
      "Epoch: 0, Step: 50336, Loss: 0.7795282602310181\n",
      "Epoch: 0, Step: 50368, Loss: 0.7263953685760498\n",
      "Epoch: 0, Step: 50400, Loss: 0.9188763499259949\n",
      "Epoch: 0, Step: 50432, Loss: 0.712781548500061\n",
      "Epoch: 0, Step: 50464, Loss: 0.9258372783660889\n",
      "Epoch: 0, Step: 50496, Loss: 0.8009926676750183\n",
      "Epoch: 0, Step: 50528, Loss: 0.8213117122650146\n",
      "Epoch: 0, Step: 50560, Loss: 0.8042066693305969\n",
      "Epoch: 0, Step: 50592, Loss: 0.8490951061248779\n",
      "Epoch: 0, Step: 50624, Loss: 0.7901655435562134\n",
      "Epoch: 0, Step: 50656, Loss: 0.7605410218238831\n",
      "Epoch: 0, Step: 50688, Loss: 0.8765807747840881\n",
      "Epoch: 0, Step: 50720, Loss: 0.8879204392433167\n",
      "Epoch: 0, Step: 50752, Loss: 0.819770872592926\n",
      "Epoch: 0, Step: 50784, Loss: 0.7910568714141846\n",
      "Epoch: 0, Step: 50816, Loss: 0.8285161256790161\n",
      "Epoch: 0, Step: 50848, Loss: 0.7242105603218079\n",
      "Epoch: 0, Step: 50880, Loss: 0.8695445656776428\n",
      "Epoch: 0, Step: 50912, Loss: 0.880010187625885\n",
      "Epoch: 0, Step: 50944, Loss: 0.7290962934494019\n",
      "Epoch: 0, Step: 50976, Loss: 0.8019090890884399\n",
      "Epoch: 0, Step: 51008, Loss: 0.7508141398429871\n",
      "Epoch: 0, Step: 51040, Loss: 0.816278874874115\n",
      "Epoch: 0, Step: 51072, Loss: 0.7951077222824097\n",
      "Epoch: 0, Step: 51104, Loss: 0.9061244130134583\n",
      "Epoch: 0, Step: 51136, Loss: 0.6686554551124573\n",
      "Epoch: 0, Step: 51168, Loss: 0.8396462202072144\n",
      "Epoch: 0, Step: 51200, Loss: 0.8350110650062561\n",
      "Epoch: 0, Step: 51232, Loss: 0.7871608138084412\n",
      "Epoch: 0, Step: 51264, Loss: 0.7718462347984314\n",
      "Epoch: 0, Step: 51296, Loss: 0.8140271306037903\n",
      "Epoch: 0, Step: 51328, Loss: 0.863091230392456\n",
      "Epoch: 0, Step: 51360, Loss: 0.8470242619514465\n",
      "Epoch: 0, Step: 51392, Loss: 0.8948742151260376\n",
      "Epoch: 0, Step: 51424, Loss: 0.8615243434906006\n",
      "Epoch: 0, Step: 51456, Loss: 0.7284248471260071\n",
      "Epoch: 0, Step: 51488, Loss: 0.8824453949928284\n",
      "Epoch: 0, Step: 51520, Loss: 0.8428613543510437\n",
      "Epoch: 0, Step: 51552, Loss: 0.8838250041007996\n",
      "Epoch: 0, Step: 51584, Loss: 0.8021404147148132\n",
      "Epoch: 0, Step: 51616, Loss: 0.8196216821670532\n",
      "Epoch: 0, Step: 51648, Loss: 0.7559107542037964\n",
      "Epoch: 0, Step: 51680, Loss: 0.8136247992515564\n",
      "Epoch: 0, Step: 51712, Loss: 0.8810505867004395\n",
      "Epoch: 0, Step: 51744, Loss: 0.7738577127456665\n",
      "Epoch: 0, Step: 51776, Loss: 0.8858602046966553\n",
      "Epoch: 0, Step: 51808, Loss: 0.8305647373199463\n",
      "Epoch: 0, Step: 51840, Loss: 0.7864941358566284\n",
      "Epoch: 0, Step: 51872, Loss: 0.8083447217941284\n",
      "Epoch: 0, Step: 51904, Loss: 0.7755647897720337\n",
      "Epoch: 0, Step: 51936, Loss: 0.7945758104324341\n",
      "Epoch: 0, Step: 51968, Loss: 0.7576587200164795\n",
      "Epoch: 0, Step: 52000, Loss: 0.7759150266647339\n",
      "Epoch: 0, Step: 52032, Loss: 0.7837606072425842\n",
      "Epoch: 0, Step: 52064, Loss: 0.9166789650917053\n",
      "Epoch: 0, Step: 52096, Loss: 0.899549126625061\n",
      "Epoch: 0, Step: 52128, Loss: 0.822553813457489\n",
      "Epoch: 0, Step: 52160, Loss: 0.7711494565010071\n",
      "Epoch: 0, Step: 52192, Loss: 0.8576382398605347\n",
      "Epoch: 0, Step: 52224, Loss: 0.8166298270225525\n",
      "Epoch: 0, Step: 52256, Loss: 0.9660738110542297\n",
      "Epoch: 0, Step: 52288, Loss: 0.968464195728302\n",
      "Epoch: 0, Step: 52320, Loss: 0.8712353706359863\n",
      "Epoch: 0, Step: 52352, Loss: 0.8671026825904846\n",
      "Epoch: 0, Step: 52384, Loss: 0.6853073835372925\n",
      "Epoch: 0, Step: 52416, Loss: 0.8397759199142456\n",
      "Epoch: 0, Step: 52448, Loss: 0.8674076795578003\n",
      "Epoch: 0, Step: 52480, Loss: 0.8189606070518494\n",
      "Epoch: 0, Step: 52512, Loss: 0.8615565299987793\n",
      "Epoch: 0, Step: 52544, Loss: 0.8927604556083679\n",
      "Epoch: 0, Step: 52576, Loss: 0.936348557472229\n",
      "Epoch: 0, Step: 52608, Loss: 0.8865810036659241\n",
      "Epoch: 0, Step: 52640, Loss: 0.8396393656730652\n",
      "Epoch: 0, Step: 52672, Loss: 0.8271521925926208\n",
      "Epoch: 0, Step: 52704, Loss: 0.8206002712249756\n",
      "Epoch: 0, Step: 52736, Loss: 0.7785316109657288\n",
      "Epoch: 0, Step: 52768, Loss: 0.7916340231895447\n",
      "Epoch: 0, Step: 52800, Loss: 0.9187344312667847\n",
      "Epoch: 0, Step: 52832, Loss: 0.8079855442047119\n",
      "Epoch: 0, Step: 52864, Loss: 0.7707022428512573\n",
      "Epoch: 0, Step: 52896, Loss: 0.851047933101654\n",
      "Epoch: 0, Step: 52928, Loss: 0.8099116086959839\n",
      "Epoch: 0, Step: 52960, Loss: 0.8690866231918335\n",
      "Epoch: 0, Step: 52992, Loss: 0.756191074848175\n",
      "Epoch: 0, Step: 53024, Loss: 0.829329252243042\n",
      "Epoch: 0, Step: 53056, Loss: 0.8056048154830933\n",
      "Epoch: 0, Step: 53088, Loss: 0.8266531229019165\n",
      "Epoch: 0, Step: 53120, Loss: 0.9410620331764221\n",
      "Epoch: 0, Step: 53152, Loss: 0.8343150615692139\n",
      "Epoch: 0, Step: 53184, Loss: 0.7625314593315125\n",
      "Epoch: 0, Step: 53216, Loss: 0.8377082347869873\n",
      "Epoch: 0, Step: 53248, Loss: 0.9065350294113159\n",
      "Epoch: 0, Step: 53280, Loss: 0.88541179895401\n",
      "Epoch: 0, Step: 53312, Loss: 0.8672010898590088\n",
      "Epoch: 0, Step: 53344, Loss: 0.902190089225769\n",
      "Epoch: 0, Step: 53376, Loss: 0.8734214901924133\n",
      "Epoch: 0, Step: 53408, Loss: 0.9095866680145264\n",
      "Epoch: 0, Step: 53440, Loss: 0.8808497190475464\n",
      "Epoch: 0, Step: 53472, Loss: 0.7788065671920776\n",
      "Epoch: 0, Step: 53504, Loss: 0.8356163501739502\n",
      "Epoch: 0, Step: 53536, Loss: 0.872261106967926\n",
      "Epoch: 0, Step: 53568, Loss: 0.9535454511642456\n",
      "Epoch: 0, Step: 53600, Loss: 0.7381295561790466\n",
      "Epoch: 0, Step: 53632, Loss: 0.9920634031295776\n",
      "Epoch: 0, Step: 53664, Loss: 0.8917657136917114\n",
      "Epoch: 0, Step: 53696, Loss: 0.8464572429656982\n",
      "Epoch: 0, Step: 53728, Loss: 0.7889476418495178\n",
      "Epoch: 0, Step: 53760, Loss: 0.7606188058853149\n",
      "Epoch: 0, Step: 53792, Loss: 0.8963301777839661\n",
      "Epoch: 0, Step: 53824, Loss: 0.7784008383750916\n",
      "Epoch: 0, Step: 53856, Loss: 0.8332929015159607\n",
      "Epoch: 0, Step: 53888, Loss: 0.7631928324699402\n",
      "Epoch: 0, Step: 53920, Loss: 0.7983591556549072\n",
      "Epoch: 0, Step: 53952, Loss: 0.8454106450080872\n",
      "Epoch: 0, Step: 53984, Loss: 0.8398486971855164\n",
      "Epoch: 0, Step: 54016, Loss: 0.7905838489532471\n",
      "Epoch: 0, Step: 54048, Loss: 0.8114030957221985\n",
      "Epoch: 0, Step: 54080, Loss: 0.7688403725624084\n",
      "Epoch: 0, Step: 54112, Loss: 0.8611963391304016\n",
      "Epoch: 0, Step: 54144, Loss: 0.6776984930038452\n",
      "Epoch: 0, Step: 54176, Loss: 0.7476991415023804\n",
      "Epoch: 0, Step: 54208, Loss: 0.8350422978401184\n",
      "Epoch: 0, Step: 54240, Loss: 0.8716884851455688\n",
      "Epoch: 0, Step: 54272, Loss: 0.8349969387054443\n",
      "Epoch: 0, Step: 54304, Loss: 0.9239950776100159\n",
      "Epoch: 0, Step: 54336, Loss: 0.9315462708473206\n",
      "Epoch: 0, Step: 54368, Loss: 0.8653583526611328\n",
      "Epoch: 0, Step: 54400, Loss: 0.8618575930595398\n",
      "Epoch: 0, Step: 54432, Loss: 0.7687731981277466\n",
      "Epoch: 0, Step: 54464, Loss: 0.7901583909988403\n",
      "Epoch: 0, Step: 54496, Loss: 0.7967371344566345\n",
      "Epoch: 0, Step: 54528, Loss: 0.8862517476081848\n",
      "Epoch: 0, Step: 54560, Loss: 0.9475128054618835\n",
      "Epoch: 0, Step: 54592, Loss: 0.835953950881958\n",
      "Epoch: 0, Step: 54624, Loss: 0.766622006893158\n",
      "Epoch: 0, Step: 54656, Loss: 0.8335347771644592\n",
      "Epoch: 0, Step: 54688, Loss: 0.8077871203422546\n",
      "Epoch: 0, Step: 54720, Loss: 0.699966549873352\n",
      "Epoch: 0, Step: 54752, Loss: 0.7089555859565735\n",
      "Epoch: 0, Step: 54784, Loss: 0.7206705808639526\n",
      "Epoch: 0, Step: 54816, Loss: 0.8354228138923645\n",
      "Epoch: 0, Step: 54848, Loss: 0.7845157384872437\n",
      "Epoch: 0, Step: 54880, Loss: 0.8516713380813599\n",
      "Epoch: 0, Step: 54912, Loss: 0.8680198192596436\n",
      "Epoch: 0, Step: 54944, Loss: 0.9518284797668457\n",
      "Epoch: 0, Step: 54976, Loss: 0.8068957924842834\n",
      "Epoch: 0, Step: 55008, Loss: 0.7713575959205627\n",
      "Epoch: 0, Step: 55040, Loss: 0.814272940158844\n",
      "Epoch: 0, Step: 55072, Loss: 0.8394679427146912\n",
      "Epoch: 0, Step: 55104, Loss: 0.8570226430892944\n",
      "Epoch: 0, Step: 55136, Loss: 0.8363562822341919\n",
      "Epoch: 0, Step: 55168, Loss: 0.9278300404548645\n",
      "Epoch: 0, Step: 55200, Loss: 0.8900783061981201\n",
      "Epoch: 0, Step: 55232, Loss: 0.7980584502220154\n",
      "Epoch: 0, Step: 55264, Loss: 0.7556023001670837\n",
      "Epoch: 0, Step: 55296, Loss: 0.8224765658378601\n",
      "Epoch: 0, Step: 55328, Loss: 0.7231594324111938\n",
      "Epoch: 0, Step: 55360, Loss: 0.7884471416473389\n",
      "Epoch: 0, Step: 55392, Loss: 0.7762017846107483\n",
      "Epoch: 0, Step: 55424, Loss: 0.8647595047950745\n",
      "Epoch: 0, Step: 55456, Loss: 0.8794814944267273\n",
      "Epoch: 0, Step: 55488, Loss: 0.7371017336845398\n",
      "Epoch: 0, Step: 55520, Loss: 0.8800073862075806\n",
      "Epoch: 0, Step: 55552, Loss: 0.7265847325325012\n",
      "Epoch: 0, Step: 55584, Loss: 0.9607348442077637\n",
      "Epoch: 0, Step: 55616, Loss: 0.7714000940322876\n",
      "Epoch: 0, Step: 55648, Loss: 0.8232356905937195\n",
      "Epoch: 0, Step: 55680, Loss: 0.8029717206954956\n",
      "Epoch: 0, Step: 55712, Loss: 0.9102767109870911\n",
      "Epoch: 0, Step: 55744, Loss: 0.9261098504066467\n",
      "Epoch: 0, Step: 55776, Loss: 0.7950063943862915\n",
      "Epoch: 0, Step: 55808, Loss: 0.7468575239181519\n",
      "Epoch: 0, Step: 55840, Loss: 0.8341301679611206\n",
      "Epoch: 0, Step: 55872, Loss: 0.7802000641822815\n",
      "Epoch: 0, Step: 55904, Loss: 0.7746641039848328\n",
      "Epoch: 0, Step: 55936, Loss: 0.7759470343589783\n",
      "Epoch: 0, Step: 55968, Loss: 0.8152878880500793\n",
      "Epoch: 0, Step: 56000, Loss: 0.7803754210472107\n",
      "Epoch: 0, Step: 56032, Loss: 0.824761152267456\n",
      "Epoch: 0, Step: 56064, Loss: 0.7774443030357361\n",
      "Epoch: 0, Step: 56096, Loss: 0.8343607187271118\n",
      "Epoch: 0, Step: 56128, Loss: 0.7840636372566223\n",
      "Epoch: 0, Step: 56160, Loss: 0.8892157673835754\n",
      "Epoch: 0, Step: 56192, Loss: 0.8381054997444153\n",
      "Epoch: 0, Step: 56224, Loss: 0.7411360144615173\n",
      "Epoch: 0, Step: 56256, Loss: 0.8882706165313721\n",
      "Epoch: 0, Step: 56288, Loss: 0.8557365536689758\n",
      "Epoch: 0, Step: 56320, Loss: 0.8800204396247864\n",
      "Epoch: 0, Step: 56352, Loss: 0.8838596940040588\n",
      "Epoch: 0, Step: 56384, Loss: 0.9400213956832886\n",
      "Epoch: 0, Step: 56416, Loss: 0.9371713399887085\n",
      "Epoch: 0, Step: 56448, Loss: 0.8179843425750732\n",
      "Epoch: 0, Step: 56480, Loss: 0.8300713896751404\n",
      "Epoch: 0, Step: 56512, Loss: 0.8645780682563782\n",
      "Epoch: 0, Step: 56544, Loss: 0.7815792560577393\n",
      "Epoch: 0, Step: 56576, Loss: 0.8396769165992737\n",
      "Epoch: 0, Step: 56608, Loss: 0.962059497833252\n",
      "Epoch: 0, Step: 56640, Loss: 0.9164777994155884\n",
      "Epoch: 0, Step: 56672, Loss: 0.8810763955116272\n",
      "Epoch: 0, Step: 56704, Loss: 0.8142186999320984\n",
      "Epoch: 0, Step: 56736, Loss: 0.8681245446205139\n",
      "Epoch: 0, Step: 56768, Loss: 0.7658014893531799\n",
      "Epoch: 0, Step: 56800, Loss: 0.8422157764434814\n",
      "Epoch: 0, Step: 56832, Loss: 0.8809794187545776\n",
      "Epoch: 0, Step: 56864, Loss: 0.7846271395683289\n",
      "Epoch: 0, Step: 56896, Loss: 0.742173969745636\n",
      "Epoch: 0, Step: 56928, Loss: 0.8402801156044006\n",
      "Epoch: 0, Step: 56960, Loss: 0.813561201095581\n",
      "Epoch: 0, Step: 56992, Loss: 0.7942880988121033\n",
      "Epoch: 0, Step: 57024, Loss: 0.8495806455612183\n",
      "Epoch: 0, Step: 57056, Loss: 0.8092120289802551\n",
      "Epoch: 0, Step: 57088, Loss: 0.8507659435272217\n",
      "Epoch: 0, Step: 57120, Loss: 0.7945531010627747\n",
      "Epoch: 0, Step: 57152, Loss: 0.8233149647712708\n",
      "Epoch: 0, Step: 57184, Loss: 0.7935649752616882\n",
      "Epoch: 0, Step: 57216, Loss: 0.8692923188209534\n",
      "Epoch: 0, Step: 57248, Loss: 0.7988672852516174\n",
      "Epoch: 0, Step: 57280, Loss: 0.7711812853813171\n",
      "Epoch: 0, Step: 57312, Loss: 0.8505656719207764\n",
      "Epoch: 0, Step: 57344, Loss: 0.7127010226249695\n",
      "Epoch: 0, Step: 57376, Loss: 0.7585619688034058\n",
      "Epoch: 0, Step: 57408, Loss: 0.7416501641273499\n",
      "Epoch: 0, Step: 57440, Loss: 0.8335829377174377\n",
      "Epoch: 0, Step: 57472, Loss: 0.7941585183143616\n",
      "Epoch: 0, Step: 57504, Loss: 0.8123742938041687\n",
      "Epoch: 0, Step: 57536, Loss: 0.7345184087753296\n",
      "Epoch: 0, Step: 57568, Loss: 0.8381620645523071\n",
      "Epoch: 0, Step: 57600, Loss: 0.7715022563934326\n",
      "Epoch: 0, Step: 57632, Loss: 0.9095907211303711\n",
      "Epoch: 0, Step: 57664, Loss: 0.7283970713615417\n",
      "Epoch: 0, Step: 57696, Loss: 0.7027751803398132\n",
      "Epoch: 0, Step: 57728, Loss: 0.8787958025932312\n",
      "Epoch: 0, Step: 57760, Loss: 0.7932221293449402\n",
      "Epoch: 0, Step: 57792, Loss: 0.7704401612281799\n",
      "Epoch: 0, Step: 57824, Loss: 0.8216298222541809\n",
      "Epoch: 0, Step: 57856, Loss: 0.8205407857894897\n",
      "Epoch: 0, Step: 57888, Loss: 0.7338230609893799\n",
      "Epoch: 0, Step: 57920, Loss: 0.80267733335495\n",
      "Epoch: 0, Step: 57952, Loss: 0.7962713837623596\n",
      "Epoch: 0, Step: 57984, Loss: 0.8999577760696411\n",
      "Epoch: 0, Step: 58016, Loss: 0.8784991502761841\n",
      "Epoch: 0, Step: 58048, Loss: 0.7531463503837585\n",
      "Epoch: 0, Step: 58080, Loss: 0.9206417798995972\n",
      "Epoch: 0, Step: 58112, Loss: 0.8060678839683533\n",
      "Epoch: 0, Step: 58144, Loss: 0.7915087342262268\n",
      "Epoch: 0, Step: 58176, Loss: 0.8964473605155945\n",
      "Epoch: 0, Step: 58208, Loss: 0.7412692308425903\n",
      "Epoch: 0, Step: 58240, Loss: 0.7988001704216003\n",
      "Epoch: 0, Step: 58272, Loss: 0.8244531154632568\n",
      "Epoch: 0, Step: 58304, Loss: 0.9304043054580688\n",
      "Epoch: 0, Step: 58336, Loss: 0.911145031452179\n",
      "Epoch: 0, Step: 58368, Loss: 0.7550159692764282\n",
      "Epoch: 0, Step: 58400, Loss: 0.8061220645904541\n",
      "Epoch: 0, Step: 58432, Loss: 0.843198835849762\n",
      "Epoch: 0, Step: 58464, Loss: 0.7878977060317993\n",
      "Epoch: 0, Step: 58496, Loss: 0.7681872844696045\n",
      "Epoch: 0, Step: 58528, Loss: 0.8247662782669067\n",
      "Epoch: 0, Step: 58560, Loss: 0.8247062563896179\n",
      "Epoch: 0, Step: 58592, Loss: 0.8071618676185608\n",
      "Epoch: 0, Step: 58624, Loss: 0.8227806687355042\n",
      "Epoch: 0, Step: 58656, Loss: 0.8317165970802307\n",
      "Epoch: 0, Step: 58688, Loss: 0.8731303811073303\n",
      "Epoch: 0, Step: 58720, Loss: 0.8449683785438538\n",
      "Epoch: 0, Step: 58752, Loss: 0.8619998097419739\n",
      "Epoch: 0, Step: 58784, Loss: 0.8951526880264282\n",
      "Epoch: 0, Step: 58816, Loss: 0.8212219476699829\n",
      "Epoch: 0, Step: 58848, Loss: 0.8394663333892822\n",
      "Epoch: 0, Step: 58880, Loss: 0.7808771729469299\n",
      "Epoch: 0, Step: 58912, Loss: 0.7901159524917603\n",
      "Epoch: 0, Step: 58944, Loss: 0.892682671546936\n",
      "Epoch: 0, Step: 58976, Loss: 0.8279730677604675\n",
      "Epoch: 0, Step: 59008, Loss: 0.8733769655227661\n",
      "Epoch: 0, Step: 59040, Loss: 0.7840112447738647\n",
      "Epoch: 0, Step: 59072, Loss: 0.8686066269874573\n",
      "Epoch: 0, Step: 59104, Loss: 0.9045297503471375\n",
      "Epoch: 0, Step: 59136, Loss: 0.8747749924659729\n",
      "Epoch: 0, Step: 59168, Loss: 0.8552411794662476\n",
      "Epoch: 0, Step: 59200, Loss: 0.8658350706100464\n",
      "Epoch: 0, Step: 59232, Loss: 0.8013246059417725\n",
      "Epoch: 0, Step: 59264, Loss: 0.8317223191261292\n",
      "Epoch: 0, Step: 59296, Loss: 0.861558735370636\n",
      "Epoch: 0, Step: 59328, Loss: 0.8921148180961609\n",
      "Epoch: 0, Step: 59360, Loss: 0.8824741244316101\n",
      "Epoch: 0, Step: 59392, Loss: 0.7919317483901978\n",
      "Epoch: 0, Step: 59424, Loss: 0.880251407623291\n",
      "Epoch: 0, Step: 59456, Loss: 0.8200624585151672\n",
      "Epoch: 0, Step: 59488, Loss: 0.8564837574958801\n",
      "Epoch: 0, Step: 59520, Loss: 0.8233256936073303\n",
      "Epoch: 0, Step: 59552, Loss: 0.8949257731437683\n",
      "Epoch: 0, Step: 59584, Loss: 0.8644731044769287\n",
      "Epoch: 0, Step: 59616, Loss: 0.934588611125946\n",
      "Epoch: 0, Step: 59648, Loss: 0.8040688633918762\n",
      "Epoch: 0, Step: 59680, Loss: 0.821236252784729\n",
      "Epoch: 0, Step: 59712, Loss: 0.8057129979133606\n",
      "Epoch: 0, Step: 59744, Loss: 0.8032519817352295\n",
      "Epoch: 0, Step: 59776, Loss: 0.8126669526100159\n",
      "Epoch: 0, Step: 59808, Loss: 0.762501060962677\n",
      "Epoch: 0, Step: 59840, Loss: 0.7414388060569763\n",
      "Epoch: 0, Step: 59872, Loss: 0.7458550930023193\n",
      "Epoch: 0, Step: 59904, Loss: 0.8341543674468994\n",
      "Epoch: 0, Step: 59936, Loss: 0.8519212603569031\n",
      "Epoch: 0, Step: 59968, Loss: 0.8883446455001831\n",
      "Epoch: 0, Step: 60000, Loss: 0.9043291807174683\n",
      "Epoch: 0, Step: 60032, Loss: 0.8267932534217834\n",
      "Epoch: 0, Step: 60064, Loss: 0.8700394630432129\n",
      "Epoch: 0, Step: 60096, Loss: 0.8019929528236389\n",
      "Epoch: 0, Step: 60128, Loss: 0.8714529275894165\n",
      "Epoch: 0, Step: 60160, Loss: 0.8773738145828247\n",
      "Epoch: 0, Step: 60192, Loss: 0.8302025198936462\n",
      "Epoch: 0, Step: 60224, Loss: 0.8509103655815125\n",
      "Epoch: 0, Step: 60256, Loss: 0.8556328415870667\n",
      "Epoch: 0, Step: 60288, Loss: 0.801594078540802\n",
      "Epoch: 0, Step: 60320, Loss: 0.8340258002281189\n",
      "Epoch: 0, Step: 60352, Loss: 0.9096840620040894\n",
      "Epoch: 0, Step: 60384, Loss: 0.7515898942947388\n",
      "Epoch: 0, Step: 60416, Loss: 0.9943425059318542\n",
      "Epoch: 0, Step: 60448, Loss: 0.7672627568244934\n",
      "Epoch: 0, Step: 60480, Loss: 0.7563140988349915\n",
      "Epoch: 0, Step: 60512, Loss: 0.8672669529914856\n",
      "Epoch: 0, Step: 60544, Loss: 0.7849994897842407\n",
      "Epoch: 0, Step: 60576, Loss: 0.7498019933700562\n",
      "Epoch: 0, Step: 60608, Loss: 0.8923506736755371\n",
      "Epoch: 0, Step: 60640, Loss: 0.8549783229827881\n",
      "Epoch: 0, Step: 60672, Loss: 0.9642512798309326\n",
      "Epoch: 0, Step: 60704, Loss: 0.9513031244277954\n",
      "Epoch: 0, Step: 60736, Loss: 0.8471554517745972\n",
      "Epoch: 0, Step: 60768, Loss: 0.886911928653717\n",
      "Epoch: 0, Step: 60800, Loss: 0.8175456523895264\n",
      "Epoch: 0, Step: 60832, Loss: 0.8225685358047485\n",
      "Epoch: 0, Step: 60864, Loss: 0.7149335741996765\n",
      "Epoch: 0, Step: 60896, Loss: 0.8087108135223389\n",
      "Epoch: 0, Step: 60928, Loss: 0.9436953663825989\n",
      "Epoch: 0, Step: 60960, Loss: 0.867642879486084\n",
      "Epoch: 0, Step: 60992, Loss: 0.810195803642273\n",
      "Epoch: 0, Step: 61024, Loss: 0.7951983213424683\n",
      "Epoch: 0, Step: 61056, Loss: 0.8213208317756653\n",
      "Epoch: 0, Step: 61088, Loss: 0.7749361395835876\n",
      "Epoch: 0, Step: 61120, Loss: 0.8942146301269531\n",
      "Epoch: 0, Step: 61152, Loss: 0.8386030793190002\n",
      "Epoch: 0, Step: 61184, Loss: 0.7578437924385071\n",
      "Epoch: 0, Step: 61216, Loss: 0.8252147436141968\n",
      "Epoch: 0, Step: 61248, Loss: 0.7672656178474426\n",
      "Epoch: 0, Step: 61280, Loss: 0.7560079097747803\n",
      "Epoch: 0, Step: 61312, Loss: 0.8160776495933533\n",
      "Epoch: 0, Step: 61344, Loss: 0.7284300327301025\n",
      "Epoch: 0, Step: 61376, Loss: 0.8574076294898987\n",
      "Epoch: 0, Step: 61408, Loss: 0.8692469596862793\n",
      "Epoch: 0, Step: 61440, Loss: 0.851269543170929\n",
      "Epoch: 0, Step: 61472, Loss: 0.8249600529670715\n",
      "Epoch: 0, Step: 61504, Loss: 0.9822359681129456\n",
      "Epoch: 0, Step: 61536, Loss: 0.7668703198432922\n",
      "Epoch: 0, Step: 61568, Loss: 0.799960196018219\n",
      "Epoch: 0, Step: 61600, Loss: 0.8470059633255005\n",
      "Epoch: 0, Step: 61632, Loss: 0.9202401041984558\n",
      "Epoch: 0, Step: 61664, Loss: 0.8901663422584534\n",
      "Epoch: 0, Step: 61696, Loss: 0.7663418054580688\n",
      "Epoch: 0, Step: 61728, Loss: 0.8367229700088501\n",
      "Epoch: 0, Step: 61760, Loss: 0.9845730066299438\n",
      "Epoch: 0, Step: 61792, Loss: 0.803837239742279\n",
      "Epoch: 0, Step: 61824, Loss: 0.7687026858329773\n",
      "Epoch: 0, Step: 61856, Loss: 0.9067380428314209\n",
      "Epoch: 0, Step: 61888, Loss: 0.7420600056648254\n",
      "Epoch: 0, Step: 61920, Loss: 0.7746309638023376\n",
      "Epoch: 0, Step: 61952, Loss: 0.8091879487037659\n",
      "Epoch: 0, Step: 61984, Loss: 0.8253673315048218\n",
      "Epoch: 0, Step: 62016, Loss: 0.8944599032402039\n",
      "Epoch: 0, Step: 62048, Loss: 0.8489038348197937\n",
      "Epoch: 0, Step: 62080, Loss: 0.7993217706680298\n",
      "Epoch: 0, Step: 62112, Loss: 0.8395535945892334\n",
      "Epoch: 0, Step: 62144, Loss: 0.8435056209564209\n",
      "Epoch: 0, Step: 62176, Loss: 0.8673372864723206\n",
      "Epoch: 0, Step: 62208, Loss: 0.7601110339164734\n",
      "Epoch: 0, Step: 62240, Loss: 0.8824015855789185\n",
      "Epoch: 0, Step: 62272, Loss: 0.7910802364349365\n",
      "Epoch: 0, Step: 62304, Loss: 0.8862789869308472\n",
      "Epoch: 0, Step: 62336, Loss: 0.8394781351089478\n",
      "Epoch: 0, Step: 62368, Loss: 0.7984039187431335\n",
      "Epoch: 0, Step: 62400, Loss: 0.7573288679122925\n",
      "Epoch: 0, Step: 62432, Loss: 0.7961873412132263\n",
      "Epoch: 0, Step: 62464, Loss: 0.8310955762863159\n",
      "Epoch: 0, Step: 62496, Loss: 0.7748064398765564\n",
      "Epoch: 0, Step: 62528, Loss: 0.8372558951377869\n",
      "Epoch: 0, Step: 62560, Loss: 0.7774451375007629\n",
      "Epoch: 0, Step: 62592, Loss: 0.7679380178451538\n",
      "Epoch: 0, Step: 62624, Loss: 0.8281285762786865\n",
      "Epoch: 0, Step: 62656, Loss: 0.9569612145423889\n",
      "Epoch: 0, Step: 62688, Loss: 0.8650162816047668\n",
      "Epoch: 0, Step: 62720, Loss: 0.9057263135910034\n",
      "Epoch: 0, Step: 62752, Loss: 0.904201865196228\n",
      "Epoch: 0, Step: 62784, Loss: 0.796716570854187\n",
      "Epoch: 0, Step: 62816, Loss: 0.8210572600364685\n",
      "Epoch: 0, Step: 62848, Loss: 0.8397128582000732\n",
      "Epoch: 0, Step: 62880, Loss: 0.8329394459724426\n",
      "Epoch: 0, Step: 62912, Loss: 0.7896098494529724\n",
      "Epoch: 0, Step: 62944, Loss: 0.804378092288971\n",
      "Epoch: 0, Step: 62976, Loss: 0.7960402965545654\n",
      "Epoch: 0, Step: 63008, Loss: 0.7791454195976257\n",
      "Epoch: 0, Step: 63040, Loss: 0.8465471863746643\n",
      "Epoch: 0, Step: 63072, Loss: 0.8971739411354065\n",
      "Epoch: 0, Step: 63104, Loss: 0.7807764410972595\n",
      "Epoch: 0, Step: 63136, Loss: 0.8044894933700562\n",
      "Epoch: 0, Step: 63168, Loss: 0.8455199003219604\n",
      "Epoch: 0, Step: 63200, Loss: 0.8831716775894165\n",
      "Epoch: 0, Step: 63232, Loss: 0.8688260912895203\n",
      "Epoch: 0, Step: 63264, Loss: 0.7044652104377747\n",
      "Epoch: 0, Step: 63296, Loss: 0.8076165318489075\n",
      "Epoch: 0, Step: 63328, Loss: 0.7281449437141418\n",
      "Epoch: 0, Step: 63360, Loss: 0.742093026638031\n",
      "Epoch: 0, Step: 63392, Loss: 0.8008725643157959\n",
      "Epoch: 0, Step: 63424, Loss: 0.8023221492767334\n",
      "Epoch: 0, Step: 63456, Loss: 0.8731111288070679\n",
      "Epoch: 0, Step: 63488, Loss: 0.8585100173950195\n",
      "Epoch: 0, Step: 63520, Loss: 0.8089748024940491\n",
      "Epoch: 0, Step: 63552, Loss: 0.8860006928443909\n",
      "Epoch: 0, Step: 63584, Loss: 0.761986494064331\n",
      "Epoch: 0, Step: 63616, Loss: 0.9399745464324951\n",
      "Epoch: 0, Step: 63648, Loss: 0.8205054402351379\n",
      "Epoch: 0, Step: 63680, Loss: 0.8338689208030701\n",
      "Epoch: 0, Step: 63712, Loss: 0.9004033207893372\n",
      "Epoch: 0, Step: 63744, Loss: 0.8577671051025391\n",
      "Epoch: 0, Step: 63776, Loss: 0.8638785481452942\n",
      "Epoch: 0, Step: 63808, Loss: 0.8309033513069153\n",
      "Epoch: 0, Step: 63840, Loss: 0.7443810105323792\n",
      "Epoch: 0, Step: 63872, Loss: 0.7146677374839783\n",
      "Epoch: 0, Step: 63904, Loss: 0.781213641166687\n",
      "Epoch: 0, Step: 63936, Loss: 0.85443514585495\n",
      "Epoch: 0, Step: 63968, Loss: 0.9096959233283997\n",
      "Epoch: 0, Step: 64000, Loss: 0.8238881826400757\n",
      "Epoch: 0, Step: 64032, Loss: 0.7987921237945557\n",
      "Epoch: 0, Step: 64064, Loss: 0.7678095102310181\n",
      "Epoch: 0, Step: 64096, Loss: 0.8399513959884644\n",
      "Epoch: 0, Step: 64128, Loss: 0.8107920289039612\n",
      "Epoch: 0, Step: 64160, Loss: 0.7915083765983582\n",
      "Epoch: 0, Step: 64192, Loss: 0.7835413217544556\n",
      "Epoch: 0, Step: 64224, Loss: 0.9064849019050598\n",
      "Epoch: 0, Step: 64256, Loss: 0.8769369125366211\n",
      "Epoch: 0, Step: 64288, Loss: 0.779677152633667\n",
      "Epoch: 0, Step: 64320, Loss: 0.8180972933769226\n",
      "Epoch: 0, Step: 64352, Loss: 0.8944151997566223\n",
      "Epoch: 0, Step: 64384, Loss: 0.7596868276596069\n",
      "Epoch: 0, Step: 64416, Loss: 0.8984627723693848\n",
      "Epoch: 0, Step: 64448, Loss: 0.9029436707496643\n",
      "Epoch: 0, Step: 64480, Loss: 0.91045081615448\n",
      "Epoch: 0, Step: 64512, Loss: 0.7634726166725159\n",
      "Epoch: 0, Step: 64544, Loss: 0.8340394496917725\n",
      "Epoch: 0, Step: 64576, Loss: 0.7953503727912903\n",
      "Epoch: 0, Step: 64608, Loss: 0.8473305106163025\n",
      "Epoch: 0, Step: 64640, Loss: 0.8700613975524902\n",
      "Epoch: 0, Step: 64672, Loss: 0.8574658036231995\n",
      "Epoch: 0, Step: 64704, Loss: 0.9253710508346558\n",
      "Epoch: 0, Step: 64736, Loss: 0.7891640067100525\n",
      "Epoch: 0, Step: 64768, Loss: 0.7808120846748352\n",
      "Epoch: 0, Step: 64800, Loss: 0.902644693851471\n",
      "Epoch: 0, Step: 64832, Loss: 0.8397875428199768\n",
      "Epoch: 0, Step: 64864, Loss: 0.8220387101173401\n",
      "Epoch: 0, Step: 64896, Loss: 0.8368910551071167\n",
      "Epoch: 0, Step: 64928, Loss: 0.8236989378929138\n",
      "Epoch: 0, Step: 64960, Loss: 0.7589710354804993\n",
      "Epoch: 0, Step: 64992, Loss: 0.8031555414199829\n",
      "Epoch: 0, Step: 65024, Loss: 0.8180959224700928\n",
      "Epoch: 0, Step: 65056, Loss: 0.8897179365158081\n",
      "Epoch: 0, Step: 65088, Loss: 0.8974068760871887\n",
      "Epoch: 0, Step: 65120, Loss: 0.8171493411064148\n",
      "Epoch: 0, Step: 65152, Loss: 0.7695175409317017\n",
      "Epoch: 0, Step: 65184, Loss: 0.7984848618507385\n",
      "Epoch: 0, Step: 65216, Loss: 0.8446673154830933\n",
      "Epoch: 0, Step: 65248, Loss: 0.7924835085868835\n",
      "Epoch: 0, Step: 65280, Loss: 0.8120972514152527\n",
      "Epoch: 0, Step: 65312, Loss: 0.8831754326820374\n",
      "Epoch: 0, Step: 65344, Loss: 0.9100503921508789\n",
      "Epoch: 0, Step: 65376, Loss: 0.8448200821876526\n",
      "Epoch: 0, Step: 65408, Loss: 0.8903006315231323\n",
      "Epoch: 0, Step: 65440, Loss: 0.8591907024383545\n",
      "Epoch: 0, Step: 65472, Loss: 0.7890611290931702\n",
      "Epoch: 0, Step: 65504, Loss: 0.8482415676116943\n",
      "Epoch: 0, Step: 65536, Loss: 0.8121082782745361\n",
      "Epoch: 0, Step: 65568, Loss: 0.8076419234275818\n",
      "Epoch: 0, Step: 65600, Loss: 0.8317081928253174\n",
      "Epoch: 0, Step: 65632, Loss: 0.9017269611358643\n",
      "Epoch: 0, Step: 65664, Loss: 0.8586443662643433\n",
      "Epoch: 0, Step: 65696, Loss: 0.8530271053314209\n",
      "Epoch: 0, Step: 65728, Loss: 0.6755825877189636\n",
      "Epoch: 0, Step: 65760, Loss: 0.820533037185669\n",
      "Epoch: 0, Step: 65792, Loss: 0.8713123798370361\n",
      "Epoch: 0, Step: 65824, Loss: 0.8762619495391846\n",
      "Epoch: 0, Step: 65856, Loss: 0.8794527649879456\n",
      "Epoch: 0, Step: 65888, Loss: 0.8266100287437439\n",
      "Epoch: 0, Step: 65920, Loss: 0.8106839656829834\n",
      "Epoch: 0, Step: 65952, Loss: 0.8056707978248596\n",
      "Epoch: 0, Step: 65984, Loss: 0.8137527704238892\n",
      "Epoch: 0, Step: 66016, Loss: 0.8711301684379578\n",
      "Epoch: 0, Step: 66048, Loss: 0.8849847316741943\n",
      "Epoch: 0, Step: 66080, Loss: 0.9174078702926636\n",
      "Epoch: 0, Step: 66112, Loss: 0.8556210994720459\n",
      "Epoch: 0, Step: 66144, Loss: 0.7814882397651672\n",
      "Epoch: 0, Step: 66176, Loss: 0.7951629757881165\n",
      "Epoch: 0, Step: 66208, Loss: 0.8884238004684448\n",
      "Epoch: 0, Step: 66240, Loss: 0.8254678249359131\n",
      "Epoch: 0, Step: 66272, Loss: 0.8346059322357178\n",
      "Epoch: 0, Step: 66304, Loss: 0.8977611064910889\n",
      "Epoch: 0, Step: 66336, Loss: 0.8184829354286194\n",
      "Epoch: 0, Step: 66368, Loss: 0.7489579916000366\n",
      "Epoch: 0, Step: 66400, Loss: 0.8864395022392273\n",
      "Epoch: 0, Step: 66432, Loss: 0.8459088802337646\n",
      "Epoch: 0, Step: 66464, Loss: 0.8857627511024475\n",
      "Epoch: 0, Step: 66496, Loss: 0.740012526512146\n",
      "Epoch: 0, Step: 66528, Loss: 0.8611832857131958\n",
      "Epoch: 0, Step: 66560, Loss: 0.8395321369171143\n",
      "Epoch: 0, Step: 66592, Loss: 0.7793747186660767\n",
      "Epoch: 0, Step: 66624, Loss: 0.7450534105300903\n",
      "Epoch: 0, Step: 66656, Loss: 0.8348780870437622\n",
      "Epoch: 0, Step: 66688, Loss: 0.8519059419631958\n",
      "Epoch: 0, Step: 66720, Loss: 0.8466140031814575\n",
      "Epoch: 0, Step: 66752, Loss: 0.7821601629257202\n",
      "Epoch: 0, Step: 66784, Loss: 0.8305122256278992\n",
      "Epoch: 0, Step: 66816, Loss: 0.8346937894821167\n",
      "Epoch: 0, Step: 66848, Loss: 0.7955979108810425\n",
      "Epoch: 0, Step: 66880, Loss: 0.8790346384048462\n",
      "Epoch: 0, Step: 66912, Loss: 0.9174487590789795\n",
      "Epoch: 0, Step: 66944, Loss: 0.8293111324310303\n",
      "Epoch: 0, Step: 66976, Loss: 0.8811794519424438\n",
      "Epoch: 0, Step: 67008, Loss: 0.7412105798721313\n",
      "Epoch: 0, Step: 67040, Loss: 0.6896408200263977\n",
      "Epoch: 0, Step: 67072, Loss: 0.8342110514640808\n",
      "Epoch: 0, Step: 67104, Loss: 0.844702959060669\n",
      "Epoch: 0, Step: 67136, Loss: 0.8462714552879333\n",
      "Epoch: 0, Step: 67168, Loss: 0.7589404582977295\n",
      "Epoch: 0, Step: 67200, Loss: 0.8813366889953613\n",
      "Epoch: 0, Step: 67232, Loss: 0.7640275359153748\n",
      "Epoch: 0, Step: 67264, Loss: 0.8292657732963562\n",
      "Epoch: 0, Step: 67296, Loss: 0.8821767568588257\n",
      "Epoch: 0, Step: 67328, Loss: 0.783830463886261\n",
      "Epoch: 0, Step: 67360, Loss: 0.7520721554756165\n",
      "Epoch: 0, Step: 67392, Loss: 0.8516454100608826\n",
      "Epoch: 0, Step: 67424, Loss: 0.818349301815033\n",
      "Epoch: 0, Step: 67456, Loss: 0.8747883439064026\n",
      "Epoch: 0, Step: 67488, Loss: 0.8912503123283386\n",
      "Epoch: 0, Step: 67520, Loss: 0.8063894510269165\n",
      "Epoch: 0, Step: 67552, Loss: 0.7359887957572937\n",
      "Epoch: 0, Step: 67584, Loss: 0.9232059717178345\n",
      "Epoch: 0, Step: 67616, Loss: 0.8259343504905701\n",
      "Epoch: 0, Step: 67648, Loss: 0.8192998766899109\n",
      "Epoch: 0, Step: 67680, Loss: 0.7341663241386414\n",
      "Epoch: 0, Step: 67712, Loss: 0.867053747177124\n",
      "Epoch: 0, Step: 67744, Loss: 0.8783155083656311\n",
      "Epoch: 0, Step: 67776, Loss: 0.8326861262321472\n",
      "Epoch: 0, Step: 67808, Loss: 0.8113588690757751\n",
      "Epoch: 0, Step: 67840, Loss: 0.9309712648391724\n",
      "Epoch: 0, Step: 67872, Loss: 0.7105025053024292\n",
      "Epoch: 0, Step: 67904, Loss: 0.8423547148704529\n",
      "Epoch: 0, Step: 67936, Loss: 0.8001195788383484\n",
      "Epoch: 0, Step: 67968, Loss: 0.7263230681419373\n",
      "Epoch: 0, Step: 68000, Loss: 0.6609787344932556\n",
      "Epoch: 0, Step: 68032, Loss: 0.984724760055542\n",
      "Epoch: 0, Step: 68064, Loss: 0.9203087091445923\n",
      "Epoch: 0, Step: 68096, Loss: 0.7946875095367432\n",
      "Epoch: 0, Step: 68128, Loss: 0.7302799820899963\n",
      "Epoch: 0, Step: 68160, Loss: 0.7718867063522339\n",
      "Epoch: 0, Step: 68192, Loss: 0.8390394449234009\n",
      "Epoch: 0, Step: 68224, Loss: 0.8855621218681335\n",
      "Epoch: 0, Step: 68256, Loss: 0.8782674670219421\n",
      "Epoch: 0, Step: 68288, Loss: 0.8783161044120789\n",
      "Epoch: 0, Step: 68320, Loss: 0.9051941633224487\n",
      "Epoch: 0, Step: 68352, Loss: 0.8340398669242859\n",
      "Epoch: 0, Step: 68384, Loss: 0.8943654298782349\n",
      "Epoch: 0, Step: 68416, Loss: 0.7897621989250183\n",
      "Epoch: 0, Step: 68448, Loss: 0.7942884564399719\n",
      "Epoch: 0, Step: 68480, Loss: 0.8263779878616333\n",
      "Epoch: 0, Step: 68512, Loss: 0.774718701839447\n",
      "Epoch: 0, Step: 68544, Loss: 0.8828266859054565\n",
      "Epoch: 0, Step: 68576, Loss: 0.9388198852539062\n",
      "Epoch: 0, Step: 68608, Loss: 0.8438540101051331\n",
      "Epoch: 0, Step: 68640, Loss: 0.9136694669723511\n",
      "Epoch: 0, Step: 68672, Loss: 0.7538677453994751\n",
      "Epoch: 0, Step: 68704, Loss: 0.863239049911499\n",
      "Epoch: 0, Step: 68736, Loss: 0.8455970287322998\n",
      "Epoch: 0, Step: 68768, Loss: 0.7694123387336731\n",
      "Epoch: 0, Step: 68800, Loss: 0.6895777583122253\n",
      "Epoch: 0, Step: 68832, Loss: 0.7568395137786865\n",
      "Epoch: 0, Step: 68864, Loss: 0.7782703638076782\n",
      "Epoch: 0, Step: 68896, Loss: 0.7809856534004211\n",
      "Epoch: 0, Step: 68928, Loss: 0.7645013332366943\n",
      "Epoch: 0, Step: 68960, Loss: 0.7942206859588623\n",
      "Epoch: 0, Step: 68992, Loss: 0.8934299349784851\n",
      "Epoch: 0, Step: 69024, Loss: 0.9371922612190247\n",
      "Epoch: 0, Step: 69056, Loss: 0.8447917103767395\n",
      "Epoch: 0, Step: 69088, Loss: 0.7914865016937256\n",
      "Epoch: 0, Step: 69120, Loss: 0.7583600878715515\n",
      "Epoch: 0, Step: 69152, Loss: 0.8978225588798523\n",
      "Epoch: 0, Step: 69184, Loss: 0.8837369084358215\n",
      "Epoch: 0, Step: 69216, Loss: 0.7548179030418396\n",
      "Epoch: 0, Step: 69248, Loss: 0.7898048758506775\n",
      "Epoch: 0, Step: 69280, Loss: 0.8113291263580322\n",
      "Epoch: 0, Step: 69312, Loss: 0.8433214426040649\n",
      "Epoch: 0, Step: 69344, Loss: 0.797094464302063\n",
      "Epoch: 0, Step: 69376, Loss: 0.7900182604789734\n",
      "Epoch: 0, Step: 69408, Loss: 0.8163427710533142\n",
      "Epoch: 0, Step: 69440, Loss: 0.9044086933135986\n",
      "Epoch: 0, Step: 69472, Loss: 0.8076977133750916\n",
      "Epoch: 0, Step: 69504, Loss: 0.7919065356254578\n",
      "Epoch: 0, Step: 69536, Loss: 0.8922781944274902\n",
      "Epoch: 0, Step: 69568, Loss: 0.7233558893203735\n",
      "Epoch: 0, Step: 69600, Loss: 0.7705038785934448\n",
      "Epoch: 0, Step: 69632, Loss: 0.7665689587593079\n",
      "Epoch: 0, Step: 69664, Loss: 0.7960716485977173\n",
      "Epoch: 0, Step: 69696, Loss: 0.8600881695747375\n",
      "Epoch: 0, Step: 69728, Loss: 0.9020822644233704\n",
      "Epoch: 0, Step: 69760, Loss: 0.8825767040252686\n",
      "Epoch: 0, Step: 69792, Loss: 0.8252357840538025\n",
      "Epoch: 0, Step: 69824, Loss: 0.8754671812057495\n",
      "Epoch: 0, Step: 69856, Loss: 0.7551141977310181\n",
      "Epoch: 0, Step: 69888, Loss: 0.8399902582168579\n",
      "Epoch: 0, Step: 69920, Loss: 0.9403127431869507\n",
      "Epoch: 0, Step: 69952, Loss: 0.8459027409553528\n",
      "Epoch: 0, Step: 69984, Loss: 0.7948154807090759\n",
      "Epoch: 0, Step: 70016, Loss: 0.7849199175834656\n",
      "Epoch: 0, Step: 70048, Loss: 0.7537692785263062\n",
      "Epoch: 0, Step: 70080, Loss: 0.8470306992530823\n",
      "Epoch: 0, Step: 70112, Loss: 0.8504307866096497\n",
      "Epoch: 0, Step: 70144, Loss: 0.8094652891159058\n",
      "Epoch: 0, Step: 70176, Loss: 0.7896802425384521\n",
      "Epoch: 0, Step: 70208, Loss: 0.8338064551353455\n",
      "Epoch: 0, Step: 70240, Loss: 0.8769665360450745\n",
      "Epoch: 0, Step: 70272, Loss: 0.9089498519897461\n",
      "Epoch: 0, Step: 70304, Loss: 0.9041399955749512\n",
      "Epoch: 0, Step: 70336, Loss: 0.9888139367103577\n",
      "Epoch: 0, Step: 70368, Loss: 0.8170340061187744\n",
      "Epoch: 0, Step: 70400, Loss: 0.8581093549728394\n",
      "Epoch: 0, Step: 70432, Loss: 0.7437763810157776\n",
      "Epoch: 0, Step: 70464, Loss: 0.755683958530426\n",
      "Epoch: 0, Step: 70496, Loss: 0.8126890063285828\n",
      "Epoch: 0, Step: 70528, Loss: 0.8333407044410706\n",
      "Epoch: 0, Step: 70560, Loss: 0.8803431987762451\n",
      "Epoch: 0, Step: 70592, Loss: 0.6538525819778442\n",
      "Epoch: 0, Step: 70624, Loss: 0.8120827078819275\n",
      "Epoch: 0, Step: 70656, Loss: 0.7597487568855286\n",
      "Epoch: 0, Step: 70688, Loss: 0.7699660062789917\n",
      "Epoch: 0, Step: 70720, Loss: 0.844623863697052\n",
      "Epoch: 0, Step: 70752, Loss: 0.7842938899993896\n",
      "Epoch: 0, Step: 70784, Loss: 0.7463721036911011\n",
      "Epoch: 0, Step: 70816, Loss: 0.8123400807380676\n",
      "Epoch: 0, Step: 70848, Loss: 0.8632071018218994\n",
      "Epoch: 0, Step: 70880, Loss: 0.8348260521888733\n",
      "Epoch: 0, Step: 70912, Loss: 0.7429171204566956\n",
      "Epoch: 0, Step: 70944, Loss: 0.6973997950553894\n",
      "Epoch: 0, Step: 70976, Loss: 0.8321835398674011\n",
      "Epoch: 0, Step: 71008, Loss: 0.984480082988739\n",
      "Epoch: 0, Step: 71040, Loss: 0.7996227145195007\n",
      "Epoch: 0, Step: 71072, Loss: 0.7889557480812073\n",
      "Epoch: 0, Step: 71104, Loss: 0.8531949520111084\n",
      "Epoch: 0, Step: 71136, Loss: 0.8256374597549438\n",
      "Epoch: 0, Step: 71168, Loss: 0.7410227060317993\n",
      "Epoch: 0, Step: 71200, Loss: 0.860761284828186\n",
      "Epoch: 0, Step: 71232, Loss: 0.8118928074836731\n",
      "Epoch: 0, Step: 71264, Loss: 0.8388214707374573\n",
      "Epoch: 0, Step: 71296, Loss: 0.8838320970535278\n",
      "Epoch: 0, Step: 71328, Loss: 0.8759255409240723\n",
      "Epoch: 0, Step: 71360, Loss: 0.6714244484901428\n",
      "Epoch: 0, Step: 71392, Loss: 0.8855068683624268\n",
      "Epoch: 0, Step: 71424, Loss: 0.8447089791297913\n",
      "Epoch: 0, Step: 71456, Loss: 0.7719140648841858\n",
      "Epoch: 0, Step: 71488, Loss: 0.8486329913139343\n",
      "Epoch: 0, Step: 71520, Loss: 0.8594032526016235\n",
      "Epoch: 0, Step: 71552, Loss: 0.8930134773254395\n",
      "Epoch: 0, Step: 71584, Loss: 0.8091313242912292\n",
      "Epoch: 0, Step: 71616, Loss: 0.8150075078010559\n",
      "Epoch: 0, Step: 71648, Loss: 0.7225426435470581\n",
      "Epoch: 0, Step: 71680, Loss: 0.8621624708175659\n",
      "Epoch: 0, Step: 71712, Loss: 0.8626709580421448\n",
      "Epoch: 0, Step: 71744, Loss: 0.8529953360557556\n",
      "Epoch: 0, Step: 71776, Loss: 0.8684374094009399\n",
      "Epoch: 0, Step: 71808, Loss: 0.8283918499946594\n",
      "Epoch: 0, Step: 71840, Loss: 0.8186208605766296\n",
      "Epoch: 0, Step: 71872, Loss: 0.893487274646759\n",
      "Epoch: 0, Step: 71904, Loss: 0.9301278591156006\n",
      "Epoch: 0, Step: 71936, Loss: 0.892950177192688\n",
      "Epoch: 0, Step: 71968, Loss: 0.7864736318588257\n",
      "Epoch: 0, Step: 72000, Loss: 0.7982064485549927\n",
      "Epoch: 0, Step: 72032, Loss: 0.8907984495162964\n",
      "Epoch: 0, Step: 72064, Loss: 0.8749977350234985\n",
      "Epoch: 0, Step: 72096, Loss: 0.7985341548919678\n",
      "Epoch: 0, Step: 72128, Loss: 0.785312294960022\n",
      "Epoch: 0, Step: 72160, Loss: 0.7989300489425659\n",
      "Epoch: 0, Step: 72192, Loss: 0.8271953463554382\n",
      "Epoch: 0, Step: 72224, Loss: 0.7766807675361633\n",
      "Epoch: 0, Step: 72256, Loss: 0.892257034778595\n",
      "Epoch: 0, Step: 72288, Loss: 0.8415212035179138\n",
      "Epoch: 0, Step: 72320, Loss: 0.8315943479537964\n",
      "Epoch: 0, Step: 72352, Loss: 0.8705576658248901\n",
      "Epoch: 0, Step: 72384, Loss: 0.849005937576294\n",
      "Epoch: 0, Step: 72416, Loss: 0.7319561243057251\n",
      "Epoch: 0, Step: 72448, Loss: 0.9037665128707886\n",
      "Epoch: 0, Step: 72480, Loss: 0.807228684425354\n",
      "Epoch: 0, Step: 72512, Loss: 0.8330550789833069\n",
      "Epoch: 0, Step: 72544, Loss: 0.7956947088241577\n",
      "Epoch: 0, Step: 72576, Loss: 0.8762295842170715\n",
      "Epoch: 0, Step: 72608, Loss: 0.8082688450813293\n",
      "Epoch: 0, Step: 72640, Loss: 0.8386679887771606\n",
      "Epoch: 0, Step: 72672, Loss: 0.7925865054130554\n",
      "Epoch: 0, Step: 72704, Loss: 0.7771617770195007\n",
      "Epoch: 0, Step: 72736, Loss: 0.8452222347259521\n",
      "Epoch: 0, Step: 72768, Loss: 0.8133878111839294\n",
      "Epoch: 0, Step: 72800, Loss: 0.7328421473503113\n",
      "Epoch: 0, Step: 72832, Loss: 0.8325278759002686\n",
      "Epoch: 0, Step: 72864, Loss: 0.8079043030738831\n",
      "Epoch: 0, Step: 72896, Loss: 0.7170847058296204\n",
      "Epoch: 0, Step: 72928, Loss: 0.7978894710540771\n",
      "Epoch: 0, Step: 72960, Loss: 0.8168005347251892\n",
      "Epoch: 0, Step: 72992, Loss: 0.8224878907203674\n",
      "Epoch: 0, Step: 73024, Loss: 0.7427366971969604\n",
      "Epoch: 0, Step: 73056, Loss: 0.8175815939903259\n",
      "Epoch: 0, Step: 73088, Loss: 0.8931466937065125\n",
      "Epoch: 0, Step: 73120, Loss: 0.7887348532676697\n",
      "Epoch: 0, Step: 73152, Loss: 0.9037424325942993\n",
      "Epoch: 0, Step: 73184, Loss: 0.7598721981048584\n",
      "Epoch: 0, Step: 73216, Loss: 0.8335603475570679\n",
      "Epoch: 0, Step: 73248, Loss: 0.8645541667938232\n",
      "Epoch: 0, Step: 73280, Loss: 0.7841781973838806\n",
      "Epoch: 0, Step: 73312, Loss: 0.9837603569030762\n",
      "Epoch: 0, Step: 73344, Loss: 0.8783888816833496\n",
      "Epoch: 0, Step: 73376, Loss: 0.8310317397117615\n",
      "Epoch: 0, Step: 73408, Loss: 0.9010590314865112\n",
      "Epoch: 0, Step: 73440, Loss: 0.9011077880859375\n",
      "Epoch: 0, Step: 73472, Loss: 0.7839897871017456\n",
      "Epoch: 0, Step: 73504, Loss: 0.8761988878250122\n",
      "Epoch: 0, Step: 73536, Loss: 0.7574942111968994\n",
      "Epoch: 0, Step: 73568, Loss: 0.8384255170822144\n",
      "Epoch: 0, Step: 73600, Loss: 0.8764806985855103\n",
      "Epoch: 0, Step: 73632, Loss: 0.8240877389907837\n",
      "Epoch: 0, Step: 73664, Loss: 0.8208531141281128\n",
      "Epoch: 0, Step: 73696, Loss: 0.8728739619255066\n",
      "Epoch: 0, Step: 73728, Loss: 0.8442114591598511\n",
      "Epoch: 0, Step: 73760, Loss: 0.9062373638153076\n",
      "Epoch: 0, Step: 73792, Loss: 0.8577749133110046\n",
      "Epoch: 0, Step: 73824, Loss: 0.9013030529022217\n",
      "Epoch: 0, Step: 73856, Loss: 0.8917855024337769\n",
      "Epoch: 0, Step: 73888, Loss: 0.814556360244751\n",
      "Epoch: 0, Step: 73920, Loss: 0.798928439617157\n",
      "Epoch: 0, Step: 73952, Loss: 0.8758118152618408\n",
      "Epoch: 0, Step: 73984, Loss: 0.7890016436576843\n",
      "Epoch: 0, Step: 74016, Loss: 0.9200731515884399\n",
      "Epoch: 0, Step: 74048, Loss: 0.8019322156906128\n",
      "Epoch: 0, Step: 74080, Loss: 0.8253777027130127\n",
      "Epoch: 0, Step: 74112, Loss: 0.8513561487197876\n",
      "Epoch: 0, Step: 74144, Loss: 0.8725912570953369\n",
      "Epoch: 0, Step: 74176, Loss: 0.7869855761528015\n",
      "Epoch: 0, Step: 74208, Loss: 0.7131844162940979\n",
      "Epoch: 0, Step: 74240, Loss: 0.8534748554229736\n",
      "Epoch: 0, Step: 74272, Loss: 0.9361004829406738\n",
      "Epoch: 0, Step: 74304, Loss: 0.8679227828979492\n",
      "Epoch: 0, Step: 74336, Loss: 0.8490334749221802\n",
      "Epoch: 0, Step: 74368, Loss: 0.857613205909729\n",
      "Epoch: 0, Step: 74400, Loss: 0.8117983341217041\n",
      "Epoch: 0, Step: 74432, Loss: 0.9769974946975708\n",
      "Epoch: 0, Step: 74464, Loss: 0.8553361296653748\n",
      "Epoch: 0, Step: 74496, Loss: 0.794574499130249\n",
      "Epoch: 0, Step: 74528, Loss: 0.926348090171814\n",
      "Epoch: 0, Step: 74560, Loss: 0.7853413820266724\n",
      "Epoch: 0, Step: 74592, Loss: 0.7422669529914856\n",
      "Epoch: 0, Step: 74624, Loss: 0.8711568713188171\n",
      "Epoch: 0, Step: 74656, Loss: 0.8158496022224426\n",
      "Epoch: 0, Step: 74688, Loss: 0.8410384058952332\n",
      "Epoch: 0, Step: 74720, Loss: 0.8286025524139404\n",
      "Epoch: 0, Step: 74752, Loss: 0.8093202114105225\n",
      "Epoch: 0, Step: 74784, Loss: 0.8441192507743835\n",
      "Epoch: 0, Step: 74816, Loss: 0.7810479998588562\n",
      "Epoch: 0, Step: 74848, Loss: 0.791103720664978\n",
      "Epoch: 0, Step: 74880, Loss: 0.7518550753593445\n",
      "Epoch: 0, Step: 74912, Loss: 0.8395577073097229\n",
      "Epoch: 0, Step: 74944, Loss: 0.8339965343475342\n",
      "Epoch: 0, Step: 74976, Loss: 0.8124497532844543\n",
      "Epoch: 0, Step: 75008, Loss: 0.7491937279701233\n",
      "Epoch: 0, Step: 75040, Loss: 0.8499066829681396\n",
      "Epoch: 0, Step: 75072, Loss: 0.8430362939834595\n",
      "Epoch: 0, Step: 75104, Loss: 0.8066383600234985\n",
      "Epoch: 0, Step: 75136, Loss: 0.8557075262069702\n",
      "Epoch: 0, Step: 75168, Loss: 0.9494194984436035\n",
      "Epoch: 0, Step: 75200, Loss: 0.826461911201477\n",
      "Epoch: 0, Step: 75232, Loss: 0.771803617477417\n",
      "Epoch: 0, Step: 75264, Loss: 0.9135385155677795\n",
      "Epoch: 0, Step: 75296, Loss: 0.9000754356384277\n",
      "Epoch: 0, Step: 75328, Loss: 0.8362880945205688\n",
      "Epoch: 0, Step: 75360, Loss: 0.7697770595550537\n",
      "Epoch: 0, Step: 75392, Loss: 0.9925910234451294\n",
      "Epoch: 0, Step: 75424, Loss: 0.7317026853561401\n",
      "Epoch: 0, Step: 75456, Loss: 0.7904775142669678\n",
      "Epoch: 0, Step: 75488, Loss: 0.8987175822257996\n",
      "Epoch: 0, Step: 75520, Loss: 0.7570917010307312\n",
      "Epoch: 0, Step: 75552, Loss: 0.8395809531211853\n",
      "Epoch: 0, Step: 75584, Loss: 0.7660090923309326\n",
      "Epoch: 0, Step: 75616, Loss: 0.7829076647758484\n",
      "Epoch: 0, Step: 75648, Loss: 0.8583632111549377\n",
      "Epoch: 0, Step: 75680, Loss: 0.8042019605636597\n",
      "Epoch: 0, Step: 75712, Loss: 0.8277251720428467\n",
      "Epoch: 0, Step: 75744, Loss: 0.7373651266098022\n",
      "Epoch: 0, Step: 75776, Loss: 0.8800700306892395\n",
      "Epoch: 0, Step: 75808, Loss: 0.8705840110778809\n",
      "Epoch: 0, Step: 75840, Loss: 0.8178673982620239\n",
      "Epoch: 0, Step: 75872, Loss: 0.9235050678253174\n",
      "Epoch: 0, Step: 75904, Loss: 0.8465301990509033\n",
      "Epoch: 0, Step: 75936, Loss: 0.7416139841079712\n",
      "Epoch: 0, Step: 75968, Loss: 0.7704868912696838\n",
      "Epoch: 0, Step: 76000, Loss: 0.728422999382019\n",
      "Epoch: 0, Step: 76032, Loss: 0.8749743103981018\n",
      "Epoch: 0, Step: 76064, Loss: 0.7958911061286926\n",
      "Epoch: 0, Step: 76096, Loss: 0.8731508255004883\n",
      "Epoch: 0, Step: 76128, Loss: 0.7410753965377808\n",
      "Epoch: 0, Step: 76160, Loss: 0.7520771622657776\n",
      "Epoch: 0, Step: 76192, Loss: 0.8455036878585815\n",
      "Epoch: 0, Step: 76224, Loss: 0.9323692917823792\n",
      "Epoch: 0, Step: 76256, Loss: 0.869337260723114\n",
      "Epoch: 0, Step: 76288, Loss: 0.7345351576805115\n",
      "Epoch: 0, Step: 76320, Loss: 0.7690221071243286\n",
      "Epoch: 0, Step: 76352, Loss: 0.8886345624923706\n",
      "Epoch: 0, Step: 76384, Loss: 0.8211485147476196\n",
      "Epoch: 0, Step: 76416, Loss: 0.8318048715591431\n",
      "Epoch: 0, Step: 76448, Loss: 0.7686480283737183\n",
      "Epoch: 0, Step: 76480, Loss: 0.9316654801368713\n",
      "Epoch: 0, Step: 76512, Loss: 0.874912440776825\n",
      "Epoch: 0, Step: 76544, Loss: 0.7458363175392151\n",
      "Epoch: 0, Step: 76576, Loss: 0.8233920931816101\n",
      "Epoch: 0, Step: 76608, Loss: 0.8408018350601196\n",
      "Epoch: 0, Step: 76640, Loss: 0.7833226323127747\n",
      "Epoch: 0, Step: 76672, Loss: 0.7044563889503479\n",
      "Epoch: 0, Step: 76704, Loss: 0.7296152710914612\n",
      "Epoch: 0, Step: 76736, Loss: 0.8160728216171265\n",
      "Epoch: 0, Step: 76768, Loss: 0.7925224900245667\n",
      "Epoch: 0, Step: 76800, Loss: 0.8737338185310364\n",
      "Epoch: 0, Step: 76832, Loss: 0.8611133098602295\n",
      "Epoch: 0, Step: 76864, Loss: 0.8483871817588806\n",
      "Epoch: 0, Step: 76896, Loss: 0.8578721880912781\n",
      "Epoch: 0, Step: 76928, Loss: 0.8605934381484985\n",
      "Epoch: 0, Step: 76960, Loss: 0.8501128554344177\n",
      "Epoch: 0, Step: 76992, Loss: 0.81827712059021\n",
      "Epoch: 0, Step: 77024, Loss: 0.8709195256233215\n",
      "Epoch: 0, Step: 77056, Loss: 0.9242005944252014\n",
      "Epoch: 0, Step: 77088, Loss: 0.8033303022384644\n",
      "Epoch: 0, Step: 77120, Loss: 0.8766670227050781\n",
      "Epoch: 0, Step: 77152, Loss: 0.9074214100837708\n",
      "Epoch: 0, Step: 77184, Loss: 0.8274732828140259\n",
      "Epoch: 0, Step: 77216, Loss: 0.8041055798530579\n",
      "Epoch: 0, Step: 77248, Loss: 0.8046940565109253\n",
      "Epoch: 0, Step: 77280, Loss: 0.8093887567520142\n",
      "Epoch: 0, Step: 77312, Loss: 0.9075818061828613\n",
      "Epoch: 0, Step: 77344, Loss: 0.8341838717460632\n",
      "Epoch: 0, Step: 77376, Loss: 0.740256667137146\n",
      "Epoch: 0, Step: 77408, Loss: 0.8603736758232117\n",
      "Epoch: 0, Step: 77440, Loss: 0.8508269190788269\n",
      "Epoch: 0, Step: 77472, Loss: 0.8214111924171448\n",
      "Epoch: 0, Step: 77504, Loss: 0.845723032951355\n",
      "Epoch: 0, Step: 77536, Loss: 0.8867837190628052\n",
      "Epoch: 0, Step: 77568, Loss: 0.7679417729377747\n",
      "Epoch: 0, Step: 77600, Loss: 0.8134995102882385\n",
      "Epoch: 0, Step: 77632, Loss: 0.7606906294822693\n",
      "Epoch: 0, Step: 77664, Loss: 0.8056762218475342\n",
      "Epoch: 0, Step: 77696, Loss: 0.7903905510902405\n",
      "Epoch: 0, Step: 77728, Loss: 0.8045557737350464\n",
      "Epoch: 0, Step: 77760, Loss: 0.7856720685958862\n",
      "Epoch: 0, Step: 77792, Loss: 0.7647675275802612\n",
      "Epoch: 0, Step: 77824, Loss: 0.7739263772964478\n",
      "Epoch: 0, Step: 77856, Loss: 0.8484526872634888\n",
      "Epoch: 0, Step: 77888, Loss: 0.8497340083122253\n",
      "Epoch: 0, Step: 77920, Loss: 0.8036589026451111\n",
      "Epoch: 0, Step: 77952, Loss: 0.9048813581466675\n",
      "Epoch: 0, Step: 77984, Loss: 0.8118748068809509\n",
      "Epoch: 0, Step: 78016, Loss: 0.8472377061843872\n",
      "Epoch: 0, Step: 78048, Loss: 0.8480042219161987\n",
      "Epoch: 0, Step: 78080, Loss: 0.8701202273368835\n",
      "Epoch: 0, Step: 78112, Loss: 0.9251242280006409\n",
      "Epoch: 0, Step: 78144, Loss: 0.8596076965332031\n",
      "Epoch: 0, Step: 78176, Loss: 0.7441714406013489\n",
      "Epoch: 0, Step: 78208, Loss: 0.879167377948761\n",
      "Epoch: 0, Step: 78240, Loss: 0.8972244262695312\n",
      "Epoch: 0, Step: 78272, Loss: 0.7436979413032532\n",
      "Epoch: 0, Step: 78304, Loss: 0.879448413848877\n",
      "Epoch: 0, Step: 78336, Loss: 0.8159722685813904\n",
      "Epoch: 0, Step: 78368, Loss: 0.7891534566879272\n",
      "Epoch: 0, Step: 78400, Loss: 0.8246292471885681\n",
      "Epoch: 0, Step: 78432, Loss: 0.824078381061554\n",
      "Epoch: 0, Step: 78464, Loss: 0.9523705244064331\n",
      "Epoch: 0, Step: 78496, Loss: 0.8340601325035095\n",
      "Epoch: 0, Step: 78528, Loss: 0.7512876391410828\n",
      "Epoch: 0, Step: 78560, Loss: 0.8832024335861206\n",
      "Epoch: 0, Step: 78592, Loss: 0.748548150062561\n",
      "Epoch: 0, Step: 78624, Loss: 0.8259920477867126\n",
      "Epoch: 0, Step: 78656, Loss: 0.7797918915748596\n",
      "Epoch: 0, Step: 78688, Loss: 0.7842064499855042\n",
      "Epoch: 0, Step: 78720, Loss: 0.7760969996452332\n",
      "Epoch: 0, Step: 78752, Loss: 0.907489538192749\n",
      "Epoch: 0, Step: 78784, Loss: 0.7769089341163635\n",
      "Epoch: 0, Step: 78816, Loss: 0.7990991473197937\n",
      "Epoch: 0, Step: 78848, Loss: 0.8055217266082764\n",
      "Epoch: 0, Step: 78880, Loss: 0.847350001335144\n",
      "Epoch: 0, Step: 78912, Loss: 0.8054567575454712\n",
      "Epoch: 0, Step: 78944, Loss: 0.8161079287528992\n",
      "Epoch: 0, Step: 78976, Loss: 0.8028856515884399\n",
      "Epoch: 0, Step: 79008, Loss: 0.8082891702651978\n",
      "Epoch: 0, Step: 79040, Loss: 0.8532848954200745\n",
      "Epoch: 0, Step: 79072, Loss: 0.7851689457893372\n",
      "Epoch: 0, Step: 79104, Loss: 1.0111712217330933\n",
      "Epoch: 0, Step: 79136, Loss: 0.7976911067962646\n",
      "Epoch: 0, Step: 79168, Loss: 0.8018736243247986\n",
      "Epoch: 0, Step: 79200, Loss: 0.8080940842628479\n",
      "Epoch: 0, Step: 79232, Loss: 0.8020994067192078\n",
      "Epoch: 0, Step: 79264, Loss: 0.9451979398727417\n",
      "Epoch: 0, Step: 79296, Loss: 0.7178042531013489\n",
      "Epoch: 0, Step: 79328, Loss: 0.7953693270683289\n",
      "Epoch: 0, Step: 79360, Loss: 0.8740006685256958\n",
      "Epoch: 0, Step: 79392, Loss: 0.772205650806427\n",
      "Epoch: 0, Step: 79424, Loss: 0.9137258529663086\n",
      "Epoch: 0, Step: 79456, Loss: 0.8570362329483032\n",
      "Epoch: 0, Step: 79488, Loss: 0.8133732080459595\n",
      "Epoch: 0, Step: 79520, Loss: 0.873002827167511\n",
      "Epoch: 0, Step: 79552, Loss: 0.8240352272987366\n",
      "Epoch: 0, Step: 79584, Loss: 0.8064926862716675\n",
      "Epoch: 0, Step: 79616, Loss: 0.9068305492401123\n",
      "Epoch: 0, Step: 79648, Loss: 0.855140745639801\n",
      "Epoch: 0, Step: 79680, Loss: 0.8303887248039246\n",
      "Epoch: 0, Step: 79712, Loss: 0.8982227444648743\n",
      "Epoch: 0, Step: 79744, Loss: 0.8180645704269409\n",
      "Epoch: 0, Step: 79776, Loss: 0.8721287250518799\n",
      "Epoch: 0, Step: 79808, Loss: 0.8373864889144897\n",
      "Epoch: 0, Step: 79840, Loss: 0.7936145663261414\n",
      "Epoch: 0, Step: 79872, Loss: 0.7556065320968628\n",
      "Epoch: 0, Step: 79904, Loss: 0.7689130902290344\n",
      "Epoch: 0, Step: 79936, Loss: 0.8128584623336792\n",
      "Epoch: 0, Step: 79968, Loss: 0.9192827939987183\n",
      "Epoch: 0, Step: 80000, Loss: 0.833504319190979\n",
      "Epoch: 0, Step: 80032, Loss: 0.8153398036956787\n",
      "Epoch: 0, Step: 80064, Loss: 0.7978869080543518\n",
      "Epoch: 0, Step: 80096, Loss: 0.7695443034172058\n",
      "Epoch: 0, Step: 80128, Loss: 0.8206411600112915\n",
      "Epoch: 0, Step: 80160, Loss: 0.7789255380630493\n",
      "Epoch: 0, Step: 80192, Loss: 0.9245890378952026\n",
      "Epoch: 0, Step: 80224, Loss: 0.7399784922599792\n",
      "Epoch: 0, Step: 80256, Loss: 0.750468373298645\n",
      "Epoch: 0, Step: 80288, Loss: 0.731057345867157\n",
      "Epoch: 0, Step: 80320, Loss: 0.8281208276748657\n",
      "Epoch: 0, Step: 80352, Loss: 0.8479040265083313\n",
      "Epoch: 0, Step: 80384, Loss: 0.8402959704399109\n",
      "Epoch: 0, Step: 80416, Loss: 0.7901953458786011\n",
      "Epoch: 0, Step: 80448, Loss: 0.8669699430465698\n",
      "Epoch: 0, Step: 80480, Loss: 0.8191302418708801\n",
      "Epoch: 0, Step: 80512, Loss: 0.8620777130126953\n",
      "Epoch: 0, Step: 80544, Loss: 0.7584031224250793\n",
      "Epoch: 0, Step: 80576, Loss: 0.7407659888267517\n",
      "Epoch: 0, Step: 80608, Loss: 0.7677794098854065\n",
      "Epoch: 0, Step: 80640, Loss: 0.7985762357711792\n",
      "Epoch: 0, Step: 80672, Loss: 0.7928507328033447\n",
      "Epoch: 0, Step: 80704, Loss: 0.8789334893226624\n",
      "Epoch: 0, Step: 80736, Loss: 0.8589303493499756\n",
      "Epoch: 0, Step: 80768, Loss: 0.8664581179618835\n",
      "Epoch: 0, Step: 80800, Loss: 0.8422583937644958\n",
      "Epoch: 0, Step: 80832, Loss: 0.738308846950531\n",
      "Epoch: 0, Step: 80864, Loss: 0.7718850374221802\n",
      "Epoch: 0, Step: 80896, Loss: 0.9020079374313354\n",
      "Epoch: 0, Step: 80928, Loss: 0.7982476353645325\n",
      "Epoch: 0, Step: 80960, Loss: 0.8868967294692993\n",
      "Epoch: 0, Step: 80992, Loss: 0.8177913427352905\n",
      "Epoch: 0, Step: 81024, Loss: 0.8126888871192932\n",
      "Epoch: 0, Step: 81056, Loss: 0.858131468296051\n",
      "Epoch: 0, Step: 81088, Loss: 0.9163792729377747\n",
      "Epoch: 0, Step: 81120, Loss: 0.8281089663505554\n",
      "Epoch: 0, Step: 81152, Loss: 0.8847644329071045\n",
      "Epoch: 0, Step: 81184, Loss: 0.809691309928894\n",
      "Epoch: 0, Step: 81216, Loss: 0.8080852031707764\n",
      "Epoch: 0, Step: 81248, Loss: 0.7971392869949341\n",
      "Epoch: 0, Step: 81280, Loss: 0.8385162949562073\n",
      "Epoch: 0, Step: 81312, Loss: 0.7611632943153381\n",
      "Epoch: 0, Step: 81344, Loss: 0.8028920888900757\n",
      "Epoch: 0, Step: 81376, Loss: 0.8666186928749084\n",
      "Epoch: 0, Step: 81408, Loss: 0.8089202046394348\n",
      "Epoch: 0, Step: 81440, Loss: 0.7390201091766357\n",
      "Epoch: 0, Step: 81472, Loss: 0.7984887361526489\n",
      "Epoch: 0, Step: 81504, Loss: 0.83400958776474\n",
      "Epoch: 0, Step: 81536, Loss: 0.7475935220718384\n",
      "Epoch: 0, Step: 81568, Loss: 0.7854426503181458\n",
      "Epoch: 0, Step: 81600, Loss: 0.78370600938797\n",
      "Epoch: 0, Step: 81632, Loss: 0.7790025472640991\n",
      "Epoch: 0, Step: 81664, Loss: 0.7733843922615051\n",
      "Epoch: 0, Step: 81696, Loss: 0.8335657715797424\n",
      "Epoch: 0, Step: 81728, Loss: 0.863838255405426\n",
      "Epoch: 0, Step: 81760, Loss: 0.8903645277023315\n",
      "Epoch: 0, Step: 81792, Loss: 0.8114131093025208\n",
      "Epoch: 0, Step: 81824, Loss: 0.7894536852836609\n",
      "Epoch: 0, Step: 81856, Loss: 0.8014369606971741\n",
      "Epoch: 0, Step: 81888, Loss: 0.8530592322349548\n",
      "Epoch: 0, Step: 81920, Loss: 0.7744078040122986\n",
      "Epoch: 0, Step: 81952, Loss: 0.7850221395492554\n",
      "Epoch: 0, Step: 81984, Loss: 0.7592567205429077\n",
      "Epoch: 0, Step: 82016, Loss: 0.8393466472625732\n",
      "Epoch: 0, Step: 82048, Loss: 0.8735601305961609\n",
      "Epoch: 0, Step: 82080, Loss: 0.7829574346542358\n",
      "Epoch: 0, Step: 82112, Loss: 0.8262408375740051\n",
      "Epoch: 0, Step: 82144, Loss: 0.780486524105072\n",
      "Epoch: 0, Step: 82176, Loss: 0.885766863822937\n",
      "Epoch: 0, Step: 82208, Loss: 0.758522629737854\n",
      "Epoch: 0, Step: 82240, Loss: 0.864996612071991\n",
      "Epoch: 0, Step: 82272, Loss: 0.7475049495697021\n",
      "Epoch: 0, Step: 82304, Loss: 0.8535830974578857\n",
      "Epoch: 0, Step: 82336, Loss: 0.8454180955886841\n",
      "Epoch: 0, Step: 82368, Loss: 0.8283975124359131\n",
      "Epoch: 0, Step: 82400, Loss: 0.8408969640731812\n",
      "Epoch: 0, Step: 82432, Loss: 0.7715842723846436\n",
      "Epoch: 0, Step: 82464, Loss: 0.8082711100578308\n",
      "Epoch: 0, Step: 82496, Loss: 0.8461256623268127\n",
      "Epoch: 0, Step: 82528, Loss: 0.8500296473503113\n",
      "Epoch: 0, Step: 82560, Loss: 0.8432630300521851\n",
      "Epoch: 0, Step: 82592, Loss: 0.7926667332649231\n",
      "Epoch: 0, Step: 82624, Loss: 0.7587262392044067\n",
      "Epoch: 0, Step: 82656, Loss: 0.7668929696083069\n",
      "Epoch: 0, Step: 82688, Loss: 0.8641770482063293\n",
      "Epoch: 0, Step: 82720, Loss: 0.8727427124977112\n",
      "Epoch: 0, Step: 82752, Loss: 0.8658803105354309\n",
      "Epoch: 0, Step: 82784, Loss: 0.852598249912262\n",
      "Epoch: 0, Step: 82816, Loss: 0.8197237253189087\n",
      "Epoch: 0, Step: 82848, Loss: 0.7039557099342346\n",
      "Epoch: 0, Step: 82880, Loss: 0.9076054692268372\n",
      "Epoch: 0, Step: 82912, Loss: 0.8994532823562622\n",
      "Epoch: 0, Step: 82944, Loss: 0.736203134059906\n",
      "Epoch: 0, Step: 82976, Loss: 0.8144395351409912\n",
      "Epoch: 0, Step: 83008, Loss: 0.9676518440246582\n",
      "Epoch: 0, Step: 83040, Loss: 0.7702739238739014\n",
      "Epoch: 0, Step: 83072, Loss: 0.8863173127174377\n",
      "Epoch: 0, Step: 83104, Loss: 0.7604972720146179\n",
      "Epoch: 0, Step: 83136, Loss: 0.8224037885665894\n",
      "Epoch: 0, Step: 83168, Loss: 0.8360667824745178\n",
      "Epoch: 0, Step: 83200, Loss: 0.8700010180473328\n",
      "Epoch: 0, Step: 83232, Loss: 0.7718517780303955\n",
      "Epoch: 0, Step: 83264, Loss: 0.9197168350219727\n",
      "Epoch: 0, Step: 83296, Loss: 0.8408530354499817\n",
      "Epoch: 0, Step: 83328, Loss: 0.7797320485115051\n",
      "Epoch: 0, Step: 83360, Loss: 0.793546736240387\n",
      "Epoch: 0, Step: 83392, Loss: 0.7976223230361938\n",
      "Epoch: 0, Step: 83424, Loss: 0.783267080783844\n",
      "Epoch: 0, Step: 83456, Loss: 0.7703403830528259\n",
      "Epoch: 0, Step: 83488, Loss: 0.7865687012672424\n",
      "Epoch: 0, Step: 83520, Loss: 0.7816184163093567\n",
      "Epoch: 0, Step: 83552, Loss: 0.8570044636726379\n",
      "Epoch: 0, Step: 83584, Loss: 0.7860438823699951\n",
      "Epoch: 0, Step: 83616, Loss: 0.7728289365768433\n",
      "Epoch: 0, Step: 83648, Loss: 0.8354750871658325\n",
      "Epoch: 0, Step: 83680, Loss: 0.8309332132339478\n",
      "Epoch: 0, Step: 83712, Loss: 0.8773887753486633\n",
      "Epoch: 0, Step: 83744, Loss: 0.8456301093101501\n",
      "Epoch: 0, Step: 83776, Loss: 0.8637745976448059\n",
      "Epoch: 0, Step: 83808, Loss: 0.8602679371833801\n",
      "Epoch: 0, Step: 83840, Loss: 0.7182989716529846\n",
      "Epoch: 0, Step: 83872, Loss: 0.9157356023788452\n",
      "Epoch: 0, Step: 83904, Loss: 0.9177282452583313\n",
      "Epoch: 0, Step: 83936, Loss: 0.8269974589347839\n",
      "Epoch: 0, Step: 83968, Loss: 0.8127833604812622\n",
      "Epoch: 0, Step: 84000, Loss: 0.7500988841056824\n",
      "Epoch: 0, Step: 84032, Loss: 0.7901868224143982\n",
      "Epoch: 0, Step: 84064, Loss: 0.8815863728523254\n",
      "Epoch: 0, Step: 84096, Loss: 0.8923242092132568\n",
      "Epoch: 0, Step: 84128, Loss: 0.8172481060028076\n",
      "Epoch: 0, Step: 84160, Loss: 0.7065562009811401\n",
      "Epoch: 0, Step: 84192, Loss: 0.8542412519454956\n",
      "Epoch: 0, Step: 84224, Loss: 0.8770248293876648\n",
      "Epoch: 0, Step: 84256, Loss: 0.8744136691093445\n",
      "Epoch: 0, Step: 84288, Loss: 0.7173184156417847\n",
      "Epoch: 0, Step: 84320, Loss: 0.8440148830413818\n",
      "Epoch: 0, Step: 84352, Loss: 0.7809821963310242\n",
      "Epoch: 0, Step: 84384, Loss: 0.9082584977149963\n",
      "Epoch: 0, Step: 84416, Loss: 0.7577965259552002\n",
      "Epoch: 0, Step: 84448, Loss: 0.7537482380867004\n",
      "Epoch: 0, Step: 84480, Loss: 0.7735467553138733\n",
      "Epoch: 0, Step: 84512, Loss: 0.7771963477134705\n",
      "Epoch: 0, Step: 84544, Loss: 0.8319274187088013\n",
      "Epoch: 0, Step: 84576, Loss: 0.8153223395347595\n",
      "Epoch: 0, Step: 84608, Loss: 0.9196499586105347\n",
      "Epoch: 0, Step: 84640, Loss: 0.7204793095588684\n",
      "Epoch: 0, Step: 84672, Loss: 0.8054232001304626\n",
      "Epoch: 0, Step: 84704, Loss: 0.9300538897514343\n",
      "Epoch: 0, Step: 84736, Loss: 0.8180950880050659\n",
      "Epoch: 0, Step: 84768, Loss: 0.7889201045036316\n",
      "Epoch: 0, Step: 84800, Loss: 0.7313362956047058\n",
      "Epoch: 0, Step: 84832, Loss: 0.8023507595062256\n",
      "Epoch: 0, Step: 84864, Loss: 0.8502054810523987\n",
      "Epoch: 0, Step: 84896, Loss: 0.7815887331962585\n",
      "Epoch: 0, Step: 84928, Loss: 0.8869141340255737\n",
      "Epoch: 0, Step: 84960, Loss: 0.8387649059295654\n",
      "Epoch: 0, Step: 84992, Loss: 0.7741261124610901\n",
      "Epoch: 0, Step: 85024, Loss: 0.846551775932312\n",
      "Epoch: 0, Step: 85056, Loss: 0.9069768786430359\n",
      "Epoch: 0, Step: 85088, Loss: 0.7172342538833618\n",
      "Epoch: 0, Step: 85120, Loss: 0.8519484996795654\n",
      "Epoch: 0, Step: 85152, Loss: 0.8046709895133972\n",
      "Epoch: 0, Step: 85184, Loss: 0.879266083240509\n",
      "Epoch: 0, Step: 85216, Loss: 0.7788296341896057\n",
      "Epoch: 0, Step: 85248, Loss: 0.8662164211273193\n",
      "Epoch: 0, Step: 85280, Loss: 0.8556915521621704\n",
      "Epoch: 0, Step: 85312, Loss: 0.823544442653656\n",
      "Epoch: 0, Step: 85344, Loss: 0.7124671339988708\n",
      "Epoch: 0, Step: 85376, Loss: 0.7921306490898132\n",
      "Epoch: 0, Step: 85408, Loss: 0.8682321310043335\n",
      "Epoch: 0, Step: 85440, Loss: 0.8323947191238403\n",
      "Epoch: 0, Step: 85472, Loss: 0.8646457195281982\n",
      "Epoch: 0, Step: 85504, Loss: 0.737561821937561\n",
      "Epoch: 0, Step: 85536, Loss: 0.8370548486709595\n",
      "Epoch: 0, Step: 85568, Loss: 0.794008195400238\n",
      "Epoch: 0, Step: 85600, Loss: 0.7837516069412231\n",
      "Epoch: 0, Step: 85632, Loss: 0.9298055171966553\n",
      "Epoch: 0, Step: 85664, Loss: 0.8720114827156067\n",
      "Epoch: 0, Step: 85696, Loss: 0.8972683548927307\n",
      "Epoch: 0, Step: 85728, Loss: 0.7941336035728455\n",
      "Epoch: 0, Step: 85760, Loss: 0.8335352540016174\n",
      "Epoch: 0, Step: 85792, Loss: 0.8767514228820801\n",
      "Epoch: 0, Step: 85824, Loss: 0.7200729250907898\n",
      "Epoch: 0, Step: 85856, Loss: 0.83591628074646\n",
      "Epoch: 0, Step: 85888, Loss: 0.7814204692840576\n",
      "Epoch: 0, Step: 85920, Loss: 0.8305887579917908\n",
      "Epoch: 0, Step: 85952, Loss: 0.820837140083313\n",
      "Epoch: 0, Step: 85984, Loss: 0.8141297101974487\n",
      "Epoch: 0, Step: 86016, Loss: 0.8251124024391174\n",
      "Epoch: 0, Step: 86048, Loss: 0.9374369382858276\n",
      "Epoch: 0, Step: 86080, Loss: 0.8117353916168213\n",
      "Epoch: 0, Step: 86112, Loss: 0.7874408960342407\n",
      "Epoch: 0, Step: 86144, Loss: 0.8957427144050598\n",
      "Epoch: 0, Step: 86176, Loss: 0.7796118855476379\n",
      "Epoch: 0, Step: 86208, Loss: 0.7771230936050415\n",
      "Epoch: 0, Step: 86240, Loss: 0.8404126763343811\n",
      "Epoch: 0, Step: 86272, Loss: 0.8516315817832947\n",
      "Epoch: 0, Step: 86304, Loss: 0.7902659177780151\n",
      "Epoch: 0, Step: 86336, Loss: 0.8838354349136353\n",
      "Epoch: 0, Step: 86368, Loss: 0.8099850416183472\n",
      "Epoch: 0, Step: 86400, Loss: 0.8484045267105103\n",
      "Epoch: 0, Step: 86432, Loss: 0.8420568704605103\n",
      "Epoch: 0, Step: 86464, Loss: 0.8807794451713562\n",
      "Epoch: 0, Step: 86496, Loss: 0.8643025159835815\n",
      "Epoch: 0, Step: 86528, Loss: 0.853601336479187\n",
      "Epoch: 0, Step: 86560, Loss: 0.934522807598114\n",
      "Epoch: 0, Step: 86592, Loss: 0.8462725281715393\n",
      "Epoch: 0, Step: 86624, Loss: 0.8302012085914612\n",
      "Epoch: 0, Step: 86656, Loss: 0.7505959272384644\n",
      "Epoch: 0, Step: 86688, Loss: 0.7932054996490479\n",
      "Epoch: 0, Step: 86720, Loss: 0.8542897701263428\n",
      "Epoch: 0, Step: 86752, Loss: 0.7298067808151245\n",
      "Epoch: 0, Step: 86784, Loss: 0.8348502516746521\n",
      "Epoch: 0, Step: 86816, Loss: 0.8165484070777893\n",
      "Epoch: 0, Step: 86848, Loss: 0.8150092959403992\n",
      "Epoch: 0, Step: 86880, Loss: 0.6684848666191101\n",
      "Epoch: 0, Step: 86912, Loss: 0.9439831972122192\n",
      "Epoch: 0, Step: 86944, Loss: 0.8837876915931702\n",
      "Epoch: 0, Step: 86976, Loss: 0.8864893913269043\n",
      "Epoch: 0, Step: 87008, Loss: 0.7830210328102112\n",
      "Epoch: 0, Step: 87040, Loss: 0.7431007027626038\n",
      "Epoch: 0, Step: 87072, Loss: 0.87373286485672\n",
      "Epoch: 0, Step: 87104, Loss: 0.7513101100921631\n",
      "Epoch: 0, Step: 87136, Loss: 0.8229786157608032\n",
      "Epoch: 0, Step: 87168, Loss: 0.9421594142913818\n",
      "Epoch: 0, Step: 87200, Loss: 0.7634376287460327\n",
      "Epoch: 0, Step: 87232, Loss: 0.8108866810798645\n",
      "Epoch: 0, Step: 87264, Loss: 0.8608932495117188\n",
      "Epoch: 0, Step: 87296, Loss: 0.759626567363739\n",
      "Epoch: 0, Step: 87328, Loss: 0.7450365424156189\n",
      "Epoch: 0, Step: 87360, Loss: 0.8418397307395935\n",
      "Epoch: 0, Step: 87392, Loss: 0.8390153050422668\n",
      "Epoch: 0, Step: 87424, Loss: 0.7382770776748657\n",
      "Epoch: 0, Step: 87456, Loss: 0.922701358795166\n",
      "Epoch: 0, Step: 87488, Loss: 0.8600690364837646\n",
      "Epoch: 0, Step: 87520, Loss: 0.846258282661438\n",
      "Epoch: 0, Step: 87552, Loss: 0.766375720500946\n",
      "Epoch: 0, Step: 87584, Loss: 0.8813362121582031\n",
      "Epoch: 0, Step: 87616, Loss: 0.8214272856712341\n",
      "Epoch: 0, Step: 87648, Loss: 0.8306566476821899\n",
      "Epoch: 0, Step: 87680, Loss: 0.7494022250175476\n",
      "Epoch: 0, Step: 87712, Loss: 0.8429077863693237\n",
      "Epoch: 0, Step: 87744, Loss: 0.8648537397384644\n",
      "Epoch: 0, Step: 87776, Loss: 0.7901044487953186\n",
      "Epoch: 0, Step: 87808, Loss: 0.7695676684379578\n",
      "Epoch: 0, Step: 87840, Loss: 0.7532626986503601\n",
      "Epoch: 0, Step: 87872, Loss: 0.7424141764640808\n",
      "Epoch: 0, Step: 87904, Loss: 0.8566934466362\n",
      "Epoch: 0, Step: 87936, Loss: 0.8044598698616028\n",
      "Epoch: 0, Step: 87968, Loss: 0.7775839567184448\n",
      "Epoch: 0, Step: 88000, Loss: 0.7488456964492798\n",
      "Epoch: 0, Step: 88032, Loss: 0.8571961522102356\n",
      "Epoch: 0, Step: 88064, Loss: 0.8780873417854309\n",
      "Epoch: 0, Step: 88096, Loss: 0.8395405411720276\n",
      "Epoch: 0, Step: 88128, Loss: 0.786808431148529\n",
      "Epoch: 0, Step: 88160, Loss: 0.886599063873291\n",
      "Epoch: 0, Step: 88192, Loss: 0.875572681427002\n",
      "Epoch: 0, Step: 88224, Loss: 0.973259449005127\n",
      "Epoch: 0, Step: 88256, Loss: 0.8782961964607239\n",
      "Epoch: 0, Step: 88288, Loss: 0.8248361945152283\n",
      "Epoch: 0, Step: 88320, Loss: 0.8587836027145386\n",
      "Epoch: 0, Step: 88352, Loss: 0.8896263837814331\n",
      "Epoch: 0, Step: 88384, Loss: 0.8341371417045593\n",
      "Epoch: 0, Step: 88416, Loss: 0.8724275827407837\n",
      "Epoch: 0, Step: 88448, Loss: 0.7417933940887451\n",
      "Epoch: 0, Step: 88480, Loss: 0.950137197971344\n",
      "Epoch: 0, Step: 88512, Loss: 0.9464566707611084\n",
      "Epoch: 0, Step: 88544, Loss: 0.8145488500595093\n",
      "Epoch: 0, Step: 88576, Loss: 0.7136824131011963\n",
      "Epoch: 0, Step: 88608, Loss: 0.7822901606559753\n",
      "Epoch: 0, Step: 88640, Loss: 0.8011928796768188\n",
      "Epoch: 0, Step: 88672, Loss: 0.7672188878059387\n",
      "Epoch: 0, Step: 88704, Loss: 0.7465824484825134\n",
      "Epoch: 0, Step: 88736, Loss: 0.8429802656173706\n",
      "Epoch: 0, Step: 88768, Loss: 0.8533595204353333\n",
      "Epoch: 0, Step: 88800, Loss: 0.847710371017456\n",
      "Epoch: 0, Step: 88832, Loss: 0.7458909749984741\n",
      "Epoch: 0, Step: 88864, Loss: 0.784061849117279\n",
      "Epoch: 0, Step: 88896, Loss: 0.8508880734443665\n",
      "Epoch: 0, Step: 88928, Loss: 0.7816482782363892\n",
      "Epoch: 0, Step: 88960, Loss: 0.743336021900177\n",
      "Epoch: 0, Step: 88992, Loss: 0.8886770009994507\n",
      "Epoch: 0, Step: 89024, Loss: 0.8939787745475769\n",
      "Epoch: 0, Step: 89056, Loss: 0.8049935698509216\n",
      "Epoch: 0, Step: 89088, Loss: 0.8426558971405029\n",
      "Epoch: 0, Step: 89120, Loss: 0.9362404942512512\n",
      "Epoch: 0, Step: 89152, Loss: 0.8897992968559265\n",
      "Epoch: 0, Step: 89184, Loss: 0.8128911256790161\n",
      "Epoch: 0, Step: 89216, Loss: 0.8906141519546509\n",
      "Epoch: 0, Step: 89248, Loss: 0.7217272520065308\n",
      "Epoch: 0, Step: 89280, Loss: 0.8918453454971313\n",
      "Epoch: 0, Step: 89312, Loss: 0.8671008944511414\n",
      "Epoch: 0, Step: 89344, Loss: 0.7956980466842651\n",
      "Epoch: 0, Step: 89376, Loss: 0.7826440334320068\n",
      "Epoch: 0, Step: 89408, Loss: 0.8821463584899902\n",
      "Epoch: 0, Step: 89440, Loss: 0.8196169137954712\n",
      "Epoch: 0, Step: 89472, Loss: 0.7639859318733215\n",
      "Epoch: 0, Step: 89504, Loss: 0.7069450616836548\n",
      "Epoch: 0, Step: 89536, Loss: 0.733583390712738\n",
      "Epoch: 0, Step: 89568, Loss: 0.7220099568367004\n",
      "Epoch: 0, Step: 89600, Loss: 0.8917804956436157\n",
      "Epoch: 0, Step: 89632, Loss: 0.8264428377151489\n",
      "Epoch: 0, Step: 89664, Loss: 0.907816469669342\n",
      "Epoch: 0, Step: 89696, Loss: 0.7612903118133545\n",
      "Epoch: 0, Step: 89728, Loss: 0.8875221014022827\n",
      "Epoch: 0, Step: 89760, Loss: 0.8658533692359924\n",
      "Epoch: 0, Step: 89792, Loss: 0.7932295203208923\n",
      "Epoch: 0, Step: 89824, Loss: 0.8037678003311157\n",
      "Epoch: 0, Step: 89856, Loss: 0.8342284560203552\n",
      "Epoch: 0, Step: 89888, Loss: 0.8467780947685242\n",
      "Epoch: 0, Step: 89920, Loss: 0.7422793507575989\n",
      "Epoch: 0, Step: 89952, Loss: 0.8364694118499756\n",
      "Epoch: 0, Step: 89984, Loss: 0.7584866881370544\n",
      "Epoch: 0, Step: 90016, Loss: 0.7723079919815063\n",
      "Epoch: 0, Step: 90048, Loss: 0.829221785068512\n",
      "Epoch: 0, Step: 90080, Loss: 0.8719220161437988\n",
      "Epoch: 0, Step: 90112, Loss: 0.8979827761650085\n",
      "Epoch: 0, Step: 90144, Loss: 0.8260099291801453\n",
      "Epoch: 0, Step: 90176, Loss: 0.8249486684799194\n",
      "Epoch: 0, Step: 90208, Loss: 0.8842440247535706\n",
      "Epoch: 0, Step: 90240, Loss: 0.8385254144668579\n",
      "Epoch: 0, Step: 90272, Loss: 0.8661981821060181\n",
      "Epoch: 0, Step: 90304, Loss: 0.8345919251441956\n",
      "Epoch: 0, Step: 90336, Loss: 0.8716368675231934\n",
      "Epoch: 0, Step: 90368, Loss: 0.7781679034233093\n",
      "Epoch: 0, Step: 90400, Loss: 0.850345253944397\n",
      "Epoch: 0, Step: 90432, Loss: 0.924042284488678\n",
      "Epoch: 0, Step: 90464, Loss: 0.7830286622047424\n",
      "Epoch: 0, Step: 90496, Loss: 0.7700299024581909\n",
      "Epoch: 0, Step: 90528, Loss: 0.961942195892334\n",
      "Epoch: 0, Step: 90560, Loss: 0.8175693154335022\n",
      "Epoch: 0, Step: 90592, Loss: 0.829272449016571\n",
      "Epoch: 0, Step: 90624, Loss: 0.8434545397758484\n",
      "Epoch: 0, Step: 90656, Loss: 0.8110430836677551\n",
      "Epoch: 0, Step: 90688, Loss: 0.8155218362808228\n",
      "Epoch: 0, Step: 90720, Loss: 0.789348304271698\n",
      "Epoch: 0, Step: 90752, Loss: 0.9103041291236877\n",
      "Epoch: 0, Step: 90784, Loss: 0.8667794466018677\n",
      "Epoch: 0, Step: 90816, Loss: 0.802509069442749\n",
      "Epoch: 0, Step: 90848, Loss: 0.8316088914871216\n",
      "Epoch: 0, Step: 90880, Loss: 0.7395824193954468\n",
      "Epoch: 0, Step: 90912, Loss: 0.8006703853607178\n",
      "Epoch: 0, Step: 90944, Loss: 0.9743952751159668\n",
      "Epoch: 0, Step: 90976, Loss: 0.8439818024635315\n",
      "Epoch: 0, Step: 91008, Loss: 0.7745964527130127\n",
      "Epoch: 0, Step: 91040, Loss: 0.9025170803070068\n",
      "Epoch: 0, Step: 91072, Loss: 0.7953660488128662\n",
      "Epoch: 0, Step: 91104, Loss: 0.7530997395515442\n",
      "Epoch: 0, Step: 91136, Loss: 0.7962912917137146\n",
      "Epoch: 0, Step: 91168, Loss: 0.8325482606887817\n",
      "Epoch: 0, Step: 91200, Loss: 0.7777374982833862\n",
      "Epoch: 0, Step: 91232, Loss: 0.7194854021072388\n",
      "Epoch: 0, Step: 91264, Loss: 0.963601291179657\n",
      "Epoch: 0, Step: 91296, Loss: 0.7167573571205139\n",
      "Epoch: 0, Step: 91328, Loss: 0.791924238204956\n",
      "Epoch: 0, Step: 91360, Loss: 0.7417991161346436\n",
      "Epoch: 0, Step: 91392, Loss: 0.8753849267959595\n",
      "Epoch: 0, Step: 91424, Loss: 0.8942294716835022\n",
      "Epoch: 0, Step: 91456, Loss: 0.7911546230316162\n",
      "Epoch: 0, Step: 91488, Loss: 0.7914863228797913\n",
      "Epoch: 0, Step: 91520, Loss: 0.8026682138442993\n",
      "Epoch: 0, Step: 91552, Loss: 0.8434819579124451\n",
      "Epoch: 0, Step: 91584, Loss: 0.8905762434005737\n",
      "Epoch: 0, Step: 91616, Loss: 0.8180320262908936\n",
      "Epoch: 0, Step: 91648, Loss: 0.9243714213371277\n",
      "Epoch: 0, Step: 91680, Loss: 0.8069409132003784\n",
      "Epoch: 0, Step: 91712, Loss: 0.6832510232925415\n",
      "Epoch: 0, Step: 91744, Loss: 0.7774150371551514\n",
      "Epoch: 0, Step: 91776, Loss: 0.8019773960113525\n",
      "Epoch: 0, Step: 91808, Loss: 0.8605334758758545\n",
      "Epoch: 0, Step: 91840, Loss: 0.8441326022148132\n",
      "Epoch: 0, Step: 91872, Loss: 0.8038228154182434\n",
      "Epoch: 0, Step: 91904, Loss: 0.8715975880622864\n",
      "Epoch: 0, Step: 91936, Loss: 0.7195451855659485\n",
      "Epoch: 0, Step: 91968, Loss: 0.9064982533454895\n",
      "Epoch: 0, Step: 92000, Loss: 0.8535844683647156\n",
      "Epoch: 0, Step: 92032, Loss: 0.7690197229385376\n",
      "Epoch: 0, Step: 92064, Loss: 0.8685803413391113\n",
      "Epoch: 0, Step: 92096, Loss: 0.7805691361427307\n",
      "Epoch: 0, Step: 92128, Loss: 0.8650971055030823\n",
      "Epoch: 0, Step: 92160, Loss: 0.7714701294898987\n",
      "Epoch: 0, Step: 92192, Loss: 0.8297572731971741\n",
      "Epoch: 0, Step: 92224, Loss: 0.8497388958930969\n",
      "Epoch: 0, Step: 92256, Loss: 0.7711619138717651\n",
      "Epoch: 0, Step: 92288, Loss: 0.8221226334571838\n",
      "Epoch: 0, Step: 92320, Loss: 0.7857669591903687\n",
      "Epoch: 0, Step: 92352, Loss: 0.7643778920173645\n",
      "Epoch: 0, Step: 92384, Loss: 0.8182801604270935\n",
      "Epoch: 0, Step: 92416, Loss: 0.8396778702735901\n",
      "Epoch: 0, Step: 92448, Loss: 0.8498864769935608\n",
      "Epoch: 0, Step: 92480, Loss: 0.8367704749107361\n",
      "Epoch: 0, Step: 92512, Loss: 0.8610924482345581\n",
      "Epoch: 0, Step: 92544, Loss: 0.6479110717773438\n",
      "Epoch: 0, Step: 92576, Loss: 0.8721499443054199\n",
      "Epoch: 0, Step: 92608, Loss: 0.8261821866035461\n",
      "Epoch: 0, Step: 92640, Loss: 0.836912214756012\n",
      "Epoch: 0, Step: 92672, Loss: 0.884221613407135\n",
      "Epoch: 0, Step: 92704, Loss: 0.8089703917503357\n",
      "Epoch: 0, Step: 92736, Loss: 0.7964972257614136\n",
      "Epoch: 0, Step: 92768, Loss: 0.7497158646583557\n",
      "Epoch: 0, Step: 92800, Loss: 0.7704740166664124\n",
      "Epoch: 0, Step: 92832, Loss: 0.8200114965438843\n",
      "Epoch: 0, Step: 92864, Loss: 0.7241767048835754\n",
      "Epoch: 0, Step: 92896, Loss: 0.8780203461647034\n",
      "Epoch: 0, Step: 92928, Loss: 0.7819347977638245\n",
      "Epoch: 0, Step: 92960, Loss: 0.9004642367362976\n",
      "Epoch: 0, Step: 92992, Loss: 0.8359106779098511\n",
      "Epoch: 0, Step: 93024, Loss: 0.7693094611167908\n",
      "Epoch: 0, Step: 93056, Loss: 0.7361677885055542\n",
      "Epoch: 0, Step: 93088, Loss: 0.9327782392501831\n",
      "Epoch: 0, Step: 93120, Loss: 0.860319197177887\n",
      "Epoch: 0, Step: 93152, Loss: 0.8902713656425476\n",
      "Epoch: 0, Step: 93184, Loss: 0.8343492746353149\n",
      "Epoch: 0, Step: 93216, Loss: 0.8738378286361694\n",
      "Epoch: 0, Step: 93248, Loss: 0.803458571434021\n",
      "Epoch: 0, Step: 93280, Loss: 0.883192777633667\n",
      "Epoch: 0, Step: 93312, Loss: 0.7979997992515564\n",
      "Epoch: 0, Step: 93344, Loss: 0.892677366733551\n",
      "Epoch: 0, Step: 93376, Loss: 0.7985194325447083\n",
      "Epoch: 0, Step: 93408, Loss: 0.8401767611503601\n",
      "Epoch: 0, Step: 93440, Loss: 0.8452955484390259\n",
      "Epoch: 0, Step: 93472, Loss: 0.824436366558075\n",
      "Epoch: 0, Step: 93504, Loss: 0.8632346391677856\n",
      "Epoch: 0, Step: 93536, Loss: 0.9003807306289673\n",
      "Epoch: 0, Step: 93568, Loss: 0.8757815361022949\n",
      "Epoch: 0, Step: 93600, Loss: 0.7169994711875916\n",
      "Epoch: 0, Step: 93632, Loss: 0.85240638256073\n",
      "Epoch: 0, Step: 93664, Loss: 0.8289037942886353\n",
      "Epoch: 0, Step: 93696, Loss: 0.8629854917526245\n",
      "Epoch: 0, Step: 93728, Loss: 0.9296448826789856\n",
      "Epoch: 0, Step: 93760, Loss: 0.8115274906158447\n",
      "Epoch: 0, Step: 93792, Loss: 0.7623327970504761\n",
      "Epoch: 0, Step: 93824, Loss: 0.8428962230682373\n",
      "Epoch: 0, Step: 93856, Loss: 0.8572782278060913\n",
      "Epoch: 0, Step: 93888, Loss: 0.7859148383140564\n",
      "Epoch: 0, Step: 93920, Loss: 0.9340577125549316\n",
      "Epoch: 0, Step: 93952, Loss: 0.8132474422454834\n",
      "Epoch: 0, Step: 93984, Loss: 0.7801666855812073\n",
      "Epoch: 0, Step: 94016, Loss: 0.7599409818649292\n",
      "Epoch: 0, Step: 94048, Loss: 0.7928840517997742\n",
      "Epoch: 0, Step: 94080, Loss: 0.8063832521438599\n",
      "Epoch: 0, Step: 94112, Loss: 0.8544163107872009\n",
      "Epoch: 0, Step: 94144, Loss: 0.8809806704521179\n",
      "Epoch: 0, Step: 94176, Loss: 0.8099175095558167\n",
      "Epoch: 0, Step: 94208, Loss: 0.7558451890945435\n",
      "Epoch: 0, Step: 94240, Loss: 0.8469952344894409\n",
      "Epoch: 0, Step: 94272, Loss: 0.8350369930267334\n",
      "Epoch: 0, Step: 94304, Loss: 0.8697746992111206\n",
      "Epoch: 0, Step: 94336, Loss: 0.8382905125617981\n",
      "Epoch: 0, Step: 94368, Loss: 0.8889697194099426\n",
      "Epoch: 0, Step: 94400, Loss: 0.8034206032752991\n",
      "Epoch: 0, Step: 94432, Loss: 0.8559114336967468\n",
      "Epoch: 0, Step: 94464, Loss: 0.8473654389381409\n",
      "Epoch: 0, Step: 94496, Loss: 0.7741402387619019\n",
      "Epoch: 0, Step: 94528, Loss: 0.9174326658248901\n",
      "Epoch: 0, Step: 94560, Loss: 0.8724218606948853\n",
      "Epoch: 0, Step: 94592, Loss: 0.8035450577735901\n",
      "Epoch: 0, Step: 94624, Loss: 0.8208671808242798\n",
      "Epoch: 0, Step: 94656, Loss: 0.8123091459274292\n",
      "Epoch: 0, Step: 94688, Loss: 0.7779766321182251\n",
      "Epoch: 0, Step: 94720, Loss: 0.7853401899337769\n",
      "Epoch: 0, Step: 94752, Loss: 0.6991799473762512\n",
      "Epoch: 0, Step: 94784, Loss: 0.8903588056564331\n",
      "Epoch: 0, Step: 94816, Loss: 0.792846143245697\n",
      "Epoch: 0, Step: 94848, Loss: 0.8718065023422241\n",
      "Epoch: 0, Step: 94880, Loss: 0.7949717044830322\n",
      "Epoch: 0, Step: 94912, Loss: 0.8006983399391174\n",
      "Epoch: 0, Step: 94944, Loss: 0.8080639243125916\n",
      "Epoch: 0, Step: 94976, Loss: 0.7213855981826782\n",
      "Epoch: 0, Step: 95008, Loss: 0.8021312355995178\n",
      "Epoch: 0, Step: 95040, Loss: 0.8247097730636597\n",
      "Epoch: 0, Step: 95072, Loss: 0.8485121130943298\n",
      "Epoch: 0, Step: 95104, Loss: 0.8508061766624451\n",
      "Epoch: 0, Step: 95136, Loss: 0.8689432144165039\n",
      "Epoch: 0, Step: 95168, Loss: 0.7730333209037781\n",
      "Epoch: 0, Step: 95200, Loss: 0.8458659052848816\n",
      "Epoch: 0, Step: 95232, Loss: 0.8497509360313416\n",
      "Epoch: 0, Step: 95264, Loss: 0.8424137234687805\n",
      "Epoch: 0, Step: 95296, Loss: 0.7710026502609253\n",
      "Epoch: 0, Step: 95328, Loss: 0.8764196038246155\n",
      "Epoch: 0, Step: 95360, Loss: 0.8897679448127747\n",
      "Epoch: 0, Step: 95392, Loss: 0.7853119373321533\n",
      "Epoch: 0, Step: 95424, Loss: 0.8613015413284302\n",
      "Epoch: 0, Step: 95456, Loss: 0.8719266057014465\n",
      "Epoch: 0, Step: 95488, Loss: 0.7516635060310364\n",
      "Epoch: 0, Step: 95520, Loss: 0.8320677280426025\n",
      "Epoch: 0, Step: 95552, Loss: 0.7872508764266968\n",
      "Epoch: 0, Step: 95584, Loss: 0.8074339628219604\n",
      "Epoch: 0, Step: 95616, Loss: 0.777568519115448\n",
      "Epoch: 0, Step: 95648, Loss: 0.8346650004386902\n",
      "Epoch: 0, Step: 95680, Loss: 0.8656220436096191\n",
      "Epoch: 0, Step: 95712, Loss: 0.8537852168083191\n",
      "Epoch: 0, Step: 95744, Loss: 0.8446263670921326\n",
      "Epoch: 0, Step: 95776, Loss: 0.7919210195541382\n",
      "Epoch: 0, Step: 95808, Loss: 0.8203936815261841\n",
      "Epoch: 0, Step: 95840, Loss: 0.8242056965827942\n",
      "Epoch: 0, Step: 95872, Loss: 0.744968831539154\n",
      "Epoch: 0, Step: 95904, Loss: 0.9372105598449707\n",
      "Epoch: 0, Step: 95936, Loss: 0.9089821577072144\n",
      "Epoch: 0, Step: 95968, Loss: 0.771015465259552\n",
      "Epoch: 0, Step: 96000, Loss: 0.8197963237762451\n",
      "Epoch: 0, Step: 96032, Loss: 0.886768102645874\n",
      "Epoch: 0, Step: 96064, Loss: 0.8029624819755554\n",
      "Epoch: 0, Step: 96096, Loss: 0.7298302054405212\n",
      "Epoch: 0, Step: 96128, Loss: 0.7854805588722229\n",
      "Epoch: 0, Step: 96160, Loss: 0.8112372159957886\n",
      "Epoch: 0, Step: 96192, Loss: 0.9455815553665161\n",
      "Epoch: 0, Step: 96224, Loss: 0.7965428233146667\n",
      "Epoch: 0, Step: 96256, Loss: 0.8905311822891235\n",
      "Epoch: 0, Step: 96288, Loss: 0.8384392857551575\n",
      "Epoch: 0, Step: 96320, Loss: 0.8511897921562195\n",
      "Epoch: 0, Step: 96352, Loss: 0.8318507075309753\n",
      "Epoch: 0, Step: 96384, Loss: 0.8363961577415466\n",
      "Epoch: 0, Step: 96416, Loss: 0.8144981265068054\n",
      "Epoch: 0, Step: 96448, Loss: 0.728340744972229\n",
      "Epoch: 0, Step: 96480, Loss: 0.8639535903930664\n",
      "Epoch: 0, Step: 96512, Loss: 0.8172991871833801\n",
      "Epoch: 0, Step: 96544, Loss: 0.9020448923110962\n",
      "Epoch: 0, Step: 96576, Loss: 0.8672158718109131\n",
      "Epoch: 0, Step: 96608, Loss: 0.7746383547782898\n",
      "Epoch: 0, Step: 96640, Loss: 0.7924116253852844\n",
      "Epoch: 0, Step: 96672, Loss: 0.7718872427940369\n",
      "Epoch: 0, Step: 96704, Loss: 0.8456688523292542\n",
      "Epoch: 0, Step: 96736, Loss: 0.9012848138809204\n",
      "Epoch: 0, Step: 96768, Loss: 0.786541223526001\n",
      "Epoch: 0, Step: 96800, Loss: 0.8792057037353516\n",
      "Epoch: 0, Step: 96832, Loss: 0.7842990756034851\n",
      "Epoch: 0, Step: 96864, Loss: 0.865333616733551\n",
      "Epoch: 0, Step: 96896, Loss: 0.8506983518600464\n",
      "Epoch: 0, Step: 96928, Loss: 0.838857889175415\n",
      "Epoch: 0, Step: 96960, Loss: 0.9405180811882019\n",
      "Epoch: 0, Step: 96992, Loss: 0.7973187565803528\n",
      "Epoch: 0, Step: 97024, Loss: 0.8454621434211731\n",
      "Epoch: 0, Step: 97056, Loss: 0.7792364358901978\n",
      "Epoch: 0, Step: 97088, Loss: 0.8111388683319092\n",
      "Epoch: 0, Step: 97120, Loss: 0.7938601970672607\n",
      "Epoch: 0, Step: 97152, Loss: 0.9424798488616943\n",
      "Epoch: 0, Step: 97184, Loss: 0.8335650563240051\n",
      "Epoch: 0, Step: 97216, Loss: 0.801450788974762\n",
      "Epoch: 0, Step: 97248, Loss: 0.8769722580909729\n",
      "Epoch: 0, Step: 97280, Loss: 0.8955903053283691\n",
      "Epoch: 0, Step: 97312, Loss: 0.8254032731056213\n",
      "Epoch: 0, Step: 97344, Loss: 0.8229418396949768\n",
      "Epoch: 0, Step: 97376, Loss: 0.8385640382766724\n",
      "Epoch: 0, Step: 97408, Loss: 0.8675801753997803\n",
      "Epoch: 0, Step: 97440, Loss: 0.8356989026069641\n",
      "Epoch: 0, Step: 97472, Loss: 0.8273646831512451\n",
      "Epoch: 0, Step: 97504, Loss: 0.8295047283172607\n",
      "Epoch: 0, Step: 97536, Loss: 0.806816816329956\n",
      "Epoch: 0, Step: 97568, Loss: 0.9128172993659973\n",
      "Epoch: 0, Step: 97600, Loss: 0.8198640942573547\n",
      "Epoch: 0, Step: 97632, Loss: 0.7650997638702393\n",
      "Epoch: 0, Step: 97664, Loss: 0.8747655749320984\n",
      "Epoch: 0, Step: 97696, Loss: 0.7777849435806274\n",
      "Epoch: 0, Step: 97728, Loss: 0.7618386745452881\n",
      "Epoch: 0, Step: 97760, Loss: 0.8043100237846375\n",
      "Epoch: 0, Step: 97792, Loss: 0.6779661774635315\n",
      "Epoch: 0, Step: 97824, Loss: 0.9896073341369629\n",
      "Epoch: 0, Step: 97856, Loss: 0.8401267528533936\n",
      "Epoch: 0, Step: 97888, Loss: 0.8535566926002502\n",
      "Epoch: 0, Step: 97920, Loss: 0.8432255387306213\n",
      "Epoch: 0, Step: 97952, Loss: 0.8355380892753601\n",
      "Epoch: 0, Step: 97984, Loss: 0.792930543422699\n",
      "Epoch: 0, Step: 98016, Loss: 0.8468171954154968\n",
      "Epoch: 0, Step: 98048, Loss: 0.9064719080924988\n",
      "Epoch: 0, Step: 98080, Loss: 0.8104811310768127\n",
      "Epoch: 0, Step: 98112, Loss: 0.9270198941230774\n",
      "Epoch: 0, Step: 98144, Loss: 0.8531563878059387\n",
      "Epoch: 0, Step: 98176, Loss: 0.6734405159950256\n",
      "Epoch: 0, Step: 98208, Loss: 0.7219608426094055\n",
      "Epoch: 0, Step: 98240, Loss: 0.802468478679657\n",
      "Epoch: 0, Step: 98272, Loss: 0.8741965889930725\n",
      "Epoch: 0, Step: 98304, Loss: 0.8015375733375549\n",
      "Epoch: 0, Step: 98336, Loss: 0.8524941205978394\n",
      "Epoch: 0, Step: 98368, Loss: 0.7786559462547302\n",
      "Epoch: 0, Step: 98400, Loss: 0.8466507196426392\n",
      "Epoch: 0, Step: 98432, Loss: 0.7593649625778198\n",
      "Epoch: 0, Step: 98464, Loss: 0.8467376232147217\n",
      "Epoch: 0, Step: 98496, Loss: 0.7620498538017273\n",
      "Epoch: 0, Step: 98528, Loss: 0.7778646945953369\n",
      "Epoch: 0, Step: 98560, Loss: 0.8773341178894043\n",
      "Epoch: 0, Step: 98592, Loss: 0.8765071630477905\n",
      "Epoch: 0, Step: 98624, Loss: 0.9131401777267456\n",
      "Epoch: 0, Step: 98656, Loss: 0.7968288660049438\n",
      "Epoch: 0, Step: 98688, Loss: 0.8481642007827759\n",
      "Epoch: 0, Step: 98720, Loss: 0.8268502354621887\n",
      "Epoch: 0, Step: 98752, Loss: 0.8012935519218445\n",
      "Epoch: 0, Step: 98784, Loss: 0.9155267477035522\n",
      "Epoch: 0, Step: 98816, Loss: 0.7670681476593018\n",
      "Epoch: 0, Step: 98848, Loss: 0.8502515554428101\n",
      "Epoch: 0, Step: 98880, Loss: 0.8288333415985107\n",
      "Epoch: 0, Step: 98912, Loss: 0.7462717890739441\n",
      "Epoch: 0, Step: 98944, Loss: 0.7863011956214905\n",
      "Epoch: 0, Step: 98976, Loss: 0.8731691241264343\n",
      "Epoch: 0, Step: 99008, Loss: 0.834591805934906\n",
      "Epoch: 0, Step: 99040, Loss: 0.9670892953872681\n",
      "Epoch: 0, Step: 99072, Loss: 0.848910391330719\n",
      "Epoch: 0, Step: 99104, Loss: 0.7854433655738831\n",
      "Epoch: 0, Step: 99136, Loss: 0.8308570981025696\n",
      "Epoch: 0, Step: 99168, Loss: 0.7795040011405945\n",
      "Epoch: 0, Step: 99200, Loss: 0.8655784726142883\n",
      "Epoch: 0, Step: 99232, Loss: 0.8790632486343384\n",
      "Epoch: 0, Step: 99264, Loss: 0.8158775568008423\n",
      "Epoch: 0, Step: 99296, Loss: 0.8952942490577698\n",
      "Epoch: 0, Step: 99328, Loss: 0.8035038709640503\n",
      "Epoch: 0, Step: 99360, Loss: 0.7591679692268372\n",
      "Epoch: 0, Step: 99392, Loss: 0.7919893860816956\n",
      "Epoch: 0, Step: 99424, Loss: 0.8087518215179443\n",
      "Epoch: 0, Step: 99456, Loss: 0.8808805346488953\n",
      "Epoch: 0, Step: 99488, Loss: 0.7721180319786072\n",
      "Epoch: 0, Step: 99520, Loss: 0.858152449131012\n",
      "Epoch: 0, Step: 99552, Loss: 0.8323176503181458\n",
      "Epoch: 0, Step: 99584, Loss: 0.8659104108810425\n",
      "Epoch: 0, Step: 99616, Loss: 0.8179180026054382\n",
      "Epoch: 0, Step: 99648, Loss: 0.8145550489425659\n",
      "Epoch: 0, Step: 99680, Loss: 0.8928462862968445\n",
      "Epoch: 0, Step: 99712, Loss: 0.8555585741996765\n",
      "Epoch: 0, Step: 99744, Loss: 0.7916305065155029\n",
      "Epoch: 0, Step: 99776, Loss: 0.7954766750335693\n",
      "Epoch: 0, Step: 99808, Loss: 0.7145897150039673\n",
      "Epoch: 0, Step: 99840, Loss: 0.8329087495803833\n",
      "Epoch: 0, Step: 99872, Loss: 0.8142226934432983\n",
      "Epoch: 0, Step: 99904, Loss: 1.0013211965560913\n",
      "Epoch: 0, Step: 99936, Loss: 0.8630648851394653\n",
      "Epoch: 0, Step: 99968, Loss: 0.85039222240448\n",
      "Epoch: 0, Step: 100000, Loss: 0.8639417886734009\n",
      "Epoch: 0, Step: 100032, Loss: 0.8334714770317078\n",
      "Epoch: 0, Step: 100064, Loss: 0.8724886178970337\n",
      "Epoch: 0, Step: 100096, Loss: 0.8704355359077454\n",
      "Epoch: 0, Step: 100128, Loss: 0.7990667819976807\n",
      "Epoch: 0, Step: 100160, Loss: 0.6951482892036438\n",
      "Epoch: 0, Step: 100192, Loss: 0.914972186088562\n",
      "Epoch: 0, Step: 100224, Loss: 0.8703081607818604\n",
      "Epoch: 0, Step: 100256, Loss: 0.9052736163139343\n",
      "Epoch: 0, Step: 100288, Loss: 0.8569017052650452\n",
      "Epoch: 0, Step: 100320, Loss: 0.8328789472579956\n",
      "Epoch: 0, Step: 100352, Loss: 0.8771629333496094\n",
      "Epoch: 0, Step: 100384, Loss: 0.7652872800827026\n",
      "Epoch: 0, Step: 100416, Loss: 0.8167580366134644\n",
      "Epoch: 0, Step: 100448, Loss: 0.7021971940994263\n",
      "Epoch: 0, Step: 100480, Loss: 0.7448268532752991\n",
      "Epoch: 0, Step: 100512, Loss: 0.8408767580986023\n",
      "Epoch: 0, Step: 100544, Loss: 0.7731459736824036\n",
      "Epoch: 0, Step: 100576, Loss: 0.8378562331199646\n",
      "Epoch: 0, Step: 100608, Loss: 0.6602504849433899\n",
      "Epoch: 0, Step: 100640, Loss: 0.7807809710502625\n",
      "Epoch: 0, Step: 100672, Loss: 0.8323083519935608\n",
      "Epoch: 0, Step: 100704, Loss: 0.8325304388999939\n",
      "Epoch: 0, Step: 100736, Loss: 0.8965499401092529\n",
      "Epoch: 0, Step: 100768, Loss: 0.8574848771095276\n",
      "Epoch: 0, Step: 100800, Loss: 0.8430200219154358\n",
      "Epoch: 0, Step: 100832, Loss: 0.8677365779876709\n",
      "Epoch: 0, Step: 100864, Loss: 0.7631726861000061\n",
      "Epoch: 0, Step: 100896, Loss: 0.6400277614593506\n",
      "Epoch: 0, Step: 100928, Loss: 0.7815459966659546\n",
      "Epoch: 0, Step: 100960, Loss: 0.9057534337043762\n",
      "Epoch: 0, Step: 100992, Loss: 0.7520215511322021\n",
      "Epoch: 0, Step: 101024, Loss: 0.8616808652877808\n",
      "Epoch: 0, Step: 101056, Loss: 0.8557146787643433\n",
      "Epoch: 0, Step: 101088, Loss: 0.8210798501968384\n",
      "Epoch: 0, Step: 101120, Loss: 0.8862967491149902\n",
      "Epoch: 0, Step: 101152, Loss: 0.7928711771965027\n",
      "Epoch: 0, Step: 101184, Loss: 0.7505590319633484\n",
      "Epoch: 0, Step: 101216, Loss: 0.8091984391212463\n",
      "Epoch: 0, Step: 101248, Loss: 0.7405732274055481\n",
      "Epoch: 0, Step: 101280, Loss: 0.8500498533248901\n",
      "Epoch: 0, Step: 101312, Loss: 0.9684612154960632\n",
      "Epoch: 0, Step: 101344, Loss: 0.8099463582038879\n",
      "Epoch: 0, Step: 101376, Loss: 0.6549462080001831\n",
      "Epoch: 0, Step: 101408, Loss: 0.7925490140914917\n",
      "Epoch: 0, Step: 101440, Loss: 0.8806421160697937\n",
      "Epoch: 0, Step: 101472, Loss: 0.8928537964820862\n",
      "Epoch: 0, Step: 101504, Loss: 0.8580462336540222\n",
      "Epoch: 0, Step: 101536, Loss: 0.8539825081825256\n",
      "Epoch: 0, Step: 101568, Loss: 0.8163805603981018\n",
      "Epoch: 0, Step: 101600, Loss: 0.7531965970993042\n",
      "Epoch: 0, Step: 101632, Loss: 0.8852635622024536\n",
      "Epoch: 0, Step: 101664, Loss: 0.8299791216850281\n",
      "Epoch: 0, Step: 101696, Loss: 0.896568775177002\n",
      "Epoch: 0, Step: 101728, Loss: 0.8215157985687256\n",
      "Epoch: 0, Step: 101760, Loss: 0.7296050190925598\n",
      "Epoch: 0, Step: 101792, Loss: 0.8252185583114624\n",
      "Epoch: 0, Step: 101824, Loss: 0.9327262043952942\n",
      "Epoch: 0, Step: 101856, Loss: 0.6797087788581848\n",
      "Epoch: 0, Step: 101888, Loss: 0.7826352715492249\n",
      "Epoch: 0, Step: 101920, Loss: 0.8587549328804016\n",
      "Epoch: 0, Step: 101952, Loss: 0.8446966409683228\n",
      "Epoch: 0, Step: 101984, Loss: 0.8348958492279053\n",
      "Epoch: 0, Step: 102016, Loss: 0.8980711698532104\n",
      "Epoch: 0, Step: 102048, Loss: 0.8036158680915833\n",
      "Epoch: 0, Step: 102080, Loss: 0.8546121716499329\n",
      "Epoch: 0, Step: 102112, Loss: 0.8078870177268982\n",
      "Epoch: 0, Step: 102144, Loss: 0.9752403497695923\n",
      "Epoch: 0, Step: 102176, Loss: 0.8154581785202026\n",
      "Epoch: 0, Step: 102208, Loss: 0.7836244106292725\n",
      "Epoch: 0, Step: 102240, Loss: 0.8558467030525208\n",
      "Epoch: 0, Step: 102272, Loss: 0.7873449921607971\n",
      "Epoch: 0, Step: 102304, Loss: 0.8571995496749878\n",
      "Epoch: 0, Step: 102336, Loss: 0.77256178855896\n",
      "Epoch: 0, Step: 102368, Loss: 0.8652327656745911\n",
      "Epoch: 0, Step: 102400, Loss: 0.708448588848114\n",
      "Epoch: 0, Step: 102432, Loss: 0.8313296437263489\n",
      "Epoch: 0, Step: 102464, Loss: 0.799954354763031\n",
      "Epoch: 0, Step: 102496, Loss: 0.8990166187286377\n",
      "Epoch: 0, Step: 102528, Loss: 0.7722757458686829\n",
      "Epoch: 0, Step: 102560, Loss: 0.8547751307487488\n",
      "Epoch: 0, Step: 102592, Loss: 0.8555893301963806\n",
      "Epoch: 0, Step: 102624, Loss: 0.8051602244377136\n",
      "Epoch: 0, Step: 102656, Loss: 0.8411366939544678\n",
      "Epoch: 0, Step: 102688, Loss: 0.8583095073699951\n",
      "Epoch: 0, Step: 102720, Loss: 0.8203533887863159\n",
      "Epoch: 0, Step: 102752, Loss: 0.7896257638931274\n",
      "Epoch: 0, Step: 102784, Loss: 0.8227115273475647\n",
      "Epoch: 0, Step: 102816, Loss: 0.7737392783164978\n",
      "Epoch: 0, Step: 102848, Loss: 0.836472749710083\n",
      "Epoch: 0, Step: 102880, Loss: 0.8375941514968872\n",
      "Epoch: 0, Step: 102912, Loss: 0.8423113226890564\n",
      "Epoch: 0, Step: 102944, Loss: 0.8370665311813354\n",
      "Epoch: 0, Step: 102976, Loss: 0.7096120715141296\n",
      "Epoch: 0, Step: 103008, Loss: 0.7664401531219482\n",
      "Epoch: 0, Step: 103040, Loss: 0.8654395937919617\n",
      "Epoch: 0, Step: 103072, Loss: 0.8259757161140442\n",
      "Epoch: 0, Step: 103104, Loss: 0.8029186129570007\n",
      "Epoch: 0, Step: 103136, Loss: 0.828284502029419\n",
      "Epoch: 0, Step: 103168, Loss: 0.7698389887809753\n",
      "Epoch: 0, Step: 103200, Loss: 0.8715359568595886\n",
      "Epoch: 0, Step: 103232, Loss: 0.7224182486534119\n",
      "Epoch: 0, Step: 103264, Loss: 0.7982615232467651\n",
      "Epoch: 0, Step: 103296, Loss: 0.871965229511261\n",
      "Epoch: 0, Step: 103328, Loss: 0.8037811517715454\n",
      "Epoch: 0, Step: 103360, Loss: 0.7235791683197021\n",
      "Epoch: 0, Step: 103392, Loss: 0.7199051976203918\n",
      "Epoch: 0, Step: 103424, Loss: 0.8428904414176941\n",
      "Epoch: 0, Step: 103456, Loss: 0.7422586679458618\n",
      "Epoch: 0, Step: 103488, Loss: 0.8424726724624634\n",
      "Epoch: 0, Step: 103520, Loss: 0.8385642170906067\n",
      "Epoch: 0, Step: 103552, Loss: 0.8064345717430115\n",
      "Epoch: 0, Step: 103584, Loss: 0.8318383097648621\n",
      "Epoch: 0, Step: 103616, Loss: 0.8138128519058228\n",
      "Epoch: 0, Step: 103648, Loss: 0.8259207010269165\n",
      "Epoch: 0, Step: 103680, Loss: 0.7431337237358093\n",
      "Epoch: 0, Step: 103712, Loss: 0.9316278696060181\n",
      "Epoch: 0, Step: 103744, Loss: 0.8471740484237671\n",
      "Epoch: 0, Step: 103776, Loss: 0.7493741512298584\n",
      "Epoch: 0, Step: 103808, Loss: 0.8003610968589783\n",
      "Epoch: 0, Step: 103840, Loss: 0.8268810510635376\n",
      "Epoch: 0, Step: 103872, Loss: 0.7909062504768372\n",
      "Epoch: 0, Step: 103904, Loss: 0.8453119397163391\n",
      "Epoch: 0, Step: 103936, Loss: 0.7417289614677429\n",
      "Epoch: 0, Step: 103968, Loss: 0.9029086232185364\n",
      "Epoch: 0, Step: 104000, Loss: 0.8033180832862854\n",
      "Epoch: 0, Step: 104032, Loss: 0.8828232884407043\n",
      "Epoch: 0, Step: 104064, Loss: 0.7845346927642822\n",
      "Epoch: 0, Step: 104096, Loss: 0.7710627317428589\n",
      "Epoch: 0, Step: 104128, Loss: 0.9732110500335693\n",
      "Epoch: 0, Step: 104160, Loss: 0.8010217547416687\n",
      "Epoch: 0, Step: 104192, Loss: 0.7395092844963074\n",
      "Epoch: 0, Step: 104224, Loss: 0.8264495730400085\n",
      "Epoch: 0, Step: 104256, Loss: 0.8160586953163147\n",
      "Epoch: 0, Step: 104288, Loss: 0.7340619564056396\n",
      "Epoch: 0, Step: 104320, Loss: 0.797900915145874\n",
      "Epoch: 0, Step: 104352, Loss: 0.8195212483406067\n",
      "Epoch: 0, Step: 104384, Loss: 0.9236588478088379\n",
      "Epoch: 0, Step: 104416, Loss: 0.8672773838043213\n",
      "Epoch: 0, Step: 104448, Loss: 0.7736484408378601\n",
      "Epoch: 0, Step: 104480, Loss: 0.8707489967346191\n",
      "Epoch: 0, Step: 104512, Loss: 0.8771322965621948\n",
      "Epoch: 0, Step: 104544, Loss: 0.8210542798042297\n",
      "Epoch: 0, Step: 104576, Loss: 0.8093804121017456\n",
      "Epoch: 0, Step: 104608, Loss: 0.8935202360153198\n",
      "Epoch: 0, Step: 104640, Loss: 0.8055713176727295\n",
      "Epoch: 0, Step: 104672, Loss: 0.7385756969451904\n",
      "Epoch: 0, Step: 104704, Loss: 0.8291710615158081\n",
      "Epoch: 0, Step: 104736, Loss: 0.8168801665306091\n",
      "Epoch: 0, Step: 104768, Loss: 0.8160499930381775\n",
      "Epoch: 0, Step: 104800, Loss: 0.8998472094535828\n",
      "Epoch: 0, Step: 104832, Loss: 0.8106211423873901\n",
      "Epoch: 0, Step: 104864, Loss: 0.851239800453186\n",
      "Epoch: 0, Step: 104896, Loss: 0.7888925075531006\n",
      "Epoch: 0, Step: 104928, Loss: 0.9305241703987122\n",
      "Epoch: 0, Step: 104960, Loss: 0.896431028842926\n",
      "Epoch: 0, Step: 104992, Loss: 0.8559482097625732\n",
      "Epoch: 0, Step: 105024, Loss: 0.8949958086013794\n",
      "Epoch: 0, Step: 105056, Loss: 0.8126543164253235\n",
      "Epoch: 0, Step: 105088, Loss: 0.9134128093719482\n",
      "Epoch: 0, Step: 105120, Loss: 0.7861639261245728\n",
      "Epoch: 0, Step: 105152, Loss: 0.8161457777023315\n",
      "Epoch: 0, Step: 105184, Loss: 0.8076131343841553\n",
      "Epoch: 0, Step: 105216, Loss: 0.7950464487075806\n",
      "Epoch: 0, Step: 105248, Loss: 0.7907600998878479\n",
      "Epoch: 0, Step: 105280, Loss: 0.7865896821022034\n",
      "Epoch: 0, Step: 105312, Loss: 0.7916125655174255\n",
      "Epoch: 0, Step: 105344, Loss: 0.7649442553520203\n",
      "Epoch: 0, Step: 105376, Loss: 0.9473667740821838\n",
      "Epoch: 0, Step: 105408, Loss: 0.815423846244812\n",
      "Epoch: 0, Step: 105440, Loss: 0.830285906791687\n",
      "Epoch: 0, Step: 105472, Loss: 0.849633514881134\n",
      "Epoch: 0, Step: 105504, Loss: 0.8120323419570923\n",
      "Epoch: 0, Step: 105536, Loss: 0.8908647298812866\n",
      "Epoch: 0, Step: 105568, Loss: 0.7799980044364929\n",
      "Epoch: 0, Step: 105600, Loss: 0.8371990323066711\n",
      "Epoch: 0, Step: 105632, Loss: 0.8338054418563843\n",
      "Epoch: 0, Step: 105664, Loss: 0.7061495184898376\n",
      "Epoch: 0, Step: 105696, Loss: 0.8770735859870911\n",
      "Epoch: 0, Step: 105728, Loss: 0.8649581670761108\n",
      "Epoch: 0, Step: 105760, Loss: 0.8125429749488831\n",
      "Epoch: 0, Step: 105792, Loss: 0.78645920753479\n",
      "Epoch: 0, Step: 105824, Loss: 0.8097735643386841\n",
      "Epoch: 0, Step: 105856, Loss: 0.7825778126716614\n",
      "Epoch: 0, Step: 105888, Loss: 0.7671136260032654\n",
      "Epoch: 0, Step: 105920, Loss: 0.7856151461601257\n",
      "Epoch: 0, Step: 105952, Loss: 0.7931196093559265\n",
      "Epoch: 0, Step: 105984, Loss: 0.8382989764213562\n",
      "Epoch: 0, Step: 106016, Loss: 0.8196417093276978\n",
      "Epoch: 0, Step: 106048, Loss: 0.8239210844039917\n",
      "Epoch: 0, Step: 106080, Loss: 0.7261818647384644\n",
      "Epoch: 0, Step: 106112, Loss: 0.8060325384140015\n",
      "Epoch: 0, Step: 106144, Loss: 0.8348642587661743\n",
      "Epoch: 0, Step: 106176, Loss: 0.7902957797050476\n",
      "Epoch: 0, Step: 106208, Loss: 0.7334173917770386\n",
      "Epoch: 0, Step: 106240, Loss: 0.7421566247940063\n",
      "Epoch: 0, Step: 106272, Loss: 0.8328494429588318\n",
      "Epoch: 0, Step: 106304, Loss: 0.8059183359146118\n",
      "Epoch: 0, Step: 106336, Loss: 0.908511221408844\n",
      "Epoch: 0, Step: 106368, Loss: 0.6690093278884888\n",
      "Epoch: 0, Step: 106400, Loss: 0.7556325793266296\n",
      "Epoch: 0, Step: 106432, Loss: 0.8069794774055481\n",
      "Epoch: 0, Step: 106464, Loss: 0.8115411996841431\n",
      "Epoch: 0, Step: 106496, Loss: 0.7345397472381592\n",
      "Epoch: 0, Step: 106528, Loss: 0.8695648908615112\n",
      "Epoch: 0, Step: 106560, Loss: 0.8485310673713684\n",
      "Epoch: 0, Step: 106592, Loss: 0.9489288926124573\n",
      "Epoch: 0, Step: 106624, Loss: 0.773287832736969\n",
      "Epoch: 0, Step: 106656, Loss: 0.7550392746925354\n",
      "Epoch: 0, Step: 106688, Loss: 0.8482587337493896\n",
      "Epoch: 0, Step: 106720, Loss: 0.901100754737854\n",
      "Epoch: 0, Step: 106752, Loss: 0.9041464328765869\n",
      "Epoch: 0, Step: 106784, Loss: 0.9040279388427734\n",
      "Epoch: 0, Step: 106816, Loss: 0.8633672595024109\n",
      "Epoch: 0, Step: 106848, Loss: 0.7911693453788757\n",
      "Epoch: 0, Step: 106880, Loss: 0.7494460344314575\n",
      "Epoch: 0, Step: 106912, Loss: 0.7684198617935181\n",
      "Epoch: 0, Step: 106944, Loss: 0.8455429673194885\n",
      "Epoch: 0, Step: 106976, Loss: 0.8483045101165771\n",
      "Epoch: 0, Step: 107008, Loss: 0.9012901782989502\n",
      "Epoch: 0, Step: 107040, Loss: 0.8357359170913696\n",
      "Epoch: 0, Step: 107072, Loss: 0.7698627710342407\n",
      "Epoch: 0, Step: 107104, Loss: 0.8264506459236145\n",
      "Epoch: 0, Step: 107136, Loss: 0.9103230237960815\n",
      "Epoch: 0, Step: 107168, Loss: 0.8287084102630615\n",
      "Epoch: 0, Step: 107200, Loss: 0.8077990412712097\n",
      "Epoch: 0, Step: 107232, Loss: 0.8028398156166077\n",
      "Epoch: 0, Step: 107264, Loss: 0.8061614632606506\n",
      "Epoch: 0, Step: 107296, Loss: 0.7840226292610168\n",
      "Epoch: 0, Step: 107328, Loss: 0.8295690417289734\n",
      "Epoch: 0, Step: 107360, Loss: 0.7876656651496887\n",
      "Epoch: 0, Step: 107392, Loss: 0.7446273565292358\n",
      "Epoch: 0, Step: 107424, Loss: 0.8745628595352173\n",
      "Epoch: 0, Step: 107456, Loss: 0.8224232792854309\n",
      "Epoch: 0, Step: 107488, Loss: 0.7866404056549072\n",
      "Epoch: 0, Step: 107520, Loss: 0.8049058318138123\n",
      "Epoch: 0, Step: 107552, Loss: 0.7745025157928467\n",
      "Epoch: 0, Step: 107584, Loss: 0.9126887321472168\n",
      "Epoch: 0, Step: 107616, Loss: 0.9114738702774048\n",
      "Epoch: 0, Step: 107648, Loss: 0.8885093331336975\n",
      "Epoch: 0, Step: 107680, Loss: 0.909346342086792\n",
      "Epoch: 0, Step: 107712, Loss: 0.8638807535171509\n",
      "Epoch: 0, Step: 107744, Loss: 0.7447493672370911\n",
      "Epoch: 0, Step: 107776, Loss: 0.7513817548751831\n",
      "Epoch: 0, Step: 107808, Loss: 0.8973363041877747\n",
      "Epoch: 0, Step: 107840, Loss: 0.9016996622085571\n",
      "Epoch: 0, Step: 107872, Loss: 0.9874264001846313\n",
      "Epoch: 0, Step: 107904, Loss: 0.7995415329933167\n",
      "Epoch: 0, Step: 107936, Loss: 0.8047639727592468\n",
      "Epoch: 0, Step: 107968, Loss: 0.8394854068756104\n",
      "Epoch: 0, Step: 108000, Loss: 0.9124835133552551\n",
      "Epoch: 0, Step: 108032, Loss: 0.8717124462127686\n",
      "Epoch: 0, Step: 108064, Loss: 0.8006390333175659\n",
      "Epoch: 0, Step: 108096, Loss: 0.8551231622695923\n",
      "Epoch: 0, Step: 108128, Loss: 0.8555529117584229\n",
      "Epoch: 0, Step: 108160, Loss: 0.852712869644165\n",
      "Epoch: 0, Step: 108192, Loss: 0.8587071895599365\n",
      "Epoch: 0, Step: 108224, Loss: 0.8354174494743347\n",
      "Epoch: 0, Step: 108256, Loss: 0.7863988280296326\n",
      "Epoch: 0, Step: 108288, Loss: 0.8254810571670532\n",
      "Epoch: 0, Step: 108320, Loss: 0.8748068809509277\n",
      "Epoch: 0, Step: 108352, Loss: 0.8271273374557495\n",
      "Epoch: 0, Step: 108384, Loss: 0.8759679794311523\n",
      "Epoch: 0, Step: 108416, Loss: 0.8523523211479187\n",
      "Epoch: 0, Step: 108448, Loss: 0.8633214831352234\n",
      "Epoch: 0, Step: 108480, Loss: 0.8183988928794861\n",
      "Epoch: 0, Step: 108512, Loss: 0.8216205835342407\n",
      "Epoch: 0, Step: 108544, Loss: 0.7668797969818115\n",
      "Epoch: 0, Step: 108576, Loss: 0.7819033265113831\n",
      "Epoch: 0, Step: 108608, Loss: 0.8490245342254639\n",
      "Epoch: 0, Step: 108640, Loss: 0.9085530042648315\n",
      "Epoch: 0, Step: 108672, Loss: 0.7994917035102844\n",
      "Epoch: 0, Step: 108704, Loss: 0.8040161728858948\n",
      "Epoch: 0, Step: 108736, Loss: 0.8456347584724426\n",
      "Epoch: 0, Step: 108768, Loss: 0.8487009406089783\n",
      "Epoch: 0, Step: 108800, Loss: 0.9265280961990356\n",
      "Epoch: 0, Step: 108832, Loss: 0.6535784006118774\n",
      "Epoch: 0, Step: 108864, Loss: 0.7973896265029907\n",
      "Epoch: 0, Step: 108896, Loss: 0.9093222618103027\n",
      "Epoch: 0, Step: 108928, Loss: 0.8264981508255005\n",
      "Epoch: 0, Step: 108960, Loss: 0.8120830059051514\n",
      "Epoch: 0, Step: 108992, Loss: 0.7745312452316284\n",
      "Epoch: 0, Step: 109024, Loss: 0.8150975108146667\n",
      "Epoch: 0, Step: 109056, Loss: 0.8955184817314148\n",
      "Epoch: 0, Step: 109088, Loss: 0.8290186524391174\n",
      "Epoch: 0, Step: 109120, Loss: 0.8844100832939148\n",
      "Epoch: 0, Step: 109152, Loss: 0.7895461916923523\n",
      "Epoch: 0, Step: 109184, Loss: 0.7905403971672058\n",
      "Epoch: 0, Step: 109216, Loss: 0.776763379573822\n",
      "Epoch: 0, Step: 109248, Loss: 0.7767811417579651\n",
      "Epoch: 0, Step: 109280, Loss: 0.8102768659591675\n",
      "Epoch: 0, Step: 109312, Loss: 0.8297304511070251\n",
      "Epoch: 0, Step: 109344, Loss: 0.8377775549888611\n",
      "Epoch: 0, Step: 109376, Loss: 0.8072869181632996\n",
      "Epoch: 0, Step: 109408, Loss: 0.86136794090271\n",
      "Epoch: 0, Step: 109440, Loss: 0.8515523076057434\n",
      "Epoch: 0, Step: 109472, Loss: 0.8177493214607239\n",
      "Epoch: 0, Step: 109504, Loss: 0.8744484186172485\n",
      "Epoch: 0, Step: 109536, Loss: 0.7455896735191345\n",
      "Epoch: 0, Step: 109568, Loss: 0.7969505190849304\n",
      "Epoch: 0, Step: 109600, Loss: 0.9351912140846252\n",
      "Epoch: 0, Step: 109632, Loss: 0.8850410580635071\n",
      "Epoch: 0, Step: 109664, Loss: 0.7579376101493835\n",
      "Epoch: 0, Step: 109696, Loss: 0.6722559332847595\n",
      "Epoch: 0, Step: 109728, Loss: 0.8264533877372742\n",
      "Epoch: 0, Step: 109760, Loss: 0.8115289211273193\n",
      "Epoch: 0, Step: 109792, Loss: 0.8776142001152039\n",
      "Epoch: 0, Step: 109824, Loss: 0.8294607400894165\n",
      "Epoch: 0, Step: 109856, Loss: 0.8650890588760376\n",
      "Epoch: 0, Step: 109888, Loss: 0.9189193248748779\n",
      "Epoch: 0, Step: 109920, Loss: 0.7894864678382874\n",
      "Epoch: 0, Step: 109952, Loss: 0.9042396545410156\n",
      "Epoch: 0, Step: 109984, Loss: 0.8044678568840027\n",
      "Epoch: 0, Step: 110016, Loss: 1.030347466468811\n",
      "Epoch: 0, Step: 110048, Loss: 0.8142651319503784\n",
      "Epoch: 0, Step: 110080, Loss: 0.831009566783905\n",
      "Epoch: 0, Step: 110112, Loss: 0.8440921306610107\n",
      "Epoch: 0, Step: 110144, Loss: 0.8150742053985596\n",
      "Epoch: 0, Step: 110176, Loss: 0.8331397175788879\n",
      "Epoch: 0, Step: 110208, Loss: 0.8752976059913635\n",
      "Epoch: 0, Step: 110240, Loss: 0.8175607919692993\n",
      "Epoch: 0, Step: 110272, Loss: 0.7965665459632874\n",
      "Epoch: 0, Step: 110304, Loss: 0.778659999370575\n",
      "Epoch: 0, Step: 110336, Loss: 0.8042955994606018\n",
      "Epoch: 0, Step: 110368, Loss: 0.8371595740318298\n",
      "Epoch: 0, Step: 110400, Loss: 0.72486811876297\n",
      "Epoch: 0, Step: 110432, Loss: 0.7582760453224182\n",
      "Epoch: 0, Step: 110464, Loss: 0.7985357046127319\n",
      "Epoch: 0, Step: 110496, Loss: 0.714402973651886\n",
      "Epoch: 0, Step: 110528, Loss: 0.8850150108337402\n",
      "Epoch: 0, Step: 110560, Loss: 0.6932401061058044\n",
      "Epoch: 0, Step: 110592, Loss: 0.8326098918914795\n",
      "Epoch: 0, Step: 110624, Loss: 0.825356125831604\n",
      "Epoch: 0, Step: 110656, Loss: 0.8818895816802979\n",
      "Epoch: 0, Step: 110688, Loss: 0.7447317838668823\n",
      "Epoch: 0, Step: 110720, Loss: 0.7990421652793884\n",
      "Epoch: 0, Step: 110752, Loss: 0.7745290994644165\n",
      "Epoch: 0, Step: 110784, Loss: 0.7713891863822937\n",
      "Epoch: 0, Step: 110816, Loss: 0.7766129374504089\n",
      "Epoch: 0, Step: 110848, Loss: 0.8783286213874817\n",
      "Epoch: 0, Step: 110880, Loss: 0.7929832339286804\n",
      "Epoch: 0, Step: 110912, Loss: 0.8422101736068726\n",
      "Epoch: 0, Step: 110944, Loss: 0.8281005024909973\n",
      "Epoch: 0, Step: 110976, Loss: 0.8041632175445557\n",
      "Epoch: 0, Step: 111008, Loss: 0.8176481127738953\n",
      "Epoch: 0, Step: 111040, Loss: 0.8419309854507446\n",
      "Epoch: 0, Step: 111072, Loss: 0.7789366245269775\n",
      "Epoch: 0, Step: 111104, Loss: 0.843076765537262\n",
      "Epoch: 0, Step: 111136, Loss: 0.7495632767677307\n",
      "Epoch: 0, Step: 111168, Loss: 0.7259042859077454\n",
      "Epoch: 0, Step: 111200, Loss: 0.8192989230155945\n",
      "Epoch: 0, Step: 111232, Loss: 0.8421670794487\n",
      "Epoch: 0, Step: 111264, Loss: 0.8116130232810974\n",
      "Epoch: 0, Step: 111296, Loss: 0.8349555134773254\n",
      "Epoch: 0, Step: 111328, Loss: 0.8606716394424438\n",
      "Epoch: 0, Step: 111360, Loss: 0.8228520154953003\n",
      "Epoch: 0, Step: 111392, Loss: 0.781695544719696\n",
      "Epoch: 0, Step: 111424, Loss: 0.8555682897567749\n",
      "Epoch: 0, Step: 111456, Loss: 0.6922966837882996\n",
      "Epoch: 0, Step: 111488, Loss: 0.9177674055099487\n",
      "Epoch: 0, Step: 111520, Loss: 0.8219310641288757\n",
      "Epoch: 0, Step: 111552, Loss: 0.9331439137458801\n",
      "Epoch: 0, Step: 111584, Loss: 0.7967439293861389\n",
      "Epoch: 0, Step: 111616, Loss: 0.7957035303115845\n",
      "Epoch: 0, Step: 111648, Loss: 0.7673957943916321\n",
      "Epoch: 0, Step: 111680, Loss: 0.8319531083106995\n",
      "Epoch: 0, Step: 111712, Loss: 0.8557344079017639\n",
      "Epoch: 0, Step: 111744, Loss: 0.8512106537818909\n",
      "Epoch: 0, Step: 111776, Loss: 0.8171361088752747\n",
      "Epoch: 0, Step: 111808, Loss: 0.919768214225769\n",
      "Epoch: 0, Step: 111840, Loss: 0.8489390015602112\n",
      "Epoch: 0, Step: 111872, Loss: 0.8173834681510925\n",
      "Epoch: 0, Step: 111904, Loss: 0.8786861300468445\n",
      "Epoch: 0, Step: 111936, Loss: 0.878088116645813\n",
      "Epoch: 0, Step: 111968, Loss: 0.8437601923942566\n",
      "Epoch: 0, Step: 112000, Loss: 0.8110243678092957\n",
      "Epoch: 0, Step: 112032, Loss: 0.8651262521743774\n",
      "Epoch: 0, Step: 112064, Loss: 0.8523111939430237\n",
      "Epoch: 0, Step: 112096, Loss: 1.0333441495895386\n",
      "Epoch: 0, Step: 112128, Loss: 0.7282825112342834\n",
      "Epoch: 0, Step: 112160, Loss: 0.7359254956245422\n",
      "Epoch: 0, Step: 112192, Loss: 0.8399543166160583\n",
      "Epoch: 0, Step: 112224, Loss: 0.7470619082450867\n",
      "Epoch: 0, Step: 112256, Loss: 0.7903340458869934\n",
      "Epoch: 0, Step: 112288, Loss: 0.8099685907363892\n",
      "Epoch: 0, Step: 112320, Loss: 0.7696115374565125\n",
      "Epoch: 0, Step: 112352, Loss: 0.8340438008308411\n",
      "Epoch: 0, Step: 112384, Loss: 0.7585598230361938\n",
      "Epoch: 0, Step: 112416, Loss: 0.8460622429847717\n",
      "Epoch: 0, Step: 112448, Loss: 0.901254415512085\n",
      "Epoch: 0, Step: 112480, Loss: 0.8422349095344543\n",
      "Epoch: 0, Step: 112512, Loss: 0.8216712474822998\n",
      "Epoch: 0, Step: 112544, Loss: 0.8162094950675964\n",
      "Epoch: 0, Step: 112576, Loss: 0.8006320595741272\n",
      "Epoch: 0, Step: 112608, Loss: 0.8403995037078857\n",
      "Epoch: 0, Step: 112640, Loss: 0.8178781270980835\n",
      "Epoch: 0, Step: 112672, Loss: 0.9277250170707703\n",
      "Epoch: 0, Step: 112704, Loss: 0.7067264914512634\n",
      "Epoch: 0, Step: 112736, Loss: 0.898232638835907\n",
      "Epoch: 0, Step: 112768, Loss: 0.8327170014381409\n",
      "Epoch: 0, Step: 112800, Loss: 0.789605438709259\n",
      "Epoch: 0, Step: 112832, Loss: 0.9050247073173523\n",
      "Epoch: 0, Step: 112864, Loss: 0.8592413663864136\n",
      "Epoch: 0, Step: 112896, Loss: 0.8699074387550354\n",
      "Epoch: 0, Step: 112928, Loss: 0.7764803171157837\n",
      "Epoch: 0, Step: 112960, Loss: 0.8399571776390076\n",
      "Epoch: 0, Step: 112992, Loss: 0.8184906244277954\n",
      "Epoch: 0, Step: 113024, Loss: 0.8644154667854309\n",
      "Epoch: 0, Step: 113056, Loss: 0.8465661406517029\n",
      "Epoch: 0, Step: 113088, Loss: 0.8561671376228333\n",
      "Epoch: 0, Step: 113120, Loss: 0.7722240090370178\n",
      "Epoch: 0, Step: 113152, Loss: 0.8589169383049011\n",
      "Epoch: 0, Step: 113184, Loss: 0.9187347888946533\n",
      "Epoch: 0, Step: 113216, Loss: 0.7891809344291687\n",
      "Epoch: 0, Step: 113248, Loss: 0.7232593894004822\n",
      "Epoch: 0, Step: 113280, Loss: 0.89109867811203\n",
      "Epoch: 0, Step: 113312, Loss: 0.8297259211540222\n",
      "Epoch: 0, Step: 113344, Loss: 0.7773138284683228\n",
      "Epoch: 0, Step: 113376, Loss: 0.7718085050582886\n",
      "Epoch: 0, Step: 113408, Loss: 0.853051483631134\n",
      "Epoch: 0, Step: 113440, Loss: 0.8264754414558411\n",
      "Epoch: 0, Step: 113472, Loss: 0.797415554523468\n",
      "Epoch: 0, Step: 113504, Loss: 0.8329814672470093\n",
      "Epoch: 0, Step: 113536, Loss: 0.8897362351417542\n",
      "Epoch: 0, Step: 113568, Loss: 0.8608772158622742\n",
      "Epoch: 0, Step: 113600, Loss: 0.8778262138366699\n",
      "Epoch: 0, Step: 113632, Loss: 0.8338416218757629\n",
      "Epoch: 0, Step: 113664, Loss: 0.8605133295059204\n",
      "Epoch: 0, Step: 113696, Loss: 0.7740567326545715\n",
      "Epoch: 0, Step: 113728, Loss: 0.8738710284233093\n",
      "Epoch: 0, Step: 113760, Loss: 0.8694138526916504\n",
      "Epoch: 0, Step: 113792, Loss: 0.7953466773033142\n",
      "Epoch: 0, Step: 113824, Loss: 0.8126135468482971\n",
      "Epoch: 0, Step: 113856, Loss: 0.8568565249443054\n",
      "Epoch: 0, Step: 113888, Loss: 0.8519691824913025\n",
      "Epoch: 0, Step: 113920, Loss: 0.8282791376113892\n",
      "Epoch: 0, Step: 113952, Loss: 0.9475245475769043\n",
      "Epoch: 0, Step: 113984, Loss: 0.7774203419685364\n",
      "Epoch: 0, Step: 114016, Loss: 0.7259781956672668\n",
      "Epoch: 0, Step: 114048, Loss: 0.8225295543670654\n",
      "Epoch: 0, Step: 114080, Loss: 0.8619958162307739\n",
      "Epoch: 0, Step: 114112, Loss: 0.8012109398841858\n",
      "Epoch: 0, Step: 114144, Loss: 0.8587716221809387\n",
      "Epoch: 0, Step: 114176, Loss: 0.7957637906074524\n",
      "Epoch: 0, Step: 114208, Loss: 0.8030669093132019\n",
      "Epoch: 0, Step: 114240, Loss: 0.7493122220039368\n",
      "Epoch: 0, Step: 114272, Loss: 0.7958747744560242\n",
      "Epoch: 0, Step: 114304, Loss: 0.7251230478286743\n",
      "Epoch: 0, Step: 114336, Loss: 0.8173078894615173\n",
      "Epoch: 0, Step: 114368, Loss: 0.7543221116065979\n",
      "Epoch: 0, Step: 114400, Loss: 0.798753559589386\n",
      "Epoch: 0, Step: 114432, Loss: 0.6891615986824036\n",
      "Epoch: 0, Step: 114464, Loss: 0.8657032251358032\n",
      "Epoch: 0, Step: 114496, Loss: 0.7991378307342529\n",
      "Epoch: 0, Step: 114528, Loss: 0.7796900868415833\n",
      "Epoch: 0, Step: 114560, Loss: 0.7445011734962463\n",
      "Epoch: 0, Step: 114592, Loss: 0.7979670166969299\n",
      "Epoch: 0, Step: 114624, Loss: 0.7942762970924377\n",
      "Epoch: 0, Step: 114656, Loss: 0.7285573482513428\n",
      "Epoch: 0, Step: 114688, Loss: 0.7516952753067017\n",
      "Epoch: 0, Step: 114720, Loss: 0.8485475778579712\n",
      "Epoch: 0, Step: 114752, Loss: 0.7400902509689331\n",
      "Epoch: 0, Step: 114784, Loss: 0.8036436438560486\n",
      "Epoch: 0, Step: 114816, Loss: 0.8623393177986145\n",
      "Epoch: 0, Step: 114848, Loss: 0.8173538446426392\n",
      "Epoch: 0, Step: 114880, Loss: 0.8613269329071045\n",
      "Epoch: 0, Step: 114912, Loss: 0.8601961731910706\n",
      "Epoch: 0, Step: 114944, Loss: 0.7123984098434448\n",
      "Epoch: 0, Step: 114976, Loss: 0.8079115152359009\n",
      "Epoch: 0, Step: 115008, Loss: 0.8718875050544739\n",
      "Epoch: 0, Step: 115040, Loss: 0.7019485831260681\n",
      "Epoch: 0, Step: 115072, Loss: 0.7857992053031921\n",
      "Epoch: 0, Step: 115104, Loss: 0.9009142518043518\n",
      "Epoch: 0, Step: 115136, Loss: 0.8968589901924133\n",
      "Epoch: 0, Step: 115168, Loss: 0.7738988399505615\n",
      "Epoch: 0, Step: 115200, Loss: 0.7857615947723389\n",
      "Epoch: 0, Step: 115232, Loss: 0.8778459429740906\n",
      "Epoch: 0, Step: 115264, Loss: 0.8757268786430359\n",
      "Epoch: 0, Step: 115296, Loss: 0.8643637299537659\n",
      "Epoch: 0, Step: 115328, Loss: 0.8813101053237915\n",
      "Epoch: 0, Step: 115360, Loss: 0.8202146887779236\n",
      "Epoch: 0, Step: 115392, Loss: 0.8390573859214783\n",
      "Epoch: 0, Step: 115424, Loss: 0.8531221747398376\n",
      "Epoch: 0, Step: 115456, Loss: 0.7076122164726257\n",
      "Epoch: 0, Step: 115488, Loss: 0.8818562030792236\n",
      "Epoch: 0, Step: 115520, Loss: 0.8201287388801575\n",
      "Epoch: 0, Step: 115552, Loss: 0.7927034497261047\n",
      "Epoch: 0, Step: 115584, Loss: 0.7930262684822083\n",
      "Epoch: 0, Step: 115616, Loss: 0.7195686101913452\n",
      "Epoch: 0, Step: 115648, Loss: 0.7548219561576843\n",
      "Epoch: 0, Step: 115680, Loss: 0.830551028251648\n",
      "Epoch: 0, Step: 115712, Loss: 0.7823346853256226\n",
      "Epoch: 0, Step: 115744, Loss: 0.7922351956367493\n",
      "Epoch: 0, Step: 115776, Loss: 0.8270713686943054\n",
      "Epoch: 0, Step: 115808, Loss: 0.7712557911872864\n",
      "Epoch: 0, Step: 115840, Loss: 0.8050302267074585\n",
      "Epoch: 0, Step: 115872, Loss: 0.8388305306434631\n",
      "Epoch: 0, Step: 115904, Loss: 0.8783167600631714\n",
      "Epoch: 0, Step: 115936, Loss: 0.7777793407440186\n",
      "Epoch: 0, Step: 115968, Loss: 0.9074532389640808\n",
      "Epoch: 0, Step: 116000, Loss: 0.7691407799720764\n",
      "Epoch: 0, Step: 116032, Loss: 0.7748530507087708\n",
      "Epoch: 0, Step: 116064, Loss: 0.7539974451065063\n",
      "Epoch: 0, Step: 116096, Loss: 0.8048473000526428\n",
      "Epoch: 0, Step: 116128, Loss: 0.8205865621566772\n",
      "Epoch: 0, Step: 116160, Loss: 0.8432418704032898\n",
      "Epoch: 0, Step: 116192, Loss: 0.8350750207901001\n",
      "Epoch: 0, Step: 116224, Loss: 0.7411622405052185\n",
      "Epoch: 0, Step: 116256, Loss: 0.8690489530563354\n",
      "Epoch: 0, Step: 116288, Loss: 0.8385692834854126\n",
      "Epoch: 0, Step: 116320, Loss: 0.7769518494606018\n",
      "Epoch: 0, Step: 116352, Loss: 0.7392391562461853\n",
      "Epoch: 0, Step: 116384, Loss: 0.8774272799491882\n",
      "Epoch: 0, Step: 116416, Loss: 0.7869507074356079\n",
      "Epoch: 0, Step: 116448, Loss: 0.8000737428665161\n",
      "Epoch: 0, Step: 116480, Loss: 0.9020206332206726\n",
      "Epoch: 0, Step: 116512, Loss: 0.8153035640716553\n",
      "Epoch: 0, Step: 116544, Loss: 0.7912551164627075\n",
      "Epoch: 0, Step: 116576, Loss: 0.9031875729560852\n",
      "Epoch: 0, Step: 116608, Loss: 0.7304807305335999\n",
      "Epoch: 0, Step: 116640, Loss: 0.8211701512336731\n",
      "Epoch: 0, Step: 116672, Loss: 0.8846710920333862\n",
      "Epoch: 0, Step: 116704, Loss: 0.70936119556427\n",
      "Epoch: 0, Step: 116736, Loss: 0.9683809280395508\n",
      "Epoch: 0, Step: 116768, Loss: 0.8172380328178406\n",
      "Epoch: 0, Step: 116800, Loss: 0.8134201169013977\n",
      "Epoch: 0, Step: 116832, Loss: 0.9055247902870178\n",
      "Epoch: 0, Step: 116864, Loss: 0.8599698543548584\n",
      "Epoch: 0, Step: 116896, Loss: 0.8867586851119995\n",
      "Epoch: 0, Step: 116928, Loss: 0.8099768161773682\n",
      "Epoch: 0, Step: 116960, Loss: 0.9567747712135315\n",
      "Epoch: 0, Step: 116992, Loss: 0.7983260750770569\n",
      "Epoch: 0, Step: 117024, Loss: 0.8138123154640198\n",
      "Epoch: 0, Step: 117056, Loss: 0.7454792261123657\n",
      "Epoch: 0, Step: 117088, Loss: 0.7870610356330872\n",
      "Epoch: 0, Step: 117120, Loss: 0.7549023032188416\n",
      "Epoch: 0, Step: 117152, Loss: 0.8095861673355103\n",
      "Epoch: 0, Step: 117184, Loss: 0.9579792022705078\n",
      "Epoch: 0, Step: 117216, Loss: 0.7541970610618591\n",
      "Epoch: 0, Step: 117248, Loss: 0.8136962056159973\n",
      "Epoch: 0, Step: 117280, Loss: 0.7962886691093445\n",
      "Epoch: 0, Step: 117312, Loss: 0.8424552083015442\n",
      "Epoch: 0, Step: 117344, Loss: 0.8706662654876709\n",
      "Epoch: 0, Step: 117376, Loss: 0.8291446566581726\n",
      "Epoch: 0, Step: 117408, Loss: 0.6718486547470093\n",
      "Epoch: 0, Step: 117440, Loss: 0.8821375966072083\n",
      "Epoch: 0, Step: 117472, Loss: 0.9284360408782959\n",
      "Epoch: 0, Step: 117504, Loss: 0.8554336428642273\n",
      "Epoch: 0, Step: 117536, Loss: 0.858927309513092\n",
      "Epoch: 0, Step: 117568, Loss: 0.7382872700691223\n",
      "Epoch: 0, Step: 117600, Loss: 0.831159234046936\n",
      "Epoch: 0, Step: 117632, Loss: 0.844877302646637\n",
      "Epoch: 0, Step: 117664, Loss: 0.7699970602989197\n",
      "Epoch: 0, Step: 117696, Loss: 0.8068923354148865\n",
      "Epoch: 0, Step: 117728, Loss: 0.8091107606887817\n",
      "Epoch: 0, Step: 117760, Loss: 0.9050429463386536\n",
      "Epoch: 0, Step: 117792, Loss: 0.8741560578346252\n",
      "Epoch: 0, Step: 117824, Loss: 0.7299429178237915\n",
      "Epoch: 0, Step: 117856, Loss: 0.9097164869308472\n",
      "Epoch: 0, Step: 117888, Loss: 0.8478583097457886\n",
      "Epoch: 0, Step: 117920, Loss: 0.8235247731208801\n",
      "Epoch: 0, Step: 117952, Loss: 0.8521299958229065\n",
      "Epoch: 0, Step: 117984, Loss: 0.8601975440979004\n",
      "Epoch: 0, Step: 118016, Loss: 0.8921224474906921\n",
      "Epoch: 0, Step: 118048, Loss: 0.8033038973808289\n",
      "Epoch: 0, Step: 118080, Loss: 0.7103961706161499\n",
      "Epoch: 0, Step: 118112, Loss: 0.8217607736587524\n",
      "Epoch: 0, Step: 118144, Loss: 0.7561830878257751\n",
      "Epoch: 0, Step: 118176, Loss: 0.8841891288757324\n",
      "Epoch: 0, Step: 118208, Loss: 0.7923985123634338\n",
      "Epoch: 0, Step: 118240, Loss: 0.8062310814857483\n",
      "Epoch: 0, Step: 118272, Loss: 0.7592905759811401\n",
      "Epoch: 0, Step: 118304, Loss: 0.8182436227798462\n",
      "Epoch: 0, Step: 118336, Loss: 0.8204618692398071\n",
      "Epoch: 0, Step: 118368, Loss: 0.8366937041282654\n",
      "Epoch: 0, Step: 118400, Loss: 0.7212568521499634\n",
      "Epoch: 0, Step: 118432, Loss: 0.7447250485420227\n",
      "Epoch: 0, Step: 118464, Loss: 0.7677409648895264\n",
      "Epoch: 0, Step: 118496, Loss: 0.7831225991249084\n",
      "Epoch: 0, Step: 118528, Loss: 0.7983300685882568\n",
      "Epoch: 0, Step: 118560, Loss: 0.7298552989959717\n",
      "Epoch: 0, Step: 118592, Loss: 0.801937460899353\n",
      "Epoch: 0, Step: 118624, Loss: 0.8772969245910645\n",
      "Epoch: 0, Step: 118656, Loss: 0.8021303415298462\n",
      "Epoch: 0, Step: 118688, Loss: 0.8651220202445984\n",
      "Epoch: 0, Step: 118720, Loss: 0.891106367111206\n",
      "Epoch: 0, Step: 118752, Loss: 0.8928266763687134\n",
      "Epoch: 0, Step: 118784, Loss: 0.7420917749404907\n",
      "Epoch: 0, Step: 118816, Loss: 0.8785226941108704\n",
      "Epoch: 0, Step: 118848, Loss: 0.7823641896247864\n",
      "Epoch: 0, Step: 118880, Loss: 0.7749079465866089\n",
      "Epoch: 0, Step: 118912, Loss: 0.7163756489753723\n",
      "Epoch: 0, Step: 118944, Loss: 0.8226407170295715\n",
      "Epoch: 0, Step: 118976, Loss: 0.7610536217689514\n",
      "Epoch: 0, Step: 119008, Loss: 0.8306085467338562\n",
      "Epoch: 0, Step: 119040, Loss: 0.8286854028701782\n",
      "Epoch: 0, Step: 119072, Loss: 0.9043638110160828\n",
      "Epoch: 0, Step: 119104, Loss: 0.7937480807304382\n",
      "Epoch: 0, Step: 119136, Loss: 0.7469547986984253\n",
      "Epoch: 0, Step: 119168, Loss: 0.7450546622276306\n",
      "Epoch: 0, Step: 119200, Loss: 0.7373095154762268\n",
      "Epoch: 0, Step: 119232, Loss: 0.8376748561859131\n",
      "Epoch: 0, Step: 119264, Loss: 0.8477522134780884\n",
      "Epoch: 0, Step: 119296, Loss: 0.9136031270027161\n",
      "Epoch: 0, Step: 119328, Loss: 0.7501953840255737\n",
      "Epoch: 0, Step: 119360, Loss: 0.8703306913375854\n",
      "Epoch: 0, Step: 119392, Loss: 0.7415323853492737\n",
      "Epoch: 0, Step: 119424, Loss: 0.8751922249794006\n",
      "Epoch: 0, Step: 119456, Loss: 0.9127888679504395\n",
      "Epoch: 0, Step: 119488, Loss: 0.8231340646743774\n",
      "Epoch: 0, Step: 119520, Loss: 0.778959333896637\n",
      "Epoch: 0, Step: 119552, Loss: 0.752606213092804\n",
      "Epoch: 0, Step: 119584, Loss: 0.8939325213432312\n",
      "Epoch: 0, Step: 119616, Loss: 0.9063625335693359\n",
      "Epoch: 0, Step: 119648, Loss: 0.8169878721237183\n",
      "Epoch: 0, Step: 119680, Loss: 0.915244460105896\n",
      "Epoch: 0, Step: 119712, Loss: 0.9090001583099365\n",
      "Epoch: 0, Step: 119744, Loss: 0.8346716165542603\n",
      "Epoch: 0, Step: 119776, Loss: 0.9663087129592896\n",
      "Epoch: 0, Step: 119808, Loss: 0.7882807850837708\n",
      "Epoch: 0, Step: 119840, Loss: 0.8517085909843445\n",
      "Epoch: 0, Step: 119872, Loss: 0.7036489844322205\n",
      "Epoch: 0, Step: 119904, Loss: 0.8395439982414246\n",
      "Epoch: 0, Step: 119936, Loss: 0.912367582321167\n",
      "Epoch: 0, Step: 119968, Loss: 0.8805835247039795\n",
      "Epoch: 0, Step: 120000, Loss: 0.8240919709205627\n",
      "Epoch: 0, Step: 120032, Loss: 0.88255774974823\n",
      "Epoch: 0, Step: 120064, Loss: 0.9386706352233887\n",
      "Epoch: 0, Step: 120096, Loss: 0.9038615226745605\n",
      "Epoch: 0, Step: 120128, Loss: 0.826887845993042\n",
      "Epoch: 0, Step: 120160, Loss: 0.768105685710907\n",
      "Epoch: 0, Step: 120192, Loss: 0.7821524739265442\n",
      "Epoch: 0, Step: 120224, Loss: 0.8155604004859924\n",
      "Epoch: 0, Step: 120256, Loss: 0.8177658915519714\n",
      "Epoch: 0, Step: 120288, Loss: 0.8866917490959167\n",
      "Epoch: 0, Step: 120320, Loss: 0.878362774848938\n",
      "Epoch: 0, Step: 120352, Loss: 0.8704060316085815\n",
      "Epoch: 0, Step: 120384, Loss: 0.8383294939994812\n",
      "Epoch: 0, Step: 120416, Loss: 0.8774129748344421\n",
      "Epoch: 0, Step: 120448, Loss: 0.7768025994300842\n",
      "Epoch: 0, Step: 120480, Loss: 0.8153192400932312\n",
      "Epoch: 0, Step: 120512, Loss: 0.8869066834449768\n",
      "Epoch: 0, Step: 120544, Loss: 0.7948524355888367\n",
      "Epoch: 0, Step: 120576, Loss: 0.8146643042564392\n",
      "Epoch: 0, Step: 120608, Loss: 0.8181730508804321\n",
      "Epoch: 0, Step: 120640, Loss: 0.852073609828949\n",
      "Epoch: 0, Step: 120672, Loss: 0.7484812140464783\n",
      "Epoch: 0, Step: 120704, Loss: 0.8511620163917542\n",
      "Epoch: 0, Step: 120736, Loss: 0.833098828792572\n",
      "Epoch: 0, Step: 120768, Loss: 0.8384988307952881\n",
      "Epoch: 0, Step: 120800, Loss: 0.8102989792823792\n",
      "Epoch: 0, Step: 120832, Loss: 0.8837437629699707\n",
      "Epoch: 0, Step: 120864, Loss: 0.8292580246925354\n",
      "Epoch: 0, Step: 120896, Loss: 0.8095853328704834\n",
      "Epoch: 0, Step: 120928, Loss: 0.7699034810066223\n",
      "Epoch: 0, Step: 120960, Loss: 0.7628979086875916\n",
      "Epoch: 0, Step: 120992, Loss: 0.8703057169914246\n",
      "Epoch: 0, Step: 121024, Loss: 0.8927403688430786\n",
      "Epoch: 0, Step: 121056, Loss: 0.8614824414253235\n",
      "Epoch: 0, Step: 121088, Loss: 0.8450108170509338\n",
      "Epoch: 0, Step: 121120, Loss: 0.8899032473564148\n",
      "Epoch: 0, Step: 121152, Loss: 0.7870992422103882\n",
      "Epoch: 0, Step: 121184, Loss: 0.9013369679450989\n",
      "Epoch: 0, Step: 121216, Loss: 0.838467001914978\n",
      "Epoch: 0, Step: 121248, Loss: 0.9023971557617188\n",
      "Epoch: 0, Step: 121280, Loss: 0.8560343384742737\n",
      "Epoch: 0, Step: 121312, Loss: 0.7845179438591003\n",
      "Epoch: 0, Step: 121344, Loss: 0.9312475323677063\n",
      "Epoch: 0, Step: 121376, Loss: 0.8870952129364014\n",
      "Epoch: 0, Step: 121408, Loss: 0.7221545577049255\n",
      "Epoch: 0, Step: 121440, Loss: 0.7918758988380432\n",
      "Epoch: 0, Step: 121472, Loss: 0.7887386679649353\n",
      "Epoch: 0, Step: 121504, Loss: 0.8933699727058411\n",
      "Epoch: 0, Step: 121536, Loss: 0.8528878688812256\n",
      "Epoch: 0, Step: 121568, Loss: 0.8600594401359558\n",
      "Epoch: 0, Step: 121600, Loss: 0.7628640532493591\n",
      "Epoch: 0, Step: 121632, Loss: 0.8117058873176575\n",
      "Epoch: 0, Step: 121664, Loss: 0.8449081182479858\n",
      "Epoch: 0, Step: 121696, Loss: 0.8836925029754639\n",
      "Epoch: 0, Step: 121728, Loss: 0.770138144493103\n",
      "Epoch: 0, Step: 121760, Loss: 0.7908565402030945\n",
      "Epoch: 0, Step: 121792, Loss: 0.8088274598121643\n",
      "Epoch: 0, Step: 121824, Loss: 0.7924615740776062\n",
      "Epoch: 0, Step: 121856, Loss: 0.8170549273490906\n",
      "Epoch: 0, Step: 121888, Loss: 0.894221305847168\n",
      "Epoch: 0, Step: 121920, Loss: 0.8874571919441223\n",
      "Epoch: 0, Step: 121952, Loss: 0.7225054502487183\n",
      "Epoch: 0, Step: 121984, Loss: 0.8156751990318298\n",
      "Epoch: 0, Step: 122016, Loss: 0.8686507344245911\n",
      "Epoch: 0, Step: 122048, Loss: 0.7351644039154053\n",
      "Epoch: 0, Step: 122080, Loss: 0.9018518924713135\n",
      "Epoch: 0, Step: 122112, Loss: 0.7650821208953857\n",
      "Epoch: 0, Step: 122144, Loss: 0.8439604043960571\n",
      "Epoch: 0, Step: 122176, Loss: 0.7791049480438232\n",
      "Epoch: 0, Step: 122208, Loss: 0.8416447043418884\n",
      "Epoch: 0, Step: 122240, Loss: 0.803887665271759\n",
      "Epoch: 0, Step: 122272, Loss: 0.7995002269744873\n",
      "Epoch: 0, Step: 122304, Loss: 0.7734624147415161\n",
      "Epoch: 0, Step: 122336, Loss: 0.8523628115653992\n",
      "Epoch: 0, Step: 122368, Loss: 0.8022387623786926\n",
      "Epoch: 0, Step: 122400, Loss: 0.8665663599967957\n",
      "Epoch: 0, Step: 122432, Loss: 0.8190888166427612\n",
      "Epoch: 0, Step: 122464, Loss: 0.8858487010002136\n",
      "Epoch: 0, Step: 122496, Loss: 0.782171368598938\n",
      "Epoch: 0, Step: 122528, Loss: 0.8638440370559692\n",
      "Epoch: 0, Step: 122560, Loss: 0.8563498258590698\n",
      "Epoch: 0, Step: 122592, Loss: 0.7701127529144287\n",
      "Epoch: 0, Step: 122624, Loss: 0.844219982624054\n",
      "Epoch: 0, Step: 122656, Loss: 0.8593164682388306\n",
      "Epoch: 0, Step: 122688, Loss: 0.7978284358978271\n",
      "Epoch: 0, Step: 122720, Loss: 0.7727798223495483\n",
      "Epoch: 0, Step: 122752, Loss: 0.8090841174125671\n",
      "Epoch: 0, Step: 122784, Loss: 0.838219404220581\n",
      "Epoch: 0, Step: 122816, Loss: 0.8978983163833618\n",
      "Epoch: 0, Step: 122848, Loss: 0.9495574831962585\n",
      "Epoch: 0, Step: 122880, Loss: 0.8705688714981079\n",
      "Epoch: 0, Step: 122912, Loss: 0.855812132358551\n",
      "Epoch: 0, Step: 122944, Loss: 0.8391920924186707\n",
      "Epoch: 0, Step: 122976, Loss: 0.9092388153076172\n",
      "Epoch: 0, Step: 123008, Loss: 0.9364215135574341\n",
      "Epoch: 0, Step: 123040, Loss: 0.7759369611740112\n",
      "Epoch: 0, Step: 123072, Loss: 0.8800326585769653\n",
      "Epoch: 0, Step: 123104, Loss: 0.8595125675201416\n",
      "Epoch: 0, Step: 123136, Loss: 0.7289921641349792\n",
      "Epoch: 0, Step: 123168, Loss: 0.78172767162323\n",
      "Epoch: 0, Step: 123200, Loss: 0.8550613522529602\n",
      "Epoch: 0, Step: 123232, Loss: 0.8067880272865295\n",
      "Epoch: 0, Step: 123264, Loss: 0.8037018179893494\n",
      "Epoch: 0, Step: 123296, Loss: 0.8961472511291504\n",
      "Epoch: 0, Step: 123328, Loss: 0.8587291240692139\n",
      "Epoch: 0, Step: 123360, Loss: 0.892746090888977\n",
      "Epoch: 0, Step: 123392, Loss: 0.8240788578987122\n",
      "Epoch: 0, Step: 123424, Loss: 0.7701581120491028\n",
      "Epoch: 0, Step: 123456, Loss: 0.873249888420105\n",
      "Epoch: 0, Step: 123488, Loss: 0.7534801363945007\n",
      "Epoch: 0, Step: 123520, Loss: 0.8142500519752502\n",
      "Epoch: 0, Step: 123552, Loss: 0.8431727290153503\n",
      "Epoch: 0, Step: 123584, Loss: 0.9460716247558594\n",
      "Epoch: 0, Step: 123616, Loss: 0.866113543510437\n",
      "Epoch: 0, Step: 123648, Loss: 0.770466685295105\n",
      "Epoch: 0, Step: 123680, Loss: 0.676308274269104\n",
      "Epoch: 0, Step: 123712, Loss: 0.8328967690467834\n",
      "Epoch: 0, Step: 123744, Loss: 0.849747896194458\n",
      "Epoch: 0, Step: 123776, Loss: 0.8441155552864075\n",
      "Epoch: 0, Step: 123808, Loss: 0.8795046806335449\n",
      "Epoch: 0, Step: 123840, Loss: 0.7792539000511169\n",
      "Epoch: 0, Step: 123872, Loss: 0.8061556220054626\n",
      "Epoch: 0, Step: 123904, Loss: 0.7810990810394287\n",
      "Epoch: 0, Step: 123936, Loss: 0.8022900223731995\n",
      "Epoch: 0, Step: 123968, Loss: 0.7810684442520142\n",
      "Epoch: 0, Step: 124000, Loss: 0.8274217247962952\n",
      "Epoch: 0, Step: 124032, Loss: 0.7898269891738892\n",
      "Epoch: 0, Step: 124064, Loss: 0.8896334767341614\n",
      "Epoch: 0, Step: 124096, Loss: 0.725109338760376\n",
      "Epoch: 0, Step: 124128, Loss: 0.782648503780365\n",
      "Epoch: 0, Step: 124160, Loss: 0.8190333247184753\n",
      "Epoch: 0, Step: 124192, Loss: 0.8201682567596436\n",
      "Epoch: 0, Step: 124224, Loss: 0.8989313840866089\n",
      "Epoch: 0, Step: 124256, Loss: 0.8266149759292603\n",
      "Epoch: 0, Step: 124288, Loss: 0.8504459857940674\n",
      "Epoch: 0, Step: 124320, Loss: 0.7804325819015503\n",
      "Epoch: 0, Step: 124352, Loss: 0.7653904557228088\n",
      "Epoch: 0, Step: 124384, Loss: 0.7754706740379333\n",
      "Epoch: 0, Step: 124416, Loss: 0.8587868809700012\n",
      "Epoch: 0, Step: 124448, Loss: 0.81854647397995\n",
      "Epoch: 0, Step: 124480, Loss: 0.8023856282234192\n",
      "Epoch: 0, Step: 124512, Loss: 0.8327816128730774\n",
      "Epoch: 0, Step: 124544, Loss: 0.7160478830337524\n",
      "Epoch: 0, Step: 124576, Loss: 0.7431207895278931\n",
      "Epoch: 0, Step: 124608, Loss: 0.8196885585784912\n",
      "Epoch: 0, Step: 124640, Loss: 0.8006539940834045\n",
      "Epoch: 0, Step: 124672, Loss: 0.7897010445594788\n",
      "Epoch: 0, Step: 124704, Loss: 0.8431511521339417\n",
      "Epoch: 0, Step: 124736, Loss: 0.9067860841751099\n",
      "Epoch: 0, Step: 124768, Loss: 0.8385380506515503\n",
      "Epoch: 0, Step: 124800, Loss: 0.8440526127815247\n",
      "Epoch: 0, Step: 124832, Loss: 0.8336791396141052\n",
      "Epoch: 0, Step: 124864, Loss: 0.8007073998451233\n",
      "Epoch: 0, Step: 124896, Loss: 0.8196241855621338\n",
      "Epoch: 0, Step: 124928, Loss: 0.8206850290298462\n",
      "Epoch: 0, Step: 124960, Loss: 0.9295523166656494\n",
      "Epoch: 0, Step: 124992, Loss: 0.7810651659965515\n",
      "Epoch: 0, Step: 125024, Loss: 0.8248693346977234\n",
      "Epoch: 0, Step: 125056, Loss: 0.7605953812599182\n",
      "Epoch: 0, Step: 125088, Loss: 0.9011353254318237\n",
      "Epoch: 0, Step: 125120, Loss: 0.8610273599624634\n",
      "Epoch: 0, Step: 125152, Loss: 0.8956487774848938\n",
      "Epoch: 0, Step: 125184, Loss: 0.8177846074104309\n",
      "Epoch: 0, Step: 125216, Loss: 0.8923996686935425\n",
      "Epoch: 0, Step: 125248, Loss: 0.8491501212120056\n",
      "Epoch: 0, Step: 125280, Loss: 0.7821628451347351\n",
      "Epoch: 0, Step: 125312, Loss: 0.871886134147644\n",
      "Epoch: 0, Step: 125344, Loss: 0.8412982821464539\n",
      "Epoch: 0, Step: 125376, Loss: 0.7950097918510437\n",
      "Epoch: 0, Step: 125408, Loss: 0.8123533129692078\n",
      "Epoch: 0, Step: 125440, Loss: 0.8683927059173584\n",
      "Epoch: 0, Step: 125472, Loss: 0.7716639637947083\n",
      "Epoch: 0, Step: 125504, Loss: 0.8020958304405212\n",
      "Epoch: 0, Step: 125536, Loss: 0.8696386218070984\n",
      "Epoch: 0, Step: 125568, Loss: 0.8025297522544861\n",
      "Epoch: 0, Step: 125600, Loss: 0.7817820906639099\n",
      "Epoch: 0, Step: 125632, Loss: 0.9237239360809326\n",
      "Epoch: 0, Step: 125664, Loss: 0.8218742609024048\n",
      "Epoch: 0, Step: 125696, Loss: 0.8464199304580688\n",
      "Epoch: 0, Step: 125728, Loss: 0.9000674486160278\n",
      "Epoch: 0, Step: 125760, Loss: 0.9284802675247192\n",
      "Epoch: 0, Step: 125792, Loss: 0.7977758049964905\n",
      "Epoch: 0, Step: 125824, Loss: 0.8127279877662659\n",
      "Epoch: 0, Step: 125856, Loss: 0.7341210246086121\n",
      "Epoch: 0, Step: 125888, Loss: 0.8250581622123718\n",
      "Epoch: 0, Step: 125920, Loss: 0.820525050163269\n",
      "Epoch: 0, Step: 125952, Loss: 0.9264857769012451\n",
      "Epoch: 0, Step: 125984, Loss: 0.7157665491104126\n",
      "Epoch: 0, Step: 126016, Loss: 0.7271965742111206\n",
      "Epoch: 0, Step: 126048, Loss: 0.7752647995948792\n",
      "Epoch: 0, Step: 126080, Loss: 0.7656179666519165\n",
      "Epoch: 0, Step: 126112, Loss: 0.820328950881958\n",
      "Epoch: 0, Step: 126144, Loss: 0.9164745807647705\n",
      "Epoch: 0, Step: 126176, Loss: 0.8307182192802429\n",
      "Epoch: 0, Step: 126208, Loss: 0.7736607193946838\n",
      "Epoch: 0, Step: 126240, Loss: 0.7575789093971252\n",
      "Epoch: 0, Step: 126272, Loss: 0.7982863187789917\n",
      "Epoch: 0, Step: 126304, Loss: 0.8606177568435669\n",
      "Epoch: 0, Step: 126336, Loss: 0.8128263354301453\n",
      "Epoch: 0, Step: 126368, Loss: 0.8366991877555847\n",
      "Epoch: 0, Step: 126400, Loss: 0.7564271092414856\n",
      "Epoch: 0, Step: 126432, Loss: 0.9392748475074768\n",
      "Epoch: 0, Step: 126464, Loss: 0.7787914872169495\n",
      "Epoch: 0, Step: 126496, Loss: 0.7676553130149841\n",
      "Epoch: 0, Step: 126528, Loss: 0.8093941807746887\n",
      "Epoch: 0, Step: 126560, Loss: 0.8476405739784241\n",
      "Epoch: 0, Step: 126592, Loss: 0.8936479091644287\n",
      "Epoch: 0, Step: 126624, Loss: 0.7964597940444946\n",
      "Epoch: 0, Step: 126656, Loss: 0.8834840059280396\n",
      "Epoch: 0, Step: 126688, Loss: 0.7513565421104431\n",
      "Epoch: 0, Step: 126720, Loss: 0.7270334959030151\n",
      "Epoch: 0, Step: 126752, Loss: 0.7856517434120178\n",
      "Epoch: 0, Step: 126784, Loss: 0.86449134349823\n",
      "Epoch: 0, Step: 126816, Loss: 0.901864230632782\n",
      "Epoch: 0, Step: 126848, Loss: 0.8157073855400085\n",
      "Epoch: 0, Step: 126880, Loss: 0.8268514275550842\n",
      "Epoch: 0, Step: 126912, Loss: 0.820152223110199\n",
      "Epoch: 0, Step: 126944, Loss: 0.7708972096443176\n",
      "Epoch: 0, Step: 126976, Loss: 0.8886728286743164\n",
      "Epoch: 0, Step: 127008, Loss: 0.8073407411575317\n",
      "Epoch: 0, Step: 127040, Loss: 0.7349075078964233\n",
      "Epoch: 0, Step: 127072, Loss: 0.8828126788139343\n",
      "Epoch: 0, Step: 127104, Loss: 0.8015372157096863\n",
      "Epoch: 0, Step: 127136, Loss: 0.84464430809021\n",
      "Epoch: 0, Step: 127168, Loss: 0.8239008188247681\n",
      "Epoch: 0, Step: 127200, Loss: 0.8963510990142822\n",
      "Epoch: 0, Step: 127232, Loss: 0.7383268475532532\n",
      "Epoch: 0, Step: 127264, Loss: 0.8203950524330139\n",
      "Epoch: 0, Step: 127296, Loss: 0.7530542612075806\n",
      "Epoch: 0, Step: 127328, Loss: 0.8179251551628113\n",
      "Epoch: 0, Step: 127360, Loss: 0.7773191332817078\n",
      "Epoch: 0, Step: 127392, Loss: 0.8281657695770264\n",
      "Epoch: 0, Step: 127424, Loss: 0.8201009035110474\n",
      "Epoch: 0, Step: 127456, Loss: 0.8435768485069275\n",
      "Epoch: 0, Step: 127488, Loss: 0.8014233112335205\n",
      "Epoch: 0, Step: 127520, Loss: 0.8250218033790588\n",
      "Epoch: 0, Step: 127552, Loss: 0.9110008478164673\n",
      "Epoch: 0, Step: 127584, Loss: 0.7840999960899353\n",
      "Epoch: 0, Step: 127616, Loss: 0.8700529336929321\n",
      "Epoch: 0, Step: 127648, Loss: 0.8132612109184265\n",
      "Epoch: 0, Step: 127680, Loss: 0.7551790475845337\n",
      "Epoch: 0, Step: 127712, Loss: 0.972766637802124\n",
      "Epoch: 0, Step: 127744, Loss: 0.9137623310089111\n",
      "Epoch: 0, Step: 127776, Loss: 0.918569803237915\n",
      "Epoch: 0, Step: 127808, Loss: 0.8595845699310303\n",
      "Epoch: 0, Step: 127840, Loss: 0.7979781627655029\n",
      "Epoch: 0, Step: 127872, Loss: 0.7959690690040588\n",
      "Epoch: 0, Step: 127904, Loss: 0.8541654944419861\n",
      "Epoch: 0, Step: 127936, Loss: 0.8048726916313171\n",
      "Epoch: 0, Step: 127968, Loss: 0.8430042862892151\n",
      "Epoch: 0, Step: 128000, Loss: 0.8420569896697998\n",
      "Epoch: 0, Step: 128032, Loss: 0.7558974027633667\n",
      "Epoch: 0, Step: 128064, Loss: 0.7405630350112915\n",
      "Epoch: 0, Step: 128096, Loss: 0.8664839863777161\n",
      "Epoch: 0, Step: 128128, Loss: 0.8609187006950378\n",
      "Epoch: 0, Step: 128160, Loss: 0.8250390291213989\n",
      "Epoch: 0, Step: 128192, Loss: 0.7433599829673767\n",
      "Epoch: 0, Step: 128224, Loss: 0.8383520245552063\n",
      "Epoch: 0, Step: 128256, Loss: 0.8459813594818115\n",
      "Epoch: 0, Step: 128288, Loss: 0.7147799134254456\n",
      "Epoch: 0, Step: 128320, Loss: 0.959854006767273\n",
      "Epoch: 0, Step: 128352, Loss: 0.7875244617462158\n",
      "Epoch: 0, Step: 128384, Loss: 0.7825854420661926\n",
      "Epoch: 0, Step: 128416, Loss: 0.8835010528564453\n",
      "Epoch: 0, Step: 128448, Loss: 0.8596040606498718\n",
      "Epoch: 0, Step: 128480, Loss: 0.8528533577919006\n",
      "Epoch: 0, Step: 128512, Loss: 0.8754997849464417\n",
      "Epoch: 0, Step: 128544, Loss: 0.75359046459198\n",
      "Epoch: 0, Step: 128576, Loss: 0.8727113604545593\n",
      "Epoch: 0, Step: 128608, Loss: 0.8470270037651062\n",
      "Epoch: 0, Step: 128640, Loss: 0.9234040379524231\n",
      "Epoch: 0, Step: 128672, Loss: 0.819841742515564\n",
      "Epoch: 0, Step: 128704, Loss: 0.7722108364105225\n",
      "Epoch: 0, Step: 128736, Loss: 0.7914724946022034\n",
      "Epoch: 0, Step: 128768, Loss: 0.8687897324562073\n",
      "Epoch: 0, Step: 128800, Loss: 0.8401293158531189\n",
      "Epoch: 0, Step: 128832, Loss: 0.8266009092330933\n",
      "Epoch: 0, Step: 128864, Loss: 0.847030520439148\n",
      "Epoch: 0, Step: 128896, Loss: 0.7712575197219849\n",
      "Epoch: 0, Step: 128928, Loss: 0.9081364274024963\n",
      "Epoch: 0, Step: 128960, Loss: 0.8423901200294495\n",
      "Epoch: 0, Step: 128992, Loss: 0.859485924243927\n",
      "Epoch: 0, Step: 129024, Loss: 0.7303493618965149\n",
      "Epoch: 0, Step: 129056, Loss: 0.7579361200332642\n",
      "Epoch: 0, Step: 129088, Loss: 0.9007245898246765\n",
      "Epoch: 0, Step: 129120, Loss: 0.9376616477966309\n",
      "Epoch: 0, Step: 129152, Loss: 0.8169693350791931\n",
      "Epoch: 0, Step: 129184, Loss: 0.7592210173606873\n",
      "Epoch: 0, Step: 129216, Loss: 0.7517465949058533\n",
      "Epoch: 0, Step: 129248, Loss: 0.8637099266052246\n",
      "Epoch: 0, Step: 129280, Loss: 0.8517945408821106\n",
      "Epoch: 0, Step: 129312, Loss: 0.7786754965782166\n",
      "Epoch: 0, Step: 129344, Loss: 0.8721392750740051\n",
      "Epoch: 0, Step: 129376, Loss: 0.7358629107475281\n",
      "Epoch: 0, Step: 129408, Loss: 0.8044701218605042\n",
      "Epoch: 0, Step: 129440, Loss: 0.8814318776130676\n",
      "Epoch: 0, Step: 129472, Loss: 0.8543527126312256\n",
      "Epoch: 0, Step: 129504, Loss: 0.8553435206413269\n",
      "Epoch: 0, Step: 129536, Loss: 0.740985095500946\n",
      "Epoch: 0, Step: 129568, Loss: 0.9105566740036011\n",
      "Epoch: 0, Step: 129600, Loss: 0.8767951130867004\n",
      "Epoch: 0, Step: 129632, Loss: 0.964543342590332\n",
      "Epoch: 0, Step: 129664, Loss: 0.7532231211662292\n",
      "Epoch: 0, Step: 129696, Loss: 0.6919634938240051\n",
      "Epoch: 0, Step: 129728, Loss: 0.7800120711326599\n",
      "Epoch: 0, Step: 129760, Loss: 0.8826115727424622\n",
      "Epoch: 0, Step: 129792, Loss: 0.7975229620933533\n",
      "Epoch: 0, Step: 129824, Loss: 0.7553258538246155\n",
      "Epoch: 0, Step: 129856, Loss: 0.8635574579238892\n",
      "Epoch: 0, Step: 129888, Loss: 0.7778299450874329\n",
      "Epoch: 0, Step: 129920, Loss: 0.815807580947876\n",
      "Epoch: 0, Step: 129952, Loss: 0.7909270524978638\n",
      "Epoch: 0, Step: 129984, Loss: 0.7986376881599426\n",
      "Epoch: 0, Step: 130016, Loss: 0.8980910778045654\n",
      "Epoch: 0, Step: 130048, Loss: 0.7943390011787415\n",
      "Epoch: 0, Step: 130080, Loss: 0.8500688076019287\n",
      "Epoch: 0, Step: 130112, Loss: 0.8070638179779053\n",
      "Epoch: 0, Step: 130144, Loss: 0.7854618430137634\n",
      "Epoch: 0, Step: 130176, Loss: 0.8015767335891724\n",
      "Epoch: 0, Step: 130208, Loss: 0.8643096089363098\n",
      "Epoch: 0, Step: 130240, Loss: 0.8104048371315002\n",
      "Epoch: 0, Step: 130272, Loss: 0.888309121131897\n",
      "Epoch: 0, Step: 130304, Loss: 0.84972083568573\n",
      "Epoch: 0, Step: 130336, Loss: 0.7722998857498169\n",
      "Epoch: 0, Step: 130368, Loss: 0.73944491147995\n",
      "Epoch: 0, Step: 130400, Loss: 0.844166100025177\n",
      "Epoch: 0, Step: 130432, Loss: 0.7610779404640198\n",
      "Epoch: 0, Step: 130464, Loss: 0.8202160596847534\n",
      "Epoch: 0, Step: 130496, Loss: 0.7501507997512817\n",
      "Epoch: 0, Step: 130528, Loss: 0.7873207926750183\n",
      "Epoch: 0, Step: 130560, Loss: 0.8234954476356506\n",
      "Epoch: 0, Step: 130592, Loss: 0.9238703846931458\n",
      "Epoch: 0, Step: 130624, Loss: 0.8235894441604614\n",
      "Epoch: 0, Step: 130656, Loss: 0.7003277540206909\n",
      "Epoch: 0, Step: 130688, Loss: 0.8011342287063599\n",
      "Epoch: 0, Step: 130720, Loss: 0.8249452710151672\n",
      "Epoch: 0, Step: 130752, Loss: 0.7420913577079773\n",
      "Epoch: 0, Step: 130784, Loss: 0.8125255703926086\n",
      "Epoch: 0, Step: 130816, Loss: 0.7650733590126038\n",
      "Epoch: 0, Step: 130848, Loss: 0.8538286089897156\n",
      "Epoch: 0, Step: 130880, Loss: 0.8066684603691101\n",
      "Epoch: 0, Step: 130912, Loss: 0.8311323523521423\n",
      "Epoch: 0, Step: 130944, Loss: 0.8331255316734314\n",
      "Epoch: 0, Step: 130976, Loss: 0.7822118997573853\n",
      "Epoch: 0, Step: 131008, Loss: 0.8382075428962708\n",
      "Epoch: 0, Step: 131040, Loss: 0.7593333721160889\n",
      "Epoch: 0, Step: 131072, Loss: 0.7579712271690369\n",
      "Epoch: 0, Step: 131104, Loss: 0.9368740320205688\n",
      "Epoch: 0, Step: 131136, Loss: 0.7250627875328064\n",
      "Epoch: 0, Step: 131168, Loss: 0.8757639527320862\n",
      "Epoch: 0, Step: 131200, Loss: 0.7918704152107239\n",
      "Epoch: 0, Step: 131232, Loss: 0.7886353135108948\n",
      "Epoch: 0, Step: 131264, Loss: 0.802489697933197\n",
      "Epoch: 0, Step: 131296, Loss: 0.8367098569869995\n",
      "Epoch: 0, Step: 131328, Loss: 0.8276573419570923\n",
      "Epoch: 0, Step: 131360, Loss: 0.8349312543869019\n",
      "Epoch: 0, Step: 131392, Loss: 0.82387775182724\n",
      "Epoch: 0, Step: 131424, Loss: 0.8608750104904175\n",
      "Epoch: 0, Step: 131456, Loss: 0.7770817875862122\n",
      "Epoch: 0, Step: 131488, Loss: 0.8555999994277954\n",
      "Epoch: 0, Step: 131520, Loss: 0.9352436661720276\n",
      "Epoch: 0, Step: 131552, Loss: 0.9824562072753906\n",
      "Epoch: 0, Step: 131584, Loss: 0.9017220139503479\n",
      "Epoch: 0, Step: 131616, Loss: 0.7741118669509888\n",
      "Epoch: 0, Step: 131648, Loss: 0.8646882772445679\n",
      "Epoch: 0, Step: 131680, Loss: 0.8709933757781982\n",
      "Epoch: 0, Step: 131712, Loss: 0.8354973196983337\n",
      "Epoch: 0, Step: 131744, Loss: 0.7951642274856567\n",
      "Epoch: 0, Step: 131776, Loss: 0.8764921426773071\n",
      "Epoch: 0, Step: 131808, Loss: 0.8383232951164246\n",
      "Epoch: 0, Step: 131840, Loss: 0.8753582835197449\n",
      "Epoch: 0, Step: 131872, Loss: 0.7492645978927612\n",
      "Epoch: 0, Step: 131904, Loss: 0.8951936364173889\n",
      "Epoch: 0, Step: 131936, Loss: 0.9142982363700867\n",
      "Epoch: 0, Step: 131968, Loss: 0.8376352787017822\n",
      "Epoch: 0, Step: 132000, Loss: 0.7765968441963196\n",
      "Epoch: 0, Step: 132032, Loss: 0.8683568239212036\n",
      "Epoch: 0, Step: 132064, Loss: 0.7928529381752014\n",
      "Epoch: 0, Step: 132096, Loss: 0.8145958185195923\n",
      "Epoch: 0, Step: 132128, Loss: 0.7509397864341736\n",
      "Epoch: 0, Step: 132160, Loss: 0.8185176253318787\n",
      "Epoch: 0, Step: 132192, Loss: 0.8566665053367615\n",
      "Epoch: 0, Step: 132224, Loss: 0.7634824514389038\n",
      "Epoch: 0, Step: 132256, Loss: 0.8277371525764465\n",
      "Epoch: 0, Step: 132288, Loss: 0.8201583623886108\n",
      "Epoch: 0, Step: 132320, Loss: 0.8399196863174438\n",
      "Epoch: 0, Step: 132352, Loss: 0.8284152150154114\n",
      "Epoch: 0, Step: 132384, Loss: 0.7706812620162964\n",
      "Epoch: 0, Step: 132416, Loss: 0.8234585523605347\n",
      "Epoch: 0, Step: 132448, Loss: 0.8686177730560303\n",
      "Epoch: 0, Step: 132480, Loss: 0.9323098063468933\n",
      "Epoch: 0, Step: 132512, Loss: 0.826424241065979\n",
      "Epoch: 0, Step: 132544, Loss: 0.865523636341095\n",
      "Epoch: 0, Step: 132576, Loss: 0.8181931376457214\n",
      "Epoch: 0, Step: 132608, Loss: 0.9463881254196167\n",
      "Epoch: 0, Step: 132640, Loss: 0.7921766042709351\n",
      "Epoch: 0, Step: 132672, Loss: 0.8626216053962708\n",
      "Epoch: 0, Step: 132704, Loss: 0.7970669865608215\n",
      "Epoch: 0, Step: 132736, Loss: 0.7198454141616821\n",
      "Epoch: 0, Step: 132768, Loss: 0.9010884761810303\n",
      "Epoch: 0, Step: 132800, Loss: 0.7427154779434204\n",
      "Epoch: 0, Step: 132832, Loss: 0.82929927110672\n",
      "Epoch: 0, Step: 132864, Loss: 0.708577036857605\n",
      "Epoch: 0, Step: 132896, Loss: 0.7959092259407043\n",
      "Epoch: 0, Step: 132928, Loss: 0.7576087117195129\n",
      "Epoch: 0, Step: 132960, Loss: 0.7952948212623596\n",
      "Epoch: 0, Step: 132992, Loss: 0.8375073075294495\n",
      "Epoch: 0, Step: 133024, Loss: 0.8097187876701355\n",
      "Epoch: 0, Step: 133056, Loss: 0.8594809174537659\n",
      "Epoch: 0, Step: 133088, Loss: 0.7938706874847412\n",
      "Epoch: 0, Step: 133120, Loss: 0.7240588665008545\n",
      "Epoch: 0, Step: 133152, Loss: 0.8519785404205322\n",
      "Epoch: 0, Step: 133184, Loss: 0.7543036341667175\n",
      "Epoch: 0, Step: 133216, Loss: 0.7453415393829346\n",
      "Epoch: 0, Step: 133248, Loss: 0.8167018294334412\n",
      "Epoch: 0, Step: 133280, Loss: 0.8109248280525208\n",
      "Epoch: 0, Step: 133312, Loss: 0.8525313138961792\n",
      "Epoch: 0, Step: 133344, Loss: 0.8854456543922424\n",
      "Epoch: 0, Step: 133376, Loss: 0.8702199459075928\n",
      "Epoch: 0, Step: 133408, Loss: 0.9044477343559265\n",
      "Epoch: 0, Step: 133440, Loss: 0.7717143893241882\n",
      "Epoch: 0, Step: 133472, Loss: 0.7234451174736023\n",
      "Epoch: 0, Step: 133504, Loss: 0.8293798565864563\n",
      "Epoch: 0, Step: 133536, Loss: 0.8174684047698975\n",
      "Epoch: 0, Step: 133568, Loss: 0.8466506600379944\n",
      "Epoch: 0, Step: 133600, Loss: 0.8084534406661987\n",
      "Epoch: 0, Step: 133632, Loss: 0.881693959236145\n",
      "Epoch: 0, Step: 133664, Loss: 0.6931000351905823\n",
      "Epoch: 0, Step: 133696, Loss: 0.7939971685409546\n",
      "Epoch: 0, Step: 133728, Loss: 0.8923555612564087\n",
      "Epoch: 0, Step: 133760, Loss: 0.7531341910362244\n",
      "Epoch: 0, Step: 133792, Loss: 0.8515940308570862\n",
      "Epoch: 0, Step: 133824, Loss: 0.8403669595718384\n",
      "Epoch: 0, Step: 133856, Loss: 0.830896258354187\n",
      "Epoch: 0, Step: 133888, Loss: 0.7561607956886292\n",
      "Epoch: 0, Step: 133920, Loss: 0.8647353649139404\n",
      "Epoch: 0, Step: 133952, Loss: 0.7375014424324036\n",
      "Epoch: 0, Step: 133984, Loss: 0.9124016761779785\n",
      "Epoch: 0, Step: 134016, Loss: 0.7856735587120056\n",
      "Epoch: 0, Step: 134048, Loss: 0.7802477478981018\n",
      "Epoch: 0, Step: 134080, Loss: 0.7804559469223022\n",
      "Epoch: 0, Step: 134112, Loss: 0.8696039319038391\n",
      "Epoch: 0, Step: 134144, Loss: 0.905756950378418\n",
      "Epoch: 0, Step: 134176, Loss: 0.7703324556350708\n",
      "Epoch: 0, Step: 134208, Loss: 0.8915284276008606\n",
      "Epoch: 0, Step: 134240, Loss: 0.8432736992835999\n",
      "Epoch: 0, Step: 134272, Loss: 0.8455159068107605\n",
      "Epoch: 0, Step: 134304, Loss: 0.8182020783424377\n",
      "Epoch: 0, Step: 134336, Loss: 0.8482568264007568\n",
      "Epoch: 0, Step: 134368, Loss: 0.8468207120895386\n",
      "Epoch: 0, Step: 134400, Loss: 0.7364600300788879\n",
      "Epoch: 0, Step: 134432, Loss: 0.7774714231491089\n",
      "Epoch: 0, Step: 134464, Loss: 0.7645366191864014\n",
      "Epoch: 0, Step: 134496, Loss: 0.8541577458381653\n",
      "Epoch: 0, Step: 134528, Loss: 0.8411869406700134\n",
      "Epoch: 0, Step: 134560, Loss: 0.7674402594566345\n",
      "Epoch: 0, Step: 134592, Loss: 0.8201690912246704\n",
      "Epoch: 0, Step: 134624, Loss: 0.8100041747093201\n",
      "Epoch: 0, Step: 134656, Loss: 0.8537377715110779\n",
      "Epoch: 0, Step: 134688, Loss: 0.7694374322891235\n",
      "Epoch: 0, Step: 134720, Loss: 0.7892625331878662\n",
      "Epoch: 0, Step: 134752, Loss: 0.866357147693634\n",
      "Epoch: 0, Step: 134784, Loss: 0.8528673052787781\n",
      "Epoch: 0, Step: 134816, Loss: 0.8269826769828796\n",
      "Epoch: 0, Step: 134848, Loss: 0.8634950518608093\n",
      "Epoch: 0, Step: 134880, Loss: 0.8814745545387268\n",
      "Epoch: 0, Step: 134912, Loss: 0.8011276125907898\n",
      "Epoch: 0, Step: 134944, Loss: 0.8025208115577698\n",
      "Epoch: 0, Step: 134976, Loss: 0.8307639956474304\n",
      "Epoch: 0, Step: 135008, Loss: 0.8468177914619446\n",
      "Epoch: 0, Step: 135040, Loss: 0.8817719221115112\n",
      "Epoch: 0, Step: 135072, Loss: 0.812214732170105\n",
      "Epoch: 0, Step: 135104, Loss: 0.8386889100074768\n",
      "Epoch: 0, Step: 135136, Loss: 0.8460037112236023\n",
      "Epoch: 0, Step: 135168, Loss: 0.9705117344856262\n",
      "Epoch: 0, Step: 135200, Loss: 0.8128673434257507\n",
      "Epoch: 0, Step: 135232, Loss: 0.8507207632064819\n",
      "Epoch: 0, Step: 135264, Loss: 0.8856250047683716\n",
      "Epoch: 0, Step: 135296, Loss: 0.7645065784454346\n",
      "Epoch: 0, Step: 135328, Loss: 0.8051146268844604\n",
      "Epoch: 0, Step: 135360, Loss: 0.8042171001434326\n",
      "Epoch: 0, Step: 135392, Loss: 0.7435134053230286\n",
      "Epoch: 0, Step: 135424, Loss: 0.9587057828903198\n",
      "Epoch: 0, Step: 135456, Loss: 0.8066582083702087\n",
      "Epoch: 0, Step: 135488, Loss: 0.8002879619598389\n",
      "Epoch: 0, Step: 135520, Loss: 0.8832186460494995\n",
      "Epoch: 0, Step: 135552, Loss: 0.8784416913986206\n",
      "Epoch: 0, Step: 135584, Loss: 0.829717755317688\n",
      "Epoch: 0, Step: 135616, Loss: 0.7448890209197998\n",
      "Epoch: 0, Step: 135648, Loss: 0.8957982063293457\n",
      "Epoch: 0, Step: 135680, Loss: 0.8245131969451904\n",
      "Epoch: 0, Step: 135712, Loss: 0.8128260374069214\n",
      "Epoch: 0, Step: 135744, Loss: 0.9524462819099426\n",
      "Epoch: 0, Step: 135776, Loss: 0.7978101968765259\n",
      "Epoch: 0, Step: 135808, Loss: 0.8647774457931519\n",
      "Epoch: 0, Step: 135840, Loss: 0.8001886010169983\n",
      "Epoch: 0, Step: 135872, Loss: 0.8040401339530945\n",
      "Epoch: 0, Step: 135904, Loss: 0.7272554636001587\n",
      "Epoch: 0, Step: 135936, Loss: 0.7798365354537964\n",
      "Epoch: 0, Step: 135968, Loss: 0.8646841645240784\n",
      "Epoch: 0, Step: 136000, Loss: 0.8149513006210327\n",
      "Epoch: 0, Step: 136032, Loss: 0.7646992802619934\n",
      "Epoch: 0, Step: 136064, Loss: 0.7200321555137634\n",
      "Epoch: 0, Step: 136096, Loss: 0.8428913354873657\n",
      "Epoch: 0, Step: 136128, Loss: 0.7637190222740173\n",
      "Epoch: 0, Step: 136160, Loss: 0.7985650897026062\n",
      "Epoch: 0, Step: 136192, Loss: 0.8517650961875916\n",
      "Epoch: 0, Step: 136224, Loss: 0.9023885726928711\n",
      "Epoch: 0, Step: 136256, Loss: 0.7363230586051941\n",
      "Epoch: 0, Step: 136288, Loss: 0.7680596113204956\n",
      "Epoch: 0, Step: 136320, Loss: 0.7663276195526123\n",
      "Epoch: 0, Step: 136352, Loss: 0.8018132448196411\n",
      "Epoch: 0, Step: 136384, Loss: 0.828364908695221\n",
      "Epoch: 0, Step: 136416, Loss: 0.8655106425285339\n",
      "Epoch: 0, Step: 136448, Loss: 0.8673045039176941\n",
      "Epoch: 0, Step: 136480, Loss: 0.8477455377578735\n",
      "Epoch: 0, Step: 136512, Loss: 0.8079740405082703\n",
      "Epoch: 0, Step: 136544, Loss: 0.8774433732032776\n",
      "Epoch: 0, Step: 136576, Loss: 0.8380783796310425\n",
      "Epoch: 0, Step: 136608, Loss: 0.919697105884552\n",
      "Epoch: 0, Step: 136640, Loss: 0.8690930008888245\n",
      "Epoch: 0, Step: 136672, Loss: 0.8226600885391235\n",
      "Epoch: 0, Step: 136704, Loss: 0.8827885389328003\n",
      "Epoch: 0, Step: 136736, Loss: 0.8947035074234009\n",
      "Epoch: 0, Step: 136768, Loss: 0.8412660360336304\n",
      "Epoch: 0, Step: 136800, Loss: 0.725612461566925\n",
      "Epoch: 0, Step: 136832, Loss: 0.7799673676490784\n",
      "Epoch: 0, Step: 136864, Loss: 0.8659107089042664\n",
      "Epoch: 0, Step: 136896, Loss: 0.7933363318443298\n",
      "Epoch: 0, Step: 136928, Loss: 0.8593769669532776\n",
      "Epoch: 0, Step: 136960, Loss: 0.8429226279258728\n",
      "Epoch: 0, Step: 136992, Loss: 0.7347866892814636\n",
      "Epoch: 0, Step: 137024, Loss: 0.8835740089416504\n",
      "Epoch: 0, Step: 137056, Loss: 0.9439182877540588\n",
      "Epoch: 0, Step: 137088, Loss: 0.7161925435066223\n",
      "Epoch: 0, Step: 137120, Loss: 0.7976564764976501\n",
      "Epoch: 0, Step: 137152, Loss: 0.7689526677131653\n",
      "Epoch: 0, Step: 137184, Loss: 0.8675462603569031\n",
      "Epoch: 0, Step: 137216, Loss: 0.8053821921348572\n",
      "Epoch: 0, Step: 137248, Loss: 0.790777862071991\n",
      "Epoch: 0, Step: 137280, Loss: 0.8786546587944031\n",
      "Epoch: 0, Step: 137312, Loss: 0.7888123393058777\n",
      "Epoch: 0, Step: 137344, Loss: 0.9243369102478027\n",
      "Epoch: 0, Step: 137376, Loss: 0.758711576461792\n",
      "Epoch: 0, Step: 137408, Loss: 0.8462809324264526\n",
      "Epoch: 0, Step: 137440, Loss: 0.7492949962615967\n",
      "Epoch: 0, Step: 137472, Loss: 0.8103123307228088\n",
      "Epoch: 0, Step: 137504, Loss: 0.8132403492927551\n",
      "Epoch: 0, Step: 137536, Loss: 0.7697984576225281\n",
      "Epoch: 0, Step: 137568, Loss: 0.8611472845077515\n",
      "Epoch: 0, Step: 137600, Loss: 0.8053672909736633\n",
      "Epoch: 0, Step: 137632, Loss: 0.8437202572822571\n",
      "Epoch: 0, Step: 137664, Loss: 0.8078168630599976\n",
      "Epoch: 0, Step: 137696, Loss: 0.7668235301971436\n",
      "Epoch: 0, Step: 137728, Loss: 0.8306424617767334\n",
      "Epoch: 0, Step: 137760, Loss: 0.7666126489639282\n",
      "Epoch: 0, Step: 137792, Loss: 0.8733389973640442\n",
      "Epoch: 0, Step: 137824, Loss: 0.8151456117630005\n",
      "Epoch: 0, Step: 137856, Loss: 0.7884201407432556\n",
      "Epoch: 0, Step: 137888, Loss: 0.8648610711097717\n",
      "Epoch: 0, Step: 137920, Loss: 0.8351098299026489\n",
      "Epoch: 0, Step: 137952, Loss: 0.8102715611457825\n",
      "Epoch: 0, Step: 137984, Loss: 0.8040605187416077\n",
      "Epoch: 0, Step: 138016, Loss: 0.8404614925384521\n",
      "Epoch: 0, Step: 138048, Loss: 0.8844466209411621\n",
      "Epoch: 0, Step: 138080, Loss: 0.8128631114959717\n",
      "Epoch: 0, Step: 138112, Loss: 0.8338655233383179\n",
      "Epoch: 0, Step: 138144, Loss: 0.8418785929679871\n",
      "Epoch: 0, Step: 138176, Loss: 0.7839220762252808\n",
      "Epoch: 0, Step: 138208, Loss: 0.8414318561553955\n",
      "Epoch: 0, Step: 138240, Loss: 0.7864445447921753\n",
      "Epoch: 0, Step: 138272, Loss: 0.7753779292106628\n",
      "Epoch: 0, Step: 138304, Loss: 0.8574183583259583\n",
      "Epoch: 0, Step: 138336, Loss: 0.9238408207893372\n",
      "Epoch: 0, Step: 138368, Loss: 0.7895302176475525\n",
      "Epoch: 0, Step: 138400, Loss: 0.9213885068893433\n",
      "Epoch: 0, Step: 138432, Loss: 0.8592365384101868\n",
      "Epoch: 0, Step: 138464, Loss: 0.7683994174003601\n",
      "Epoch: 0, Step: 138496, Loss: 0.7381489872932434\n",
      "Epoch: 0, Step: 138528, Loss: 0.6846420168876648\n",
      "Epoch: 0, Step: 138560, Loss: 0.7902769446372986\n",
      "Epoch: 0, Step: 138592, Loss: 0.8108805418014526\n",
      "Epoch: 0, Step: 138624, Loss: 0.8483465909957886\n",
      "Epoch: 0, Step: 138656, Loss: 0.7685450315475464\n",
      "Epoch: 0, Step: 138688, Loss: 0.7744731307029724\n",
      "Epoch: 0, Step: 138720, Loss: 0.831526517868042\n",
      "Epoch: 0, Step: 138752, Loss: 0.906703531742096\n",
      "Epoch: 0, Step: 138784, Loss: 0.7868556380271912\n",
      "Epoch: 0, Step: 138816, Loss: 0.7985687851905823\n",
      "Epoch: 0, Step: 138848, Loss: 0.7584995627403259\n",
      "Epoch: 0, Step: 138880, Loss: 0.7184388041496277\n",
      "Epoch: 0, Step: 138912, Loss: 0.8188163638114929\n",
      "Epoch: 0, Step: 138944, Loss: 0.8970257043838501\n",
      "Epoch: 0, Step: 138976, Loss: 0.8963280916213989\n",
      "Epoch: 0, Step: 139008, Loss: 0.7410746216773987\n",
      "Epoch: 0, Step: 139040, Loss: 0.8642914891242981\n",
      "Epoch: 0, Step: 139072, Loss: 0.8660762310028076\n",
      "Epoch: 0, Step: 139104, Loss: 0.7398263812065125\n",
      "Epoch: 0, Step: 139136, Loss: 0.8334820866584778\n",
      "Epoch: 0, Step: 139168, Loss: 0.7870457768440247\n",
      "Epoch: 0, Step: 139200, Loss: 0.838439404964447\n",
      "Epoch: 0, Step: 139232, Loss: 0.7068323493003845\n",
      "Epoch: 0, Step: 139264, Loss: 0.8173971772193909\n",
      "Epoch: 0, Step: 139296, Loss: 0.8258712291717529\n",
      "Epoch: 0, Step: 139328, Loss: 0.7644593715667725\n",
      "Epoch: 0, Step: 139360, Loss: 0.6764510869979858\n",
      "Epoch: 0, Step: 139392, Loss: 0.857116162776947\n",
      "Epoch: 0, Step: 139424, Loss: 0.7970874309539795\n",
      "Epoch: 0, Step: 139456, Loss: 0.7783552408218384\n",
      "Epoch: 0, Step: 139488, Loss: 0.8926122188568115\n",
      "Epoch: 0, Step: 139520, Loss: 0.791549563407898\n",
      "Epoch: 0, Step: 139552, Loss: 0.7722596526145935\n",
      "Epoch: 0, Step: 139584, Loss: 0.8872045278549194\n",
      "Epoch: 0, Step: 139616, Loss: 0.8534805178642273\n",
      "Epoch: 0, Step: 139648, Loss: 0.8390544056892395\n",
      "Epoch: 0, Step: 139680, Loss: 0.8385270237922668\n",
      "Epoch: 0, Step: 139712, Loss: 0.827193558216095\n",
      "Epoch: 0, Step: 139744, Loss: 0.8913487195968628\n",
      "Epoch: 0, Step: 139776, Loss: 0.8865257501602173\n",
      "Epoch: 0, Step: 139808, Loss: 0.845230221748352\n",
      "Epoch: 0, Step: 139840, Loss: 0.7823109030723572\n",
      "Epoch: 0, Step: 139872, Loss: 0.7559096813201904\n",
      "Epoch: 0, Step: 139904, Loss: 0.8489945530891418\n",
      "Epoch: 0, Step: 139936, Loss: 0.9607316851615906\n",
      "Epoch: 0, Step: 139968, Loss: 0.980962872505188\n",
      "Epoch: 0, Step: 140000, Loss: 0.8710252642631531\n",
      "Epoch: 0, Step: 140032, Loss: 0.8061921000480652\n",
      "Epoch: 0, Step: 140064, Loss: 0.9342612028121948\n",
      "Epoch: 0, Step: 140096, Loss: 0.7118554711341858\n",
      "Epoch: 0, Step: 140128, Loss: 0.7673631906509399\n",
      "Epoch: 0, Step: 140160, Loss: 0.8246940970420837\n",
      "Epoch: 0, Step: 140192, Loss: 0.7486171126365662\n",
      "Epoch: 0, Step: 140224, Loss: 0.8863641619682312\n",
      "Epoch: 0, Step: 140256, Loss: 0.8614156246185303\n",
      "Epoch: 0, Step: 140288, Loss: 0.8372237682342529\n",
      "Epoch: 0, Step: 140320, Loss: 0.8412877917289734\n",
      "Epoch: 0, Step: 140352, Loss: 0.946790337562561\n",
      "Epoch: 0, Step: 140384, Loss: 0.8019281625747681\n",
      "Epoch: 0, Step: 140416, Loss: 0.7306339144706726\n",
      "Epoch: 0, Step: 140448, Loss: 0.7546393871307373\n",
      "Epoch: 0, Step: 140480, Loss: 0.934532880783081\n",
      "Epoch: 0, Step: 140512, Loss: 0.7853802442550659\n",
      "Epoch: 0, Step: 140544, Loss: 0.837658703327179\n",
      "Epoch: 0, Step: 140576, Loss: 0.8260605931282043\n",
      "Epoch: 0, Step: 140608, Loss: 0.8777768015861511\n",
      "Epoch: 0, Step: 140640, Loss: 0.9229846596717834\n",
      "Epoch: 0, Step: 140672, Loss: 0.7681217789649963\n",
      "Epoch: 0, Step: 140704, Loss: 0.8351609706878662\n",
      "Epoch: 0, Step: 140736, Loss: 0.7975096106529236\n",
      "Epoch: 0, Step: 140768, Loss: 0.798561155796051\n",
      "Epoch: 0, Step: 140800, Loss: 0.8915059566497803\n",
      "Epoch: 0, Step: 140832, Loss: 0.835051953792572\n",
      "Epoch: 0, Step: 140864, Loss: 0.9136162996292114\n",
      "Epoch: 0, Step: 140896, Loss: 0.8536946773529053\n",
      "Epoch: 0, Step: 140928, Loss: 0.7541599869728088\n",
      "Epoch: 0, Step: 140960, Loss: 0.8679206371307373\n",
      "Epoch: 0, Step: 140992, Loss: 0.7961204051971436\n",
      "Epoch: 0, Step: 141024, Loss: 0.8297792077064514\n",
      "Epoch: 0, Step: 141056, Loss: 0.8251894116401672\n",
      "Epoch: 0, Step: 141088, Loss: 0.7803440690040588\n",
      "Epoch: 0, Step: 141120, Loss: 0.8981833457946777\n",
      "Epoch: 0, Step: 141152, Loss: 0.8685947060585022\n",
      "Epoch: 0, Step: 141184, Loss: 0.8086647391319275\n",
      "Epoch: 0, Step: 141216, Loss: 0.7676721811294556\n",
      "Epoch: 0, Step: 141248, Loss: 0.853340744972229\n",
      "Epoch: 0, Step: 141280, Loss: 0.7547191977500916\n",
      "Epoch: 0, Step: 141312, Loss: 0.8257931470870972\n",
      "Epoch: 0, Step: 141344, Loss: 0.8718605637550354\n",
      "Epoch: 0, Step: 141376, Loss: 0.8789140582084656\n",
      "Epoch: 0, Step: 141408, Loss: 0.8523279428482056\n",
      "Epoch: 0, Step: 141440, Loss: 0.817970335483551\n",
      "Epoch: 0, Step: 141472, Loss: 0.8028129935264587\n",
      "Epoch: 0, Step: 141504, Loss: 0.822372317314148\n",
      "Epoch: 0, Step: 141536, Loss: 0.8406011462211609\n",
      "Epoch: 0, Step: 141568, Loss: 0.7870165109634399\n",
      "Epoch: 0, Step: 141600, Loss: 0.8472481369972229\n",
      "Epoch: 0, Step: 141632, Loss: 0.7671114802360535\n",
      "Epoch: 0, Step: 141664, Loss: 0.779164731502533\n",
      "Epoch: 0, Step: 141696, Loss: 0.9063221216201782\n",
      "Epoch: 0, Step: 141728, Loss: 0.8493982553482056\n",
      "Epoch: 0, Step: 141760, Loss: 0.857395350933075\n",
      "Epoch: 0, Step: 141792, Loss: 0.8325965404510498\n",
      "Epoch: 0, Step: 141824, Loss: 0.7325645685195923\n",
      "Epoch: 0, Step: 141856, Loss: 0.8821021318435669\n",
      "Epoch: 0, Step: 141888, Loss: 1.0340838432312012\n",
      "Epoch: 0, Step: 141920, Loss: 0.7453391551971436\n",
      "Epoch: 0, Step: 141952, Loss: 0.812548816204071\n",
      "Epoch: 0, Step: 141984, Loss: 0.8158591389656067\n",
      "Epoch: 0, Step: 142016, Loss: 0.7947605848312378\n",
      "Epoch: 0, Step: 142048, Loss: 0.7926066517829895\n",
      "Epoch: 0, Step: 142080, Loss: 0.836587131023407\n",
      "Epoch: 0, Step: 142112, Loss: 0.7571319341659546\n",
      "Epoch: 0, Step: 142144, Loss: 0.8000514507293701\n",
      "Epoch: 0, Step: 142176, Loss: 0.8446986675262451\n",
      "Epoch: 0, Step: 142208, Loss: 0.7813466191291809\n",
      "Epoch: 0, Step: 142240, Loss: 0.6913501620292664\n",
      "Epoch: 0, Step: 142272, Loss: 0.8071585297584534\n",
      "Epoch: 0, Step: 142304, Loss: 0.8323527574539185\n",
      "Epoch: 0, Step: 142336, Loss: 0.816473662853241\n",
      "Epoch: 0, Step: 142368, Loss: 0.7113789319992065\n",
      "Epoch: 0, Step: 142400, Loss: 0.7949381470680237\n",
      "Epoch: 0, Step: 142432, Loss: 0.7576719522476196\n",
      "Epoch: 0, Step: 142464, Loss: 0.8861253261566162\n",
      "Epoch: 0, Step: 142496, Loss: 0.8596184849739075\n",
      "Epoch: 0, Step: 142528, Loss: 0.8566134572029114\n",
      "Epoch: 0, Step: 142560, Loss: 0.7780004739761353\n",
      "Epoch: 0, Step: 142592, Loss: 0.7762167453765869\n",
      "Epoch: 0, Step: 142624, Loss: 0.8685433864593506\n",
      "Epoch: 0, Step: 142656, Loss: 0.8505765795707703\n",
      "Epoch: 0, Step: 142688, Loss: 0.8359077572822571\n",
      "Epoch: 0, Step: 142720, Loss: 0.7741008996963501\n",
      "Epoch: 0, Step: 142752, Loss: 0.8658196926116943\n",
      "Epoch: 0, Step: 142784, Loss: 0.8084948658943176\n",
      "Epoch: 0, Step: 142816, Loss: 0.8020265102386475\n",
      "Epoch: 0, Step: 142848, Loss: 0.8495343327522278\n",
      "Epoch: 0, Step: 142880, Loss: 0.804855465888977\n",
      "Epoch: 0, Step: 142912, Loss: 0.7978373169898987\n",
      "Epoch: 0, Step: 142944, Loss: 0.8729844093322754\n",
      "Epoch: 0, Step: 142976, Loss: 0.8328871130943298\n",
      "Epoch: 0, Step: 143008, Loss: 0.8675551414489746\n",
      "Epoch: 0, Step: 143040, Loss: 0.7936837673187256\n",
      "Epoch: 0, Step: 143072, Loss: 0.7542335391044617\n",
      "Epoch: 0, Step: 143104, Loss: 0.6523342132568359\n",
      "Epoch: 0, Step: 143136, Loss: 0.8392807245254517\n",
      "Epoch: 0, Step: 143168, Loss: 0.8613078594207764\n",
      "Epoch: 0, Step: 143200, Loss: 0.7981518507003784\n",
      "Epoch: 0, Step: 143232, Loss: 0.8078747391700745\n",
      "Epoch: 0, Step: 143264, Loss: 0.7192972302436829\n",
      "Epoch: 0, Step: 143296, Loss: 0.7996593713760376\n",
      "Epoch: 0, Step: 143328, Loss: 0.7868056893348694\n",
      "Epoch: 0, Step: 143360, Loss: 0.8006274700164795\n",
      "Epoch: 0, Step: 143392, Loss: 0.8425869345664978\n",
      "Epoch: 0, Step: 143424, Loss: 0.7527654767036438\n",
      "Epoch: 0, Step: 143456, Loss: 0.8192279934883118\n",
      "Epoch: 0, Step: 143488, Loss: 0.8193295001983643\n",
      "Epoch: 0, Step: 143520, Loss: 0.710805356502533\n",
      "Epoch: 0, Step: 143552, Loss: 0.8392206430435181\n",
      "Epoch: 0, Step: 143584, Loss: 0.8447969555854797\n",
      "Epoch: 0, Step: 143616, Loss: 0.9299421310424805\n",
      "Epoch: 0, Step: 143648, Loss: 0.8862555623054504\n",
      "Epoch: 0, Step: 143680, Loss: 0.8520125150680542\n",
      "Epoch: 0, Step: 143712, Loss: 0.8844900727272034\n",
      "Epoch: 0, Step: 143744, Loss: 0.8360164165496826\n",
      "Epoch: 0, Step: 143776, Loss: 0.8086555004119873\n",
      "Epoch: 0, Step: 143808, Loss: 0.8302176594734192\n",
      "Epoch: 0, Step: 143840, Loss: 0.8265799283981323\n",
      "Epoch: 0, Step: 143872, Loss: 0.8931077718734741\n",
      "Epoch: 0, Step: 143904, Loss: 0.7599013447761536\n",
      "Epoch: 0, Step: 143936, Loss: 0.8734921216964722\n",
      "Epoch: 0, Step: 143968, Loss: 0.83055579662323\n",
      "Epoch: 0, Step: 144000, Loss: 0.8476055264472961\n",
      "Epoch: 0, Step: 144032, Loss: 0.817215621471405\n",
      "Epoch: 0, Step: 144064, Loss: 0.7778552770614624\n",
      "Epoch: 0, Step: 144096, Loss: 0.7897924780845642\n",
      "Epoch: 0, Step: 144128, Loss: 0.8469551205635071\n",
      "Epoch: 0, Step: 144160, Loss: 0.7908380031585693\n",
      "Epoch: 0, Step: 144192, Loss: 0.7648294568061829\n",
      "Epoch: 0, Step: 144224, Loss: 0.8453956842422485\n",
      "Epoch: 0, Step: 144256, Loss: 0.8608105778694153\n",
      "Epoch: 0, Step: 144288, Loss: 0.847223162651062\n",
      "Epoch: 0, Step: 144320, Loss: 0.7856324315071106\n",
      "Epoch: 0, Step: 144352, Loss: 0.7728806138038635\n",
      "Epoch: 0, Step: 144384, Loss: 0.8551948070526123\n",
      "Epoch: 0, Step: 144416, Loss: 0.8591771125793457\n",
      "Epoch: 0, Step: 144448, Loss: 0.8011391758918762\n",
      "Epoch: 0, Step: 144480, Loss: 0.7605820298194885\n",
      "Epoch: 0, Step: 144512, Loss: 0.7641421556472778\n",
      "Epoch: 0, Step: 144544, Loss: 0.8190178275108337\n",
      "Epoch: 0, Step: 144576, Loss: 0.7210257649421692\n",
      "Epoch: 0, Step: 144608, Loss: 0.8318072557449341\n",
      "Epoch: 0, Step: 144640, Loss: 0.786946713924408\n",
      "Epoch: 0, Step: 144672, Loss: 0.8177871108055115\n",
      "Epoch: 0, Step: 144704, Loss: 0.9047541618347168\n",
      "Epoch: 0, Step: 144736, Loss: 0.8130682706832886\n",
      "Epoch: 0, Step: 144768, Loss: 0.7531517148017883\n",
      "Epoch: 0, Step: 144800, Loss: 0.8521286249160767\n",
      "Epoch: 0, Step: 144832, Loss: 0.9604597687721252\n",
      "Epoch: 0, Step: 144864, Loss: 0.7756907939910889\n",
      "Epoch: 0, Step: 144896, Loss: 0.8109280467033386\n",
      "Epoch: 0, Step: 144928, Loss: 0.8012039661407471\n",
      "Epoch: 0, Step: 144960, Loss: 0.8492538332939148\n",
      "Epoch: 0, Step: 144992, Loss: 0.8257991671562195\n",
      "Epoch: 0, Step: 145024, Loss: 0.7631710171699524\n",
      "Epoch: 0, Step: 145056, Loss: 0.8361707925796509\n",
      "Epoch: 0, Step: 145088, Loss: 0.7987197041511536\n",
      "Epoch: 0, Step: 145120, Loss: 0.7701396346092224\n",
      "Epoch: 0, Step: 145152, Loss: 0.7693679928779602\n",
      "Epoch: 0, Step: 145184, Loss: 0.8581501245498657\n",
      "Epoch: 0, Step: 145216, Loss: 0.7720672488212585\n",
      "Epoch: 0, Step: 145248, Loss: 0.7796642184257507\n",
      "Epoch: 0, Step: 145280, Loss: 0.755148708820343\n",
      "Epoch: 0, Step: 145312, Loss: 0.825414776802063\n",
      "Epoch: 0, Step: 145344, Loss: 0.9012100100517273\n",
      "Epoch: 0, Step: 145376, Loss: 0.7342430949211121\n",
      "Epoch: 0, Step: 145408, Loss: 0.8232934474945068\n",
      "Epoch: 0, Step: 145440, Loss: 0.8428966999053955\n",
      "Epoch: 0, Step: 145472, Loss: 0.8237870931625366\n",
      "Epoch: 0, Step: 145504, Loss: 0.865514874458313\n",
      "Epoch: 0, Step: 145536, Loss: 0.821438193321228\n",
      "Epoch: 0, Step: 145568, Loss: 0.8396270275115967\n",
      "Epoch: 0, Step: 145600, Loss: 0.7597612738609314\n",
      "Epoch: 0, Step: 145632, Loss: 0.7950547337532043\n",
      "Epoch: 0, Step: 145664, Loss: 1.0008692741394043\n",
      "Epoch: 0, Step: 145696, Loss: 0.6942678093910217\n",
      "Epoch: 0, Step: 145728, Loss: 0.808447539806366\n",
      "Epoch: 0, Step: 145760, Loss: 0.8188537955284119\n",
      "Epoch: 0, Step: 145792, Loss: 0.7292684316635132\n",
      "Epoch: 0, Step: 145824, Loss: 0.8451347947120667\n",
      "Epoch: 0, Step: 145856, Loss: 0.7777001857757568\n",
      "Epoch: 0, Step: 145888, Loss: 0.7158415913581848\n",
      "Epoch: 0, Step: 145920, Loss: 0.8834649920463562\n",
      "Epoch: 0, Step: 145952, Loss: 0.8115714192390442\n",
      "Epoch: 0, Step: 145984, Loss: 0.8173865079879761\n",
      "Epoch: 0, Step: 146016, Loss: 0.7891625165939331\n",
      "Epoch: 0, Step: 146048, Loss: 0.7654150724411011\n",
      "Epoch: 0, Step: 146080, Loss: 0.6844258904457092\n",
      "Epoch: 0, Step: 146112, Loss: 0.8182550668716431\n",
      "Epoch: 0, Step: 146144, Loss: 0.8295164704322815\n",
      "Epoch: 0, Step: 146176, Loss: 0.7206431031227112\n",
      "Epoch: 0, Step: 146208, Loss: 0.8537100553512573\n",
      "Epoch: 0, Step: 146240, Loss: 0.8997240662574768\n",
      "Epoch: 0, Step: 146272, Loss: 0.8197320699691772\n",
      "Epoch: 0, Step: 146304, Loss: 0.8353793025016785\n",
      "Epoch: 0, Step: 146336, Loss: 0.8303871750831604\n",
      "Epoch: 0, Step: 146368, Loss: 0.8742700815200806\n",
      "Epoch: 0, Step: 146400, Loss: 0.8392089009284973\n",
      "Epoch: 0, Step: 146432, Loss: 0.8266224265098572\n",
      "Epoch: 0, Step: 146464, Loss: 0.7496885657310486\n",
      "Epoch: 0, Step: 146496, Loss: 0.875298798084259\n",
      "Epoch: 0, Step: 146528, Loss: 0.7297855615615845\n",
      "Epoch: 0, Step: 146560, Loss: 0.8087189793586731\n",
      "Epoch: 0, Step: 146592, Loss: 0.8170249462127686\n",
      "Epoch: 0, Step: 146624, Loss: 0.8074523210525513\n",
      "Epoch: 0, Step: 146656, Loss: 0.8256167769432068\n",
      "Epoch: 0, Step: 146688, Loss: 0.8890533447265625\n",
      "Epoch: 0, Step: 146720, Loss: 0.7837731838226318\n",
      "Epoch: 0, Step: 146752, Loss: 0.8476588726043701\n",
      "Epoch: 0, Step: 146784, Loss: 0.837842583656311\n",
      "Epoch: 0, Step: 146816, Loss: 0.7641779184341431\n",
      "Epoch: 0, Step: 146848, Loss: 0.7327860593795776\n",
      "Epoch: 0, Step: 146880, Loss: 0.7837089896202087\n",
      "Epoch: 0, Step: 146912, Loss: 0.8287724852561951\n",
      "Epoch: 0, Step: 146944, Loss: 0.8424524664878845\n",
      "Epoch: 0, Step: 146976, Loss: 0.8656994700431824\n",
      "Epoch: 0, Step: 147008, Loss: 0.7831850647926331\n",
      "Epoch: 0, Step: 147040, Loss: 0.7777215838432312\n",
      "Epoch: 0, Step: 147072, Loss: 0.8681138753890991\n",
      "Epoch: 0, Step: 147104, Loss: 0.8427830338478088\n",
      "Epoch: 0, Step: 147136, Loss: 0.789320707321167\n",
      "Epoch: 0, Step: 147168, Loss: 0.8119259476661682\n",
      "Epoch: 0, Step: 147200, Loss: 0.8783613443374634\n",
      "Epoch: 0, Step: 147232, Loss: 0.8338954448699951\n",
      "Epoch: 0, Step: 147264, Loss: 0.8400219678878784\n",
      "Epoch: 0, Step: 147296, Loss: 0.7361661195755005\n",
      "Epoch: 0, Step: 147328, Loss: 0.75200355052948\n",
      "Epoch: 0, Step: 147360, Loss: 0.7688060998916626\n",
      "Epoch: 0, Step: 147392, Loss: 0.8476163148880005\n",
      "Epoch: 0, Step: 147424, Loss: 0.7879152894020081\n",
      "Epoch: 0, Step: 147456, Loss: 0.7430916428565979\n",
      "Epoch: 0, Step: 147488, Loss: 0.7660524845123291\n",
      "Epoch: 0, Step: 147520, Loss: 0.8443664908409119\n",
      "Epoch: 0, Step: 147552, Loss: 0.9268077611923218\n",
      "Epoch: 0, Step: 147584, Loss: 0.7856006026268005\n",
      "Epoch: 0, Step: 147616, Loss: 0.8147245645523071\n",
      "Epoch: 0, Step: 147648, Loss: 0.7457162737846375\n",
      "Epoch: 0, Step: 147680, Loss: 0.7910498976707458\n",
      "Epoch: 0, Step: 147712, Loss: 0.8477970361709595\n",
      "Epoch: 0, Step: 147744, Loss: 0.7563906908035278\n",
      "Epoch: 0, Step: 147776, Loss: 0.8727139234542847\n",
      "Epoch: 0, Step: 147808, Loss: 0.8671677112579346\n",
      "Epoch: 0, Step: 147840, Loss: 0.8171588778495789\n",
      "Epoch: 0, Step: 147872, Loss: 0.809601902961731\n",
      "Epoch: 0, Step: 147904, Loss: 0.9531506299972534\n",
      "Epoch: 0, Step: 147936, Loss: 0.8977880477905273\n",
      "Epoch: 0, Step: 147968, Loss: 0.803044855594635\n",
      "Epoch: 0, Step: 148000, Loss: 0.7425448894500732\n",
      "Epoch: 0, Step: 148032, Loss: 0.757659375667572\n",
      "Epoch: 0, Step: 148064, Loss: 0.8495749235153198\n",
      "Epoch: 0, Step: 148096, Loss: 0.8737195730209351\n",
      "Epoch: 0, Step: 148128, Loss: 0.8401058316230774\n",
      "Epoch: 0, Step: 148160, Loss: 0.8710345029830933\n",
      "Epoch: 0, Step: 148192, Loss: 0.9208632111549377\n",
      "Epoch: 0, Step: 148224, Loss: 0.7840359210968018\n",
      "Epoch: 0, Step: 148256, Loss: 0.6922463178634644\n",
      "Epoch: 0, Step: 148288, Loss: 0.7606709003448486\n",
      "Epoch: 0, Step: 148320, Loss: 0.8382866382598877\n",
      "Epoch: 0, Step: 148352, Loss: 0.8867952823638916\n",
      "Epoch: 0, Step: 148384, Loss: 0.8148669004440308\n",
      "Epoch: 0, Step: 148416, Loss: 0.7846559286117554\n",
      "Epoch: 0, Step: 148448, Loss: 0.9310364723205566\n",
      "Epoch: 0, Step: 148480, Loss: 0.8851069211959839\n",
      "Epoch: 0, Step: 148512, Loss: 0.7867363095283508\n",
      "Epoch: 0, Step: 148544, Loss: 0.8163511157035828\n",
      "Epoch: 0, Step: 148576, Loss: 0.7255052924156189\n",
      "Epoch: 0, Step: 148608, Loss: 0.6967838406562805\n",
      "Epoch: 0, Step: 148640, Loss: 0.7534744739532471\n",
      "Epoch: 0, Step: 148672, Loss: 0.8399026393890381\n",
      "Epoch: 0, Step: 148704, Loss: 0.9862217903137207\n",
      "Epoch: 0, Step: 148736, Loss: 0.8028700351715088\n",
      "Epoch: 0, Step: 148768, Loss: 0.8381102681159973\n",
      "Epoch: 0, Step: 148800, Loss: 0.867963969707489\n",
      "Epoch: 0, Step: 148832, Loss: 0.772998571395874\n",
      "Epoch: 0, Step: 148864, Loss: 0.8434932231903076\n",
      "Epoch: 0, Step: 148896, Loss: 0.8817484974861145\n",
      "Epoch: 0, Step: 148928, Loss: 0.7937284111976624\n",
      "Epoch: 0, Step: 148960, Loss: 0.9760497808456421\n",
      "Epoch: 0, Step: 148992, Loss: 0.8011254668235779\n",
      "Epoch: 0, Step: 149024, Loss: 0.9539583921432495\n",
      "Epoch: 0, Step: 149056, Loss: 0.8237439393997192\n",
      "Epoch: 0, Step: 149088, Loss: 0.8591235280036926\n",
      "Epoch: 0, Step: 149120, Loss: 0.7258724570274353\n",
      "Epoch: 0, Step: 149152, Loss: 0.9271557927131653\n",
      "Epoch: 0, Step: 149184, Loss: 0.8916632533073425\n",
      "Epoch: 0, Step: 149216, Loss: 0.7612642645835876\n",
      "Epoch: 0, Step: 149248, Loss: 0.8200604915618896\n",
      "Epoch: 0, Step: 149280, Loss: 0.7988676428794861\n",
      "Epoch: 0, Step: 149312, Loss: 0.9121313691139221\n",
      "Epoch: 0, Step: 149344, Loss: 0.9382914304733276\n",
      "Epoch: 0, Step: 149376, Loss: 0.9653143882751465\n",
      "Epoch: 0, Step: 149408, Loss: 0.8304898142814636\n",
      "Epoch: 0, Step: 149440, Loss: 0.8930101990699768\n",
      "Epoch: 0, Step: 149472, Loss: 0.708046555519104\n",
      "Epoch: 0, Step: 149504, Loss: 0.8712168335914612\n",
      "Epoch: 0, Step: 149536, Loss: 0.8932044506072998\n",
      "Epoch: 0, Step: 149568, Loss: 0.8960556983947754\n",
      "Epoch: 0, Step: 149600, Loss: 0.8462556600570679\n",
      "Epoch: 0, Step: 149632, Loss: 0.7724018692970276\n",
      "Epoch: 0, Step: 149664, Loss: 0.9518380165100098\n",
      "Epoch: 0, Step: 149696, Loss: 0.7973576188087463\n",
      "Epoch: 0, Step: 149728, Loss: 0.9071497917175293\n",
      "Epoch: 0, Step: 149760, Loss: 0.7494800090789795\n",
      "Epoch: 0, Step: 149792, Loss: 0.8002843260765076\n",
      "Epoch: 0, Step: 149824, Loss: 0.7754111886024475\n",
      "Epoch: 0, Step: 149856, Loss: 0.7544189691543579\n",
      "Epoch: 0, Step: 149888, Loss: 0.8068146109580994\n",
      "Epoch: 0, Step: 149920, Loss: 0.8790216445922852\n",
      "Epoch: 0, Step: 149952, Loss: 0.7661283016204834\n",
      "Epoch: 0, Step: 149984, Loss: 0.8403543829917908\n",
      "Epoch: 0, Step: 150016, Loss: 0.7630468606948853\n",
      "Epoch: 0, Step: 150048, Loss: 0.9332660436630249\n",
      "Epoch: 0, Step: 150080, Loss: 0.7948901653289795\n",
      "Epoch: 0, Step: 150112, Loss: 0.9118831753730774\n",
      "Epoch: 0, Step: 150144, Loss: 0.9003207683563232\n",
      "Epoch: 0, Step: 150176, Loss: 0.8215307593345642\n",
      "Epoch: 0, Step: 150208, Loss: 0.7326586842536926\n",
      "Epoch: 0, Step: 150240, Loss: 0.8788335919380188\n",
      "Epoch: 0, Step: 150272, Loss: 0.8394192457199097\n",
      "Epoch: 0, Step: 150304, Loss: 0.7072619199752808\n",
      "Epoch: 0, Step: 150336, Loss: 0.8077487349510193\n",
      "Epoch: 0, Step: 150368, Loss: 0.8358271718025208\n",
      "Epoch: 0, Step: 150400, Loss: 0.804182231426239\n",
      "Epoch: 0, Step: 150432, Loss: 0.8551198840141296\n",
      "Epoch: 0, Step: 150464, Loss: 0.8695542812347412\n",
      "Epoch: 0, Step: 150496, Loss: 0.7460417151451111\n",
      "Epoch: 0, Step: 150528, Loss: 0.8809715509414673\n",
      "Epoch: 0, Step: 150560, Loss: 0.8244772553443909\n",
      "Epoch: 0, Step: 150592, Loss: 0.825802206993103\n",
      "Epoch: 0, Step: 150624, Loss: 0.8126815557479858\n",
      "Epoch: 0, Step: 150656, Loss: 0.8489075303077698\n",
      "Epoch: 0, Step: 150688, Loss: 0.8238765597343445\n",
      "Epoch: 0, Step: 150720, Loss: 0.8235090374946594\n",
      "Epoch: 0, Step: 150752, Loss: 0.8250580430030823\n",
      "Epoch: 0, Step: 150784, Loss: 0.7545951008796692\n",
      "Epoch: 0, Step: 150816, Loss: 0.7873290777206421\n",
      "Epoch: 0, Step: 150848, Loss: 0.7713070511817932\n",
      "Epoch: 0, Step: 150880, Loss: 0.7877336144447327\n",
      "Epoch: 0, Step: 150912, Loss: 0.900699257850647\n",
      "Epoch: 0, Step: 150944, Loss: 0.8610382676124573\n",
      "Epoch: 0, Step: 150976, Loss: 0.7837022542953491\n",
      "Epoch: 0, Step: 151008, Loss: 0.8347232341766357\n",
      "Epoch: 0, Step: 151040, Loss: 0.9113451242446899\n",
      "Epoch: 0, Step: 151072, Loss: 0.7145578265190125\n",
      "Epoch: 0, Step: 151104, Loss: 0.8986342549324036\n",
      "Epoch: 0, Step: 151136, Loss: 0.8177839517593384\n",
      "Epoch: 0, Step: 151168, Loss: 0.7284632325172424\n",
      "Epoch: 0, Step: 151200, Loss: 0.8336649537086487\n",
      "Epoch: 0, Step: 151232, Loss: 0.8624178767204285\n",
      "Epoch: 0, Step: 151264, Loss: 0.8096052408218384\n",
      "Epoch: 0, Step: 151296, Loss: 0.8367913365364075\n",
      "Epoch: 0, Step: 151328, Loss: 0.8691641092300415\n",
      "Epoch: 0, Step: 151360, Loss: 0.8123525977134705\n",
      "Epoch: 0, Step: 151392, Loss: 0.8269897699356079\n",
      "Epoch: 0, Step: 151424, Loss: 0.748716413974762\n",
      "Epoch: 0, Step: 151456, Loss: 0.8533447980880737\n",
      "Epoch: 0, Step: 151488, Loss: 0.7568220496177673\n",
      "Epoch: 0, Step: 151520, Loss: 0.7800140976905823\n",
      "Epoch: 0, Step: 151552, Loss: 0.8693578243255615\n",
      "Epoch: 0, Step: 151584, Loss: 0.7066878080368042\n",
      "Epoch: 0, Step: 151616, Loss: 0.8902931213378906\n",
      "Epoch: 0, Step: 151648, Loss: 0.8118758797645569\n",
      "Epoch: 0, Step: 151680, Loss: 0.6888977885246277\n",
      "Epoch: 0, Step: 151712, Loss: 0.8351962566375732\n",
      "Epoch: 0, Step: 151744, Loss: 0.8396033048629761\n",
      "Epoch: 0, Step: 151776, Loss: 0.882400393486023\n",
      "Epoch: 0, Step: 151808, Loss: 0.7656009197235107\n",
      "Epoch: 0, Step: 151840, Loss: 0.9025594592094421\n",
      "Epoch: 0, Step: 151872, Loss: 0.8634616732597351\n",
      "Epoch: 0, Step: 151904, Loss: 0.8152510523796082\n",
      "Epoch: 0, Step: 151936, Loss: 0.734526515007019\n",
      "Epoch: 0, Step: 151968, Loss: 0.6950893998146057\n",
      "Epoch: 0, Step: 152000, Loss: 0.8700124621391296\n",
      "Epoch: 0, Step: 152032, Loss: 0.8450076580047607\n",
      "Epoch: 0, Step: 152064, Loss: 0.8119038343429565\n",
      "Epoch: 0, Step: 152096, Loss: 0.7893478870391846\n",
      "Epoch: 0, Step: 152128, Loss: 0.8471449613571167\n",
      "Epoch: 0, Step: 152160, Loss: 0.7837429642677307\n",
      "Epoch: 0, Step: 152192, Loss: 0.7985025644302368\n",
      "Epoch: 0, Step: 152224, Loss: 0.9609854817390442\n",
      "Epoch: 0, Step: 152256, Loss: 0.9346714019775391\n",
      "Epoch: 0, Step: 152288, Loss: 0.7438508868217468\n",
      "Epoch: 0, Step: 152320, Loss: 0.7309895157814026\n",
      "Epoch: 0, Step: 152352, Loss: 0.8266887068748474\n",
      "Epoch: 0, Step: 152384, Loss: 0.7591692209243774\n",
      "Epoch: 0, Step: 152416, Loss: 0.8750388622283936\n",
      "Epoch: 0, Step: 152448, Loss: 0.7675572037696838\n",
      "Epoch: 0, Step: 152480, Loss: 0.8344654440879822\n",
      "Epoch: 0, Step: 152512, Loss: 0.8340338468551636\n",
      "Epoch: 0, Step: 152544, Loss: 0.8316076993942261\n",
      "Epoch: 0, Step: 152576, Loss: 0.8542119264602661\n",
      "Epoch: 0, Step: 152608, Loss: 0.784378707408905\n",
      "Epoch: 0, Step: 152640, Loss: 0.87629234790802\n",
      "Epoch: 0, Step: 152672, Loss: 0.7434865832328796\n",
      "Epoch: 0, Step: 152704, Loss: 0.9403401017189026\n",
      "Epoch: 0, Step: 152736, Loss: 0.8954857587814331\n",
      "Epoch: 0, Step: 152768, Loss: 0.7316324710845947\n",
      "Epoch: 0, Step: 152800, Loss: 0.8516099452972412\n",
      "Epoch: 0, Step: 152832, Loss: 0.8642376661300659\n",
      "Epoch: 0, Step: 152864, Loss: 0.9357892274856567\n",
      "Epoch: 0, Step: 152896, Loss: 0.8081735968589783\n",
      "Epoch: 0, Step: 152928, Loss: 0.8549422025680542\n",
      "Epoch: 0, Step: 152960, Loss: 0.7815005779266357\n",
      "Epoch: 0, Step: 152992, Loss: 0.899492084980011\n",
      "Epoch: 0, Step: 153024, Loss: 0.8653795719146729\n",
      "Epoch: 0, Step: 153056, Loss: 0.842608630657196\n",
      "Epoch: 0, Step: 153088, Loss: 0.8079319596290588\n",
      "Epoch: 0, Step: 153120, Loss: 0.7786270976066589\n",
      "Epoch: 0, Step: 153152, Loss: 0.8402925729751587\n",
      "Epoch: 0, Step: 153184, Loss: 0.7117868065834045\n",
      "Epoch: 0, Step: 153216, Loss: 0.8306136727333069\n",
      "Epoch: 0, Step: 153248, Loss: 0.7844563126564026\n",
      "Epoch: 0, Step: 153280, Loss: 0.8538752198219299\n",
      "Epoch: 0, Step: 153312, Loss: 0.8234862089157104\n",
      "Epoch: 0, Step: 153344, Loss: 0.8636724352836609\n",
      "Epoch: 0, Step: 153376, Loss: 0.8975750207901001\n",
      "Epoch: 0, Step: 153408, Loss: 0.8946658968925476\n",
      "Epoch: 0, Step: 153440, Loss: 0.7765602469444275\n",
      "Epoch: 0, Step: 153472, Loss: 0.7373871207237244\n",
      "Epoch: 0, Step: 153504, Loss: 0.7317835688591003\n",
      "Epoch: 0, Step: 153536, Loss: 0.8246801495552063\n",
      "Epoch: 0, Step: 153568, Loss: 0.7877600789070129\n",
      "Epoch: 0, Step: 153600, Loss: 0.8858186602592468\n",
      "Epoch: 0, Step: 153632, Loss: 0.8649349212646484\n",
      "Epoch: 0, Step: 153664, Loss: 0.8618732690811157\n",
      "Epoch: 0, Step: 153696, Loss: 0.8385373950004578\n",
      "Epoch: 0, Step: 153728, Loss: 0.8336871266365051\n",
      "Epoch: 0, Step: 153760, Loss: 0.8945460319519043\n",
      "Epoch: 0, Step: 153792, Loss: 0.8952882289886475\n",
      "Epoch: 0, Step: 153824, Loss: 0.9198254346847534\n",
      "Epoch: 0, Step: 153856, Loss: 0.8615602254867554\n",
      "Epoch: 0, Step: 153888, Loss: 0.7758362293243408\n",
      "Epoch: 0, Step: 153920, Loss: 0.7082626223564148\n",
      "Epoch: 0, Step: 153952, Loss: 0.8092840909957886\n",
      "Epoch: 0, Step: 153984, Loss: 0.7961644530296326\n",
      "Epoch: 0, Step: 154016, Loss: 0.854688286781311\n",
      "Epoch: 0, Step: 154048, Loss: 0.8294975757598877\n",
      "Epoch: 0, Step: 154080, Loss: 0.8688054084777832\n",
      "Epoch: 0, Step: 154112, Loss: 0.7494677901268005\n",
      "Epoch: 0, Step: 154144, Loss: 0.7231667041778564\n",
      "Epoch: 0, Step: 154176, Loss: 0.8147420287132263\n",
      "Epoch: 0, Step: 154208, Loss: 0.7560983300209045\n",
      "Epoch: 0, Step: 154240, Loss: 0.8002708554267883\n",
      "Epoch: 0, Step: 154272, Loss: 0.7358670830726624\n",
      "Epoch: 0, Step: 154304, Loss: 0.8609879612922668\n",
      "Epoch: 0, Step: 154336, Loss: 0.7623611092567444\n",
      "Epoch: 0, Step: 154368, Loss: 0.8787010908126831\n",
      "Epoch: 0, Step: 154400, Loss: 0.8334417939186096\n",
      "Epoch: 0, Step: 154432, Loss: 0.8424156904220581\n",
      "Epoch: 0, Step: 154464, Loss: 0.6992520093917847\n",
      "Epoch: 0, Step: 154496, Loss: 0.8341645002365112\n",
      "Epoch: 0, Step: 154528, Loss: 0.815432608127594\n",
      "Epoch: 0, Step: 154560, Loss: 0.8213569521903992\n",
      "Epoch: 0, Step: 154592, Loss: 0.7520219683647156\n",
      "Epoch: 0, Step: 154624, Loss: 0.8530600070953369\n",
      "Epoch: 0, Step: 154656, Loss: 0.8310269117355347\n",
      "Epoch: 0, Step: 154688, Loss: 0.8085161447525024\n",
      "Epoch: 0, Step: 154720, Loss: 0.8970580101013184\n",
      "Epoch: 0, Step: 154752, Loss: 0.8363906741142273\n",
      "Epoch: 0, Step: 154784, Loss: 0.8396058678627014\n",
      "Epoch: 0, Step: 154816, Loss: 0.8520687818527222\n",
      "Epoch: 0, Step: 154848, Loss: 0.7669951915740967\n",
      "Epoch: 0, Step: 154880, Loss: 0.8272395730018616\n",
      "Epoch: 0, Step: 154912, Loss: 0.825096070766449\n",
      "Epoch: 0, Step: 154944, Loss: 0.8607635498046875\n",
      "Epoch: 0, Step: 154976, Loss: 0.7714817523956299\n",
      "Epoch: 0, Step: 155008, Loss: 0.78192538022995\n",
      "Epoch: 0, Step: 155040, Loss: 0.8521419763565063\n",
      "Epoch: 0, Step: 155072, Loss: 0.898837149143219\n",
      "Epoch: 0, Step: 155104, Loss: 0.7900603413581848\n",
      "Epoch: 0, Step: 155136, Loss: 0.72779780626297\n",
      "Epoch: 0, Step: 155168, Loss: 0.7529960870742798\n",
      "Epoch: 0, Step: 155200, Loss: 0.8091152906417847\n",
      "Epoch: 0, Step: 155232, Loss: 0.866229772567749\n",
      "Epoch: 0, Step: 155264, Loss: 0.7349115610122681\n",
      "Epoch: 0, Step: 155296, Loss: 0.8148977160453796\n",
      "Epoch: 0, Step: 155328, Loss: 0.8255453705787659\n",
      "Epoch: 0, Step: 155360, Loss: 0.9431703090667725\n",
      "Epoch: 0, Step: 155392, Loss: 0.7833812832832336\n",
      "Epoch: 0, Step: 155424, Loss: 0.7862024903297424\n",
      "Epoch: 0, Step: 155456, Loss: 0.7554051876068115\n",
      "Epoch: 0, Step: 155488, Loss: 0.8441163897514343\n",
      "Epoch: 0, Step: 155520, Loss: 0.8093096017837524\n",
      "Epoch: 0, Step: 155552, Loss: 0.7708949446678162\n",
      "Epoch: 0, Step: 155584, Loss: 0.7996159195899963\n",
      "Epoch: 0, Step: 155616, Loss: 0.8763052821159363\n",
      "Epoch: 0, Step: 155648, Loss: 0.780735433101654\n",
      "Epoch: 0, Step: 155680, Loss: 0.8008671402931213\n",
      "Epoch: 0, Step: 155712, Loss: 0.8961670994758606\n",
      "Epoch: 0, Step: 155744, Loss: 0.8080217242240906\n",
      "Epoch: 0, Step: 155776, Loss: 0.7558965086936951\n",
      "Epoch: 0, Step: 155808, Loss: 0.827587366104126\n",
      "Epoch: 0, Step: 155840, Loss: 0.8398970365524292\n",
      "Epoch: 0, Step: 155872, Loss: 0.7570862174034119\n",
      "Epoch: 0, Step: 155904, Loss: 0.7917349934577942\n",
      "Epoch: 0, Step: 155936, Loss: 0.7617742419242859\n",
      "Epoch: 0, Step: 155968, Loss: 0.8670850992202759\n",
      "Epoch: 0, Step: 156000, Loss: 0.8595901727676392\n",
      "Epoch: 0, Step: 156032, Loss: 0.746905505657196\n",
      "Epoch: 0, Step: 156064, Loss: 0.7831243872642517\n",
      "Epoch: 0, Step: 156096, Loss: 0.8723751902580261\n",
      "Epoch: 0, Step: 156128, Loss: 0.8027656078338623\n",
      "Epoch: 0, Step: 156160, Loss: 0.8592446446418762\n",
      "Epoch: 0, Step: 156192, Loss: 0.8204514384269714\n",
      "Epoch: 0, Step: 156224, Loss: 0.7954415678977966\n",
      "Epoch: 0, Step: 156256, Loss: 0.8306538462638855\n",
      "Epoch: 0, Step: 156288, Loss: 0.8594187498092651\n",
      "Epoch: 0, Step: 156320, Loss: 0.8045958280563354\n",
      "Epoch: 0, Step: 156352, Loss: 0.7885944247245789\n",
      "Epoch: 0, Step: 156384, Loss: 0.802183985710144\n",
      "Epoch: 0, Step: 156416, Loss: 0.7349622845649719\n",
      "Epoch: 0, Step: 156448, Loss: 0.6925185322761536\n",
      "Epoch: 0, Step: 156480, Loss: 0.7840340733528137\n",
      "Epoch: 0, Step: 156512, Loss: 0.8681219816207886\n",
      "Epoch: 0, Step: 156544, Loss: 0.8611085414886475\n",
      "Epoch: 0, Step: 156576, Loss: 0.9279572367668152\n",
      "Epoch: 0, Step: 156608, Loss: 0.7642318606376648\n",
      "Epoch: 0, Step: 156640, Loss: 0.9178453683853149\n",
      "Epoch: 0, Step: 156672, Loss: 0.801502525806427\n",
      "Epoch: 0, Step: 156704, Loss: 0.815500020980835\n",
      "Epoch: 0, Step: 156736, Loss: 0.8596684336662292\n",
      "Epoch: 0, Step: 156768, Loss: 0.8069623112678528\n",
      "Epoch: 0, Step: 156800, Loss: 0.815981924533844\n",
      "Epoch: 0, Step: 156832, Loss: 0.934178352355957\n",
      "Epoch: 0, Step: 156864, Loss: 0.7761486768722534\n",
      "Epoch: 0, Step: 156896, Loss: 0.8560758829116821\n",
      "Epoch: 0, Step: 156928, Loss: 0.9227393269538879\n",
      "Epoch: 0, Step: 156960, Loss: 0.9349592924118042\n",
      "Epoch: 0, Step: 156992, Loss: 0.8510840535163879\n",
      "Epoch: 0, Step: 157024, Loss: 0.772629976272583\n",
      "Epoch: 0, Step: 157056, Loss: 0.7937179803848267\n",
      "Epoch: 0, Step: 157088, Loss: 0.8713791966438293\n",
      "Epoch: 0, Step: 157120, Loss: 0.84779292345047\n",
      "Epoch: 0, Step: 157152, Loss: 0.8898823857307434\n",
      "Epoch: 0, Step: 157184, Loss: 0.7479689717292786\n",
      "Epoch: 0, Step: 157216, Loss: 0.8858208656311035\n",
      "Epoch: 0, Step: 157248, Loss: 0.8065665364265442\n",
      "Epoch: 0, Step: 157280, Loss: 0.773766815662384\n",
      "Epoch: 0, Step: 157312, Loss: 0.8358857035636902\n",
      "Epoch: 0, Step: 157344, Loss: 0.8551964163780212\n",
      "Epoch: 0, Step: 157376, Loss: 0.8678852319717407\n",
      "Epoch: 0, Step: 157408, Loss: 0.8165654540061951\n",
      "Epoch: 0, Step: 157440, Loss: 0.8087427020072937\n",
      "Epoch: 0, Step: 157472, Loss: 0.788803219795227\n",
      "Epoch: 0, Step: 157504, Loss: 0.8661492466926575\n",
      "Epoch: 0, Step: 157536, Loss: 0.9199376702308655\n",
      "Epoch: 0, Step: 157568, Loss: 0.8106065988540649\n",
      "Epoch: 0, Step: 157600, Loss: 0.7078860402107239\n",
      "Epoch: 0, Step: 157632, Loss: 0.7717166543006897\n",
      "Epoch: 0, Step: 157664, Loss: 0.7977955937385559\n",
      "Epoch: 0, Step: 157696, Loss: 0.7380255460739136\n",
      "Epoch: 0, Step: 157728, Loss: 0.7587810158729553\n",
      "Epoch: 0, Step: 157760, Loss: 0.8225062489509583\n",
      "Epoch: 0, Step: 157792, Loss: 0.8351399898529053\n",
      "Epoch: 0, Step: 157824, Loss: 0.7733648419380188\n",
      "Epoch: 0, Step: 157856, Loss: 0.9271393418312073\n",
      "Epoch: 0, Step: 157888, Loss: 0.771718442440033\n",
      "Epoch: 0, Step: 157920, Loss: 0.8275454044342041\n",
      "Epoch: 0, Step: 157952, Loss: 0.7615030407905579\n",
      "Epoch: 0, Step: 157984, Loss: 0.8345528244972229\n",
      "Epoch: 0, Step: 158016, Loss: 0.8357616662979126\n",
      "Epoch: 0, Step: 158048, Loss: 0.8116989731788635\n",
      "Epoch: 0, Step: 158080, Loss: 0.8985093235969543\n",
      "Epoch: 0, Step: 158112, Loss: 0.8089276552200317\n",
      "Epoch: 0, Step: 158144, Loss: 0.8121272921562195\n",
      "Epoch: 0, Step: 158176, Loss: 0.8279488682746887\n",
      "Epoch: 0, Step: 158208, Loss: 0.8653528094291687\n",
      "Epoch: 0, Step: 158240, Loss: 0.7988274693489075\n",
      "Epoch: 0, Step: 158272, Loss: 0.8324074149131775\n",
      "Epoch: 0, Step: 158304, Loss: 0.8202784657478333\n",
      "Epoch: 0, Step: 158336, Loss: 0.809090793132782\n",
      "Epoch: 0, Step: 158368, Loss: 0.8170868158340454\n",
      "Epoch: 0, Step: 158400, Loss: 0.8777918815612793\n",
      "Epoch: 0, Step: 158432, Loss: 0.796980082988739\n",
      "Epoch: 0, Step: 158464, Loss: 0.8659499287605286\n",
      "Epoch: 0, Step: 158496, Loss: 0.8368387818336487\n",
      "Epoch: 0, Step: 158528, Loss: 0.7405551075935364\n",
      "Epoch: 0, Step: 158560, Loss: 0.7842738032341003\n",
      "Epoch: 0, Step: 158592, Loss: 0.7898046374320984\n",
      "Epoch: 0, Step: 158624, Loss: 0.8956671953201294\n",
      "Epoch: 0, Step: 158656, Loss: 0.7983623743057251\n",
      "Epoch: 0, Step: 158688, Loss: 0.9099157452583313\n",
      "Epoch: 0, Step: 158720, Loss: 0.8588348031044006\n",
      "Epoch: 0, Step: 158752, Loss: 0.8464010953903198\n",
      "Epoch: 0, Step: 158784, Loss: 0.8471928238868713\n",
      "Epoch: 0, Step: 158816, Loss: 0.8224400877952576\n",
      "Epoch: 0, Step: 158848, Loss: 0.872284471988678\n",
      "Epoch: 0, Step: 158880, Loss: 0.8687677979469299\n",
      "Epoch: 0, Step: 158912, Loss: 0.8352067470550537\n",
      "Epoch: 0, Step: 158944, Loss: 0.8251235485076904\n",
      "Epoch: 0, Step: 158976, Loss: 0.8572655916213989\n",
      "Epoch: 0, Step: 159008, Loss: 0.8554159998893738\n",
      "Epoch: 0, Step: 159040, Loss: 0.8069347739219666\n",
      "Epoch: 0, Step: 159072, Loss: 0.7582047581672668\n",
      "Epoch: 0, Step: 159104, Loss: 0.8153448700904846\n",
      "Epoch: 0, Step: 159136, Loss: 0.8481894731521606\n",
      "Epoch: 0, Step: 159168, Loss: 0.8267513513565063\n",
      "Epoch: 0, Step: 159200, Loss: 0.9036305546760559\n",
      "Epoch: 0, Step: 159232, Loss: 0.9013538956642151\n",
      "Epoch: 0, Step: 159264, Loss: 0.728107213973999\n",
      "Epoch: 0, Step: 159296, Loss: 0.8568274974822998\n",
      "Epoch: 0, Step: 159328, Loss: 0.7655013203620911\n",
      "Epoch: 0, Step: 159360, Loss: 0.8945618867874146\n",
      "Epoch: 0, Step: 159392, Loss: 0.7749470472335815\n",
      "Epoch: 0, Step: 159424, Loss: 0.8165268301963806\n",
      "Epoch: 0, Step: 159456, Loss: 0.7823846340179443\n",
      "Epoch: 0, Step: 159488, Loss: 0.8656339645385742\n",
      "Epoch: 0, Step: 159520, Loss: 0.7941479086875916\n",
      "Epoch: 0, Step: 159552, Loss: 0.750386118888855\n",
      "Epoch: 0, Step: 159584, Loss: 0.8434776663780212\n",
      "Epoch: 0, Step: 159616, Loss: 0.7881166338920593\n",
      "Epoch: 0, Step: 159648, Loss: 0.7565958499908447\n",
      "Epoch: 0, Step: 159680, Loss: 0.8666743040084839\n",
      "Epoch: 0, Step: 159712, Loss: 0.9632313847541809\n",
      "Epoch: 0, Step: 159744, Loss: 0.7038544416427612\n",
      "Epoch: 0, Step: 159776, Loss: 0.8107222318649292\n",
      "Epoch: 0, Step: 159808, Loss: 0.8637028336524963\n",
      "Epoch: 0, Step: 159840, Loss: 0.8636415600776672\n",
      "Epoch: 0, Step: 159872, Loss: 0.9005284905433655\n",
      "Epoch: 0, Step: 159904, Loss: 0.7840410470962524\n",
      "Epoch: 0, Step: 159936, Loss: 0.8823214769363403\n",
      "Epoch: 0, Step: 159968, Loss: 0.8194982409477234\n",
      "Epoch: 0, Step: 160000, Loss: 0.8542528748512268\n",
      "Epoch: 0, Step: 160032, Loss: 0.6507673263549805\n",
      "Epoch: 0, Step: 160064, Loss: 0.7316541075706482\n",
      "Epoch: 0, Step: 160096, Loss: 0.8976523876190186\n",
      "Epoch: 0, Step: 160128, Loss: 0.8892258405685425\n",
      "Epoch: 0, Step: 160160, Loss: 0.81588214635849\n",
      "Epoch: 0, Step: 160192, Loss: 0.8369208574295044\n",
      "Epoch: 0, Step: 160224, Loss: 0.9139454364776611\n",
      "Epoch: 0, Step: 160256, Loss: 0.7537203431129456\n",
      "Epoch: 0, Step: 160288, Loss: 0.8107020258903503\n",
      "Epoch: 0, Step: 160320, Loss: 0.8062936067581177\n",
      "Epoch: 0, Step: 160352, Loss: 0.8704838752746582\n",
      "Epoch: 0, Step: 160384, Loss: 0.8371655941009521\n",
      "Epoch: 0, Step: 160416, Loss: 0.815782904624939\n",
      "Epoch: 0, Step: 160448, Loss: 0.6943941712379456\n",
      "Epoch: 0, Step: 160480, Loss: 0.9435306787490845\n",
      "Epoch: 0, Step: 160512, Loss: 0.8188799023628235\n",
      "Epoch: 0, Step: 160544, Loss: 0.8736922740936279\n",
      "Epoch: 0, Step: 160576, Loss: 0.9347954392433167\n",
      "Epoch: 0, Step: 160608, Loss: 0.8854230642318726\n",
      "Epoch: 0, Step: 160640, Loss: 0.842462420463562\n",
      "Epoch: 0, Step: 160672, Loss: 0.7127018570899963\n",
      "Epoch: 0, Step: 160704, Loss: 0.8344250321388245\n",
      "Epoch: 0, Step: 160736, Loss: 0.8152839541435242\n",
      "Epoch: 0, Step: 160768, Loss: 0.7638694643974304\n",
      "Epoch: 0, Step: 160800, Loss: 0.8625076413154602\n",
      "Epoch: 0, Step: 160832, Loss: 0.8666713833808899\n",
      "Epoch: 0, Step: 160864, Loss: 0.6919966340065002\n",
      "Epoch: 0, Step: 160896, Loss: 0.7379701137542725\n",
      "Epoch: 0, Step: 160928, Loss: 0.8477827310562134\n",
      "Epoch: 0, Step: 160960, Loss: 0.8987292051315308\n",
      "Epoch: 0, Step: 160992, Loss: 0.860974133014679\n",
      "Epoch: 0, Step: 161024, Loss: 0.7906529307365417\n",
      "Epoch: 0, Step: 161056, Loss: 0.7936527132987976\n",
      "Epoch: 0, Step: 161088, Loss: 0.8400649428367615\n",
      "Epoch: 0, Step: 161120, Loss: 0.797876238822937\n",
      "Epoch: 0, Step: 161152, Loss: 0.6739548444747925\n",
      "Epoch: 0, Step: 161184, Loss: 0.7825948596000671\n",
      "Epoch: 0, Step: 161216, Loss: 0.7828369736671448\n",
      "Epoch: 0, Step: 161248, Loss: 0.8014869093894958\n",
      "Epoch: 0, Step: 161280, Loss: 0.9345898628234863\n",
      "Epoch: 0, Step: 161312, Loss: 0.7633870840072632\n",
      "Epoch: 0, Step: 161344, Loss: 0.8601956963539124\n",
      "Epoch: 0, Step: 161376, Loss: 0.8632156252861023\n",
      "Epoch: 0, Step: 161408, Loss: 0.7462298274040222\n",
      "Epoch: 0, Step: 161440, Loss: 0.7748725414276123\n",
      "Epoch: 0, Step: 161472, Loss: 0.9020119309425354\n",
      "Epoch: 0, Step: 161504, Loss: 0.832589328289032\n",
      "Epoch: 0, Step: 161536, Loss: 0.8776329755783081\n",
      "Epoch: 0, Step: 161568, Loss: 0.8931229710578918\n",
      "Epoch: 0, Step: 161600, Loss: 0.8696773648262024\n",
      "Epoch: 0, Step: 161632, Loss: 0.7163388729095459\n",
      "Epoch: 0, Step: 161664, Loss: 0.7255222201347351\n",
      "Epoch: 0, Step: 161696, Loss: 0.7829353213310242\n",
      "Epoch: 0, Step: 161728, Loss: 0.8698928356170654\n",
      "Epoch: 0, Step: 161760, Loss: 0.7818887829780579\n",
      "Epoch: 0, Step: 161792, Loss: 0.8107044100761414\n",
      "Epoch: 0, Step: 161824, Loss: 0.7293272614479065\n",
      "Epoch: 0, Step: 161856, Loss: 0.8432741761207581\n",
      "Epoch: 0, Step: 161888, Loss: 0.8639827370643616\n",
      "Epoch: 0, Step: 161920, Loss: 0.8667365908622742\n",
      "Epoch: 0, Step: 161952, Loss: 0.7643053531646729\n",
      "Epoch: 0, Step: 161984, Loss: 0.7853701710700989\n",
      "Epoch: 0, Step: 162016, Loss: 0.8590954542160034\n",
      "Epoch: 0, Step: 162048, Loss: 0.8520182967185974\n",
      "Epoch: 0, Step: 162080, Loss: 0.898507297039032\n",
      "Epoch: 0, Step: 162112, Loss: 0.7614878416061401\n",
      "Epoch: 0, Step: 162144, Loss: 0.7226597666740417\n",
      "Epoch: 0, Step: 162176, Loss: 0.7754479050636292\n",
      "Epoch: 0, Step: 162208, Loss: 0.8266692757606506\n",
      "Epoch: 0, Step: 162240, Loss: 0.7223635315895081\n",
      "Epoch: 0, Step: 162272, Loss: 0.8843937516212463\n",
      "Epoch: 0, Step: 162304, Loss: 0.7917904257774353\n",
      "Epoch: 0, Step: 162336, Loss: 0.8839467167854309\n",
      "Epoch: 0, Step: 162368, Loss: 0.7885950803756714\n",
      "Epoch: 0, Step: 162400, Loss: 0.8870813250541687\n",
      "Epoch: 0, Step: 162432, Loss: 0.8950846791267395\n",
      "Epoch: 0, Step: 162464, Loss: 0.8238329291343689\n",
      "Epoch: 0, Step: 162496, Loss: 0.8158609867095947\n",
      "Epoch: 0, Step: 162528, Loss: 0.776971697807312\n",
      "Epoch: 0, Step: 162560, Loss: 0.8366745710372925\n",
      "Epoch: 0, Step: 162592, Loss: 0.8689090609550476\n",
      "Epoch: 0, Step: 162624, Loss: 0.7611547112464905\n",
      "Epoch: 0, Step: 162656, Loss: 0.8055228590965271\n",
      "Epoch: 0, Step: 162688, Loss: 0.7777511477470398\n",
      "Epoch: 0, Step: 162720, Loss: 0.7914694547653198\n",
      "Epoch: 0, Step: 162752, Loss: 0.8831244111061096\n",
      "Epoch: 0, Step: 162784, Loss: 0.846713662147522\n",
      "Epoch: 0, Step: 162816, Loss: 0.8246227502822876\n",
      "Epoch: 0, Step: 162848, Loss: 0.812834620475769\n",
      "Epoch: 0, Step: 162880, Loss: 0.702468752861023\n",
      "Epoch: 0, Step: 162912, Loss: 0.8080839514732361\n",
      "Epoch: 0, Step: 162944, Loss: 0.8066151738166809\n",
      "Epoch: 0, Step: 162976, Loss: 0.7429839968681335\n",
      "Epoch: 0, Step: 163008, Loss: 0.802043080329895\n",
      "Epoch: 0, Step: 163040, Loss: 0.8285366296768188\n",
      "Epoch: 0, Step: 163072, Loss: 0.8870511054992676\n",
      "Epoch: 0, Step: 163104, Loss: 0.8598691821098328\n",
      "Epoch: 0, Step: 163136, Loss: 0.8144694566726685\n",
      "Epoch: 0, Step: 163168, Loss: 0.7437387704849243\n",
      "Epoch: 0, Step: 163200, Loss: 0.8178026080131531\n",
      "Epoch: 0, Step: 163232, Loss: 0.9062594175338745\n",
      "Epoch: 0, Step: 163264, Loss: 0.8149809241294861\n",
      "Epoch: 0, Step: 163296, Loss: 0.7913946509361267\n",
      "Epoch: 0, Step: 163328, Loss: 0.8568465709686279\n",
      "Epoch: 0, Step: 163360, Loss: 0.8113182187080383\n",
      "Epoch: 0, Step: 163392, Loss: 0.7467064261436462\n",
      "Epoch: 0, Step: 163424, Loss: 0.7634106278419495\n",
      "Epoch: 0, Step: 163456, Loss: 0.8189390301704407\n",
      "Epoch: 0, Step: 163488, Loss: 0.830583930015564\n",
      "Epoch: 0, Step: 163520, Loss: 0.8689019680023193\n",
      "Epoch: 0, Step: 163552, Loss: 0.8307810425758362\n",
      "Epoch: 0, Step: 163584, Loss: 0.7982860803604126\n",
      "Epoch: 0, Step: 163616, Loss: 0.8587216138839722\n",
      "Epoch: 0, Step: 163648, Loss: 0.7598729729652405\n",
      "Epoch: 0, Step: 163680, Loss: 0.7834919095039368\n",
      "Epoch: 0, Step: 163712, Loss: 0.8247550129890442\n",
      "Epoch: 0, Step: 163744, Loss: 0.8452388644218445\n",
      "Epoch: 0, Step: 163776, Loss: 0.7697312831878662\n",
      "Epoch: 0, Step: 163808, Loss: 0.8266161680221558\n",
      "Epoch: 0, Step: 163840, Loss: 0.7743359208106995\n",
      "Epoch: 0, Step: 163872, Loss: 0.8797962069511414\n",
      "Epoch: 0, Step: 163904, Loss: 0.809481680393219\n",
      "Epoch: 0, Step: 163936, Loss: 0.8148291707038879\n",
      "Epoch: 0, Step: 163968, Loss: 0.7907084226608276\n",
      "Epoch: 0, Step: 164000, Loss: 0.888379693031311\n",
      "Epoch: 0, Step: 164032, Loss: 0.7452988028526306\n",
      "Epoch: 0, Step: 164064, Loss: 0.8287161588668823\n",
      "Epoch: 0, Step: 164096, Loss: 0.8944427967071533\n",
      "Epoch: 0, Step: 164128, Loss: 0.661881685256958\n",
      "Epoch: 0, Step: 164160, Loss: 0.8730915784835815\n",
      "Epoch: 0, Step: 164192, Loss: 0.8211557269096375\n",
      "Epoch: 0, Step: 164224, Loss: 0.7078160047531128\n",
      "Epoch: 0, Step: 164256, Loss: 0.8399208784103394\n",
      "Epoch: 0, Step: 164288, Loss: 0.8748365044593811\n",
      "Epoch: 0, Step: 164320, Loss: 0.8983607292175293\n",
      "Epoch: 0, Step: 164352, Loss: 0.8380149006843567\n",
      "Epoch: 0, Step: 164384, Loss: 0.7655877470970154\n",
      "Epoch: 0, Step: 164416, Loss: 0.9006363153457642\n",
      "Epoch: 0, Step: 164448, Loss: 0.8754526376724243\n",
      "Epoch: 0, Step: 164480, Loss: 0.7532546520233154\n",
      "Epoch: 0, Step: 164512, Loss: 0.8264276385307312\n",
      "Epoch: 0, Step: 164544, Loss: 0.7956922650337219\n",
      "Epoch: 0, Step: 164576, Loss: 0.7129136919975281\n",
      "Epoch: 0, Step: 164608, Loss: 0.8630123734474182\n",
      "Epoch: 0, Step: 164640, Loss: 0.7664419412612915\n",
      "Epoch: 0, Step: 164672, Loss: 0.9019671082496643\n",
      "Epoch: 0, Step: 164704, Loss: 0.8407214879989624\n",
      "Epoch: 0, Step: 164736, Loss: 0.7327147126197815\n",
      "Epoch: 0, Step: 164768, Loss: 0.799310564994812\n",
      "Epoch: 0, Step: 164800, Loss: 0.7580741047859192\n",
      "Epoch: 0, Step: 164832, Loss: 0.7912456393241882\n",
      "Epoch: 0, Step: 164864, Loss: 0.7998231053352356\n",
      "Epoch: 0, Step: 164896, Loss: 0.790145218372345\n",
      "Epoch: 0, Step: 164928, Loss: 0.8871197700500488\n",
      "Epoch: 0, Step: 164960, Loss: 0.8730496168136597\n",
      "Epoch: 0, Step: 164992, Loss: 0.84787917137146\n",
      "Epoch: 0, Step: 165024, Loss: 0.7759514451026917\n",
      "Epoch: 0, Step: 165056, Loss: 0.7145549058914185\n",
      "Epoch: 0, Step: 165088, Loss: 0.8719818592071533\n",
      "Epoch: 0, Step: 165120, Loss: 0.8066110014915466\n",
      "Epoch: 0, Step: 165152, Loss: 0.832017183303833\n",
      "Epoch: 0, Step: 165184, Loss: 0.7893537282943726\n",
      "Epoch: 0, Step: 165216, Loss: 0.886293888092041\n",
      "Epoch: 0, Step: 165248, Loss: 0.9037735462188721\n",
      "Epoch: 0, Step: 165280, Loss: 0.7718585729598999\n",
      "Epoch: 0, Step: 165312, Loss: 0.8407761454582214\n",
      "Epoch: 0, Step: 165344, Loss: 0.8321930766105652\n",
      "Epoch: 0, Step: 165376, Loss: 0.820171594619751\n",
      "Epoch: 0, Step: 165408, Loss: 0.7623005509376526\n",
      "Epoch: 0, Step: 165440, Loss: 0.8570317625999451\n",
      "Epoch: 0, Step: 165472, Loss: 0.8304367661476135\n",
      "Epoch: 0, Step: 165504, Loss: 0.7689245939254761\n",
      "Epoch: 0, Step: 165536, Loss: 0.844547688961029\n",
      "Epoch: 0, Step: 165568, Loss: 0.8500463366508484\n",
      "Epoch: 0, Step: 165600, Loss: 0.7139211893081665\n",
      "Epoch: 0, Step: 165632, Loss: 0.826994001865387\n",
      "Epoch: 0, Step: 165664, Loss: 0.8418058156967163\n",
      "Epoch: 0, Step: 165696, Loss: 0.8544629216194153\n",
      "Epoch: 0, Step: 165728, Loss: 0.7596830129623413\n",
      "Epoch: 0, Step: 165760, Loss: 0.7691256999969482\n",
      "Epoch: 0, Step: 165792, Loss: 0.7162032723426819\n",
      "Epoch: 0, Step: 165824, Loss: 0.8949788808822632\n",
      "Epoch: 0, Step: 165856, Loss: 0.7837938666343689\n",
      "Epoch: 0, Step: 165888, Loss: 0.8002951741218567\n",
      "Epoch: 0, Step: 165920, Loss: 0.8159373998641968\n",
      "Epoch: 0, Step: 165952, Loss: 0.8847336769104004\n",
      "Epoch: 0, Step: 165984, Loss: 0.9070773124694824\n",
      "Epoch: 0, Step: 166016, Loss: 0.7635589838027954\n",
      "Epoch: 0, Step: 166048, Loss: 0.8593264818191528\n",
      "Epoch: 0, Step: 166080, Loss: 0.7289192080497742\n",
      "Epoch: 0, Step: 166112, Loss: 0.8498818874359131\n",
      "Epoch: 0, Step: 166144, Loss: 0.871655285358429\n",
      "Epoch: 0, Step: 166176, Loss: 0.8237841725349426\n",
      "Epoch: 0, Step: 166208, Loss: 0.8192225694656372\n",
      "Epoch: 0, Step: 166240, Loss: 0.8476105332374573\n",
      "Epoch: 0, Step: 166272, Loss: 0.6827061772346497\n",
      "Epoch: 0, Step: 166304, Loss: 0.8403791189193726\n",
      "Epoch: 0, Step: 166336, Loss: 0.9290164113044739\n",
      "Epoch: 0, Step: 166368, Loss: 0.7842148542404175\n",
      "Epoch: 0, Step: 166400, Loss: 0.9032265543937683\n",
      "Epoch: 0, Step: 166432, Loss: 0.8797568082809448\n",
      "Epoch: 0, Step: 166464, Loss: 0.8270283341407776\n",
      "Epoch: 0, Step: 166496, Loss: 0.9002247452735901\n",
      "Epoch: 0, Step: 166528, Loss: 0.8141888380050659\n",
      "Epoch: 0, Step: 166560, Loss: 0.8912007808685303\n",
      "Epoch: 0, Step: 166592, Loss: 0.8759448528289795\n",
      "Epoch: 0, Step: 166624, Loss: 0.8798092603683472\n",
      "Epoch: 0, Step: 166656, Loss: 0.8318386673927307\n",
      "Epoch: 0, Step: 166688, Loss: 0.7605758905410767\n",
      "Epoch: 0, Step: 166720, Loss: 0.8236380815505981\n",
      "Epoch: 0, Step: 166752, Loss: 0.9131448268890381\n",
      "Epoch: 0, Step: 166784, Loss: 0.8680580854415894\n",
      "Epoch: 0, Step: 166816, Loss: 0.8871937990188599\n",
      "Epoch: 0, Step: 166848, Loss: 0.7652493715286255\n",
      "Epoch: 0, Step: 166880, Loss: 0.7975466251373291\n",
      "Epoch: 0, Step: 166912, Loss: 0.8072031736373901\n",
      "Epoch: 0, Step: 166944, Loss: 0.802724301815033\n",
      "Epoch: 0, Step: 166976, Loss: 0.8626915216445923\n",
      "Epoch: 0, Step: 167008, Loss: 0.7930505871772766\n",
      "Epoch: 0, Step: 167040, Loss: 0.8805272579193115\n",
      "Epoch: 0, Step: 167072, Loss: 0.80085289478302\n",
      "Epoch: 0, Step: 167104, Loss: 0.8011313080787659\n",
      "Epoch: 0, Step: 167136, Loss: 0.822284996509552\n",
      "Epoch: 0, Step: 167168, Loss: 0.9271036386489868\n",
      "Epoch: 0, Step: 167200, Loss: 0.7157177329063416\n",
      "Epoch: 0, Step: 167232, Loss: 0.8331416249275208\n",
      "Epoch: 0, Step: 167264, Loss: 0.8183449506759644\n",
      "Epoch: 0, Step: 167296, Loss: 0.8049454092979431\n",
      "Epoch: 0, Step: 167328, Loss: 0.743738055229187\n",
      "Epoch: 0, Step: 167360, Loss: 0.8083718419075012\n",
      "Epoch: 0, Step: 167392, Loss: 0.9717366099357605\n",
      "Epoch: 0, Step: 167424, Loss: 0.8340839743614197\n",
      "Epoch: 0, Step: 167456, Loss: 0.8469091653823853\n",
      "Epoch: 0, Step: 167488, Loss: 0.8050666451454163\n",
      "Epoch: 0, Step: 167520, Loss: 0.8494846224784851\n",
      "Epoch: 0, Step: 167552, Loss: 0.7688901424407959\n",
      "Epoch: 0, Step: 167584, Loss: 0.7861360907554626\n",
      "Epoch: 0, Step: 167616, Loss: 0.8039352297782898\n",
      "Epoch: 0, Step: 167648, Loss: 0.9123997688293457\n",
      "Epoch: 0, Step: 167680, Loss: 0.7890669703483582\n",
      "Epoch: 0, Step: 167712, Loss: 0.7170284986495972\n",
      "Epoch: 0, Step: 167744, Loss: 0.8923310041427612\n",
      "Epoch: 0, Step: 167776, Loss: 0.7996571660041809\n",
      "Epoch: 0, Step: 167808, Loss: 0.8830926418304443\n",
      "Epoch: 0, Step: 167840, Loss: 0.9104501605033875\n",
      "Epoch: 0, Step: 167872, Loss: 0.812995970249176\n",
      "Epoch: 0, Step: 167904, Loss: 0.8384932279586792\n",
      "Epoch: 0, Step: 167936, Loss: 0.7065428495407104\n",
      "Epoch: 0, Step: 167968, Loss: 0.8543301224708557\n",
      "Epoch: 0, Step: 168000, Loss: 0.8005030751228333\n",
      "Epoch: 0, Step: 168032, Loss: 0.7615997195243835\n",
      "Epoch: 0, Step: 168064, Loss: 0.8534036874771118\n",
      "Epoch: 0, Step: 168096, Loss: 0.8004772067070007\n",
      "Epoch: 0, Step: 168128, Loss: 0.8459781408309937\n",
      "Epoch: 0, Step: 168160, Loss: 0.7893355488777161\n",
      "Epoch: 0, Step: 168192, Loss: 0.8394054770469666\n",
      "Epoch: 0, Step: 168224, Loss: 0.7294266223907471\n",
      "Epoch: 0, Step: 168256, Loss: 0.8051604628562927\n",
      "Epoch: 0, Step: 168288, Loss: 0.886911928653717\n",
      "Epoch: 0, Step: 168320, Loss: 0.8551805019378662\n",
      "Epoch: 0, Step: 168352, Loss: 0.7894275188446045\n",
      "Epoch: 0, Step: 168384, Loss: 0.8171432018280029\n",
      "Epoch: 0, Step: 168416, Loss: 0.7058258652687073\n",
      "Epoch: 0, Step: 168448, Loss: 0.7683090567588806\n",
      "Epoch: 0, Step: 168480, Loss: 0.790644109249115\n",
      "Epoch: 0, Step: 168512, Loss: 0.8059981465339661\n",
      "Epoch: 0, Step: 168544, Loss: 0.7469578385353088\n",
      "Epoch: 0, Step: 168576, Loss: 0.9322841167449951\n",
      "Epoch: 0, Step: 168608, Loss: 0.728417158126831\n",
      "Epoch: 0, Step: 168640, Loss: 0.8538819551467896\n",
      "Epoch: 0, Step: 168672, Loss: 0.8429184556007385\n",
      "Epoch: 0, Step: 168704, Loss: 0.8227508068084717\n",
      "Epoch: 0, Step: 168736, Loss: 0.8709686994552612\n",
      "Epoch: 0, Step: 168768, Loss: 0.9608557820320129\n",
      "Epoch: 0, Step: 168800, Loss: 0.7971683144569397\n",
      "Epoch: 0, Step: 168832, Loss: 0.7838959097862244\n",
      "Epoch: 0, Step: 168864, Loss: 0.8015041947364807\n",
      "Epoch: 0, Step: 168896, Loss: 0.8440361618995667\n",
      "Epoch: 0, Step: 168928, Loss: 0.8208582997322083\n",
      "Epoch: 0, Step: 168960, Loss: 0.8222099542617798\n",
      "Epoch: 0, Step: 168992, Loss: 0.7564615607261658\n",
      "Epoch: 0, Step: 169024, Loss: 0.8369615077972412\n",
      "Epoch: 0, Step: 169056, Loss: 0.8188028931617737\n",
      "Epoch: 0, Step: 169088, Loss: 0.7786881923675537\n",
      "Epoch: 0, Step: 169120, Loss: 0.9304213523864746\n",
      "Epoch: 0, Step: 169152, Loss: 0.8051882386207581\n",
      "Epoch: 0, Step: 169184, Loss: 0.667667031288147\n",
      "Epoch: 0, Step: 169216, Loss: 0.7626668214797974\n",
      "Epoch: 0, Step: 169248, Loss: 0.7773875594139099\n",
      "Epoch: 0, Step: 169280, Loss: 0.848127543926239\n",
      "Epoch: 0, Step: 169312, Loss: 0.7723116278648376\n",
      "Epoch: 0, Step: 169344, Loss: 0.8210947513580322\n",
      "Epoch: 0, Step: 169376, Loss: 0.7607520222663879\n",
      "Epoch: 0, Step: 169408, Loss: 0.7203983068466187\n",
      "Epoch: 0, Step: 169440, Loss: 0.9742300510406494\n",
      "Epoch: 0, Step: 169472, Loss: 0.9254196882247925\n",
      "Epoch: 0, Step: 169504, Loss: 0.8158018589019775\n",
      "Epoch: 0, Step: 169536, Loss: 0.8474118709564209\n",
      "Epoch: 0, Step: 169568, Loss: 0.8052972555160522\n",
      "Epoch: 0, Step: 169600, Loss: 0.8507369160652161\n",
      "Epoch: 0, Step: 169632, Loss: 0.8223491907119751\n",
      "Epoch: 0, Step: 169664, Loss: 0.7778970003128052\n",
      "Epoch: 0, Step: 169696, Loss: 0.8042327761650085\n",
      "Epoch: 0, Step: 169728, Loss: 0.8100542426109314\n",
      "Epoch: 0, Step: 169760, Loss: 0.8406679034233093\n",
      "Epoch: 0, Step: 169792, Loss: 0.7936171293258667\n",
      "Epoch: 0, Step: 169824, Loss: 0.7286299467086792\n",
      "Epoch: 0, Step: 169856, Loss: 0.8846186399459839\n",
      "Epoch: 0, Step: 169888, Loss: 0.7613537907600403\n",
      "Epoch: 0, Step: 169920, Loss: 0.8201100826263428\n",
      "Epoch: 0, Step: 169952, Loss: 0.7786362171173096\n",
      "Epoch: 0, Step: 169984, Loss: 0.7024189233779907\n",
      "Epoch: 0, Step: 170016, Loss: 0.8760632276535034\n",
      "Epoch: 0, Step: 170048, Loss: 0.8262316584587097\n",
      "Epoch: 0, Step: 170080, Loss: 0.7498427629470825\n",
      "Epoch: 0, Step: 170112, Loss: 0.7617514133453369\n",
      "Epoch: 0, Step: 170144, Loss: 0.9026858806610107\n",
      "Epoch: 0, Step: 170176, Loss: 0.7370873093605042\n",
      "Epoch: 0, Step: 170208, Loss: 0.8323558568954468\n",
      "Epoch: 0, Step: 170240, Loss: 0.7901326417922974\n",
      "Epoch: 0, Step: 170272, Loss: 0.8701308369636536\n",
      "Epoch: 0, Step: 170304, Loss: 0.8098790049552917\n",
      "Epoch: 0, Step: 170336, Loss: 0.8136715292930603\n",
      "Epoch: 0, Step: 170368, Loss: 0.7847393751144409\n",
      "Epoch: 0, Step: 170400, Loss: 0.8296434879302979\n",
      "Epoch: 0, Step: 170432, Loss: 0.7937632203102112\n",
      "Epoch: 0, Step: 170464, Loss: 0.8651542663574219\n",
      "Epoch: 0, Step: 170496, Loss: 0.8753362894058228\n",
      "Epoch: 0, Step: 170528, Loss: 0.815596342086792\n",
      "Epoch: 0, Step: 170560, Loss: 0.7481426000595093\n",
      "Epoch: 0, Step: 170592, Loss: 0.8506003618240356\n",
      "Epoch: 0, Step: 170624, Loss: 0.8007122874259949\n",
      "Epoch: 0, Step: 170656, Loss: 0.7787278294563293\n",
      "Epoch: 0, Step: 170688, Loss: 0.7816912531852722\n",
      "Epoch: 0, Step: 170720, Loss: 0.9174579381942749\n",
      "Epoch: 0, Step: 170752, Loss: 0.7684525847434998\n",
      "Epoch: 0, Step: 170784, Loss: 0.8123289942741394\n",
      "Epoch: 0, Step: 170816, Loss: 0.8805308938026428\n",
      "Epoch: 0, Step: 170848, Loss: 0.8845369815826416\n",
      "Epoch: 0, Step: 170880, Loss: 0.7643587589263916\n",
      "Epoch: 0, Step: 170912, Loss: 0.7435639500617981\n",
      "Epoch: 0, Step: 170944, Loss: 0.7353735566139221\n",
      "Epoch: 0, Step: 170976, Loss: 0.9141901135444641\n",
      "Epoch: 0, Step: 171008, Loss: 0.9369327425956726\n",
      "Epoch: 0, Step: 171040, Loss: 0.9202154278755188\n",
      "Epoch: 0, Step: 171072, Loss: 0.8037763833999634\n",
      "Epoch: 0, Step: 171104, Loss: 0.7416465282440186\n",
      "Epoch: 0, Step: 171136, Loss: 0.8136690855026245\n",
      "Epoch: 0, Step: 171168, Loss: 0.7506478428840637\n",
      "Epoch: 0, Step: 171200, Loss: 0.847925066947937\n",
      "Epoch: 0, Step: 171232, Loss: 0.8603937029838562\n",
      "Epoch: 0, Step: 171264, Loss: 0.8167203068733215\n",
      "Epoch: 0, Step: 171296, Loss: 0.7666350603103638\n",
      "Epoch: 0, Step: 171328, Loss: 0.8554233908653259\n",
      "Epoch: 0, Step: 171360, Loss: 0.7363740801811218\n",
      "Epoch: 0, Step: 171392, Loss: 0.9956037402153015\n",
      "Epoch: 0, Step: 171424, Loss: 0.7775792479515076\n",
      "Epoch: 0, Step: 171456, Loss: 0.8560474514961243\n",
      "Epoch: 0, Step: 171488, Loss: 0.9324853420257568\n",
      "Epoch: 0, Step: 171520, Loss: 0.8867555856704712\n",
      "Epoch: 0, Step: 171552, Loss: 0.7808253765106201\n",
      "Epoch: 0, Step: 171584, Loss: 0.8209418654441833\n",
      "Epoch: 0, Step: 171616, Loss: 0.9148193597793579\n",
      "Epoch: 0, Step: 171648, Loss: 0.8305745720863342\n",
      "Epoch: 0, Step: 171680, Loss: 0.7345188856124878\n",
      "Epoch: 0, Step: 171712, Loss: 0.8906620740890503\n",
      "Epoch: 0, Step: 171744, Loss: 0.8517501354217529\n",
      "Epoch: 0, Step: 171776, Loss: 0.8398376703262329\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dropout_rates = [0.4, 0.4, 0.4] # also try [0.5, 0.5, 0.5]  # Dropout rates for each layer except output\n",
    "model = Net([x_train.shape[1], 20, 10, 10, y_train.shape[1]], dropout_rates)\n",
    "model.train(train_loader, verbose=True)\n",
    "model.predict(test_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(model.train_loss, label='Training Loss', marker='x')\n",
    "plt.plot(model.validation_loss, label='Validation Loss', marker='x')\n",
    "# plt.yscale('log')\n",
    "# plt.xlim([0, 10])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2606.3250452305583]\n",
      "[2587.02666612047]\n"
     ]
    }
   ],
   "source": [
    "print(model.training_loss)\n",
    "print(model.validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "model_name = \"model1\"\n",
    "\n",
    "# Save the model weights\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "joblib.dump(x_scaler, rf'../models/{model_name}_xscaler')\n",
    "joblib.dump(y_scaler, rf'../models/{model_name}_yscaler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Net:\n\tMissing key(s) in state_dict: \"model.13.weight\", \"model.13.bias\", \"model.14.weight\", \"model.14.bias\", \"model.14.running_mean\", \"model.14.running_var\", \"model.17.weight\", \"model.17.bias\", \"model.18.weight\", \"model.18.bias\", \"model.18.running_mean\", \"model.18.running_var\", \"model.21.weight\", \"model.21.bias\", \"model.22.weight\", \"model.22.bias\", \"model.22.running_mean\", \"model.22.running_var\", \"model.25.weight\", \"model.25.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the saved weights\u001b[39;00m\n\u001b[1;32m      7\u001b[0m model_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_weights.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mnew_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/research/tfo_inverse_modelling/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Net:\n\tMissing key(s) in state_dict: \"model.13.weight\", \"model.13.bias\", \"model.14.weight\", \"model.14.bias\", \"model.14.running_mean\", \"model.14.running_var\", \"model.17.weight\", \"model.17.bias\", \"model.18.weight\", \"model.18.bias\", \"model.18.running_mean\", \"model.18.running_var\", \"model.21.weight\", \"model.21.bias\", \"model.22.weight\", \"model.22.bias\", \"model.22.running_mean\", \"model.22.running_var\", \"model.25.weight\", \"model.25.bias\". "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dropout_rates = [0.1, 0.1, 0.1]\n",
    "\n",
    "# Create an instance of the model\n",
    "new_model = Net([x_train.shape[1], 20, 10, 10, y_train.shape[1]], dropout_rates)\n",
    "\n",
    "# Load the saved weights\n",
    "model_weights = 'model_weights.pth'\n",
    "new_model.load_state_dict(torch.load(model_weights))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
